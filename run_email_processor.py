# ==============================================================================
# run_email_processor.py (å®Œæˆç‰ˆ)
# ==============================================================================

import sys
import os
import toml
import imaplib
import email
from email.header import decode_header, make_header
from email.utils import parsedate_to_datetime
from datetime import datetime
import traceback
import psycopg2
from psycopg2.extras import DictCursor
import google.generativeai as genai
import json
import re
import io
import fitz
import docx
import pandas as pd

# --- ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š ---
_SECRETS = None
_CONFIG = None

# ==============================================================================
# 1. ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ç¾¤
# ==============================================================================

# --- è¨­å®šãƒ»DBæ¥ç¶šé–¢é€£ ---
def load_secrets():
    global _SECRETS
    if _SECRETS is not None: return _SECRETS
    try:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        secrets_path = os.path.join(current_dir, '.streamlit', 'secrets.toml')
        with open(secrets_path, "r", encoding="utf-8") as f: _SECRETS = toml.load(f)
        return _SECRETS
    except Exception as e:
        print(f"âŒ secrets.toml ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def load_app_config():
    global _CONFIG
    if _CONFIG is not None: return _CONFIG
    try:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(current_dir, 'config.toml')
        with open(config_path, "r", encoding="utf-8") as f: _CONFIG = toml.load(f)
        return _CONFIG
    except Exception as e:
        print(f"âŒ config.toml ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return {}

def get_db_connection():
    secrets = load_secrets()
    if not secrets or "DATABASE_URL" not in secrets: raise ValueError("DATABASE_URLãŒsecrets.tomlã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    return psycopg2.connect(secrets["DATABASE_URL"], cursor_factory=DictCursor)

def configure_genai():
    secrets = load_secrets()
    if not secrets or "GOOGLE_API_KEY" not in secrets: raise ValueError("GOOGLE_API_KEYãŒsecrets.tomlã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    genai.configure(api_key=secrets["GOOGLE_API_KEY"])

# --- ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºãƒ»æ•´å½¢é–¢é€£ ---
def clean_and_format_text(text: str) -> str:
    if not text: return ""
    text_with_tabs = re.sub(r' {2,}', '\t', text)
    full_text = "\n".join([line.strip() for line in text_with_tabs.splitlines()])
    return re.sub(r'\n{3,}', '\n\n', full_text).strip()

def extract_text_from_pdf(file_bytes):
    try:
        with fitz.open(stream=file_bytes, filetype="pdf") as doc: raw_text = "".join(page.get_text() for page in doc)
        formatted_text = clean_and_format_text(raw_text)
        return formatted_text if formatted_text else "[PDFãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¤±æ•—: å†…å®¹ãŒç©ºã¾ãŸã¯ç”»åƒPDF]"
    except Exception as e: return f"[PDFãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}]"

def extract_text_from_docx(file_bytes):
    try:
        doc = docx.Document(io.BytesIO(file_bytes))
        raw_text = "\n".join([p.text for p in doc.paragraphs])
        formatted_text = clean_and_format_text(raw_text)
        return formatted_text if formatted_text else "[DOCXãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¤±æ•—: å†…å®¹ãŒç©º]"
    except Exception as e: return f"[DOCXãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}]"

def extract_text_from_excel(file_bytes: bytes) -> str:
    try:
        xls = pd.ExcelFile(io.BytesIO(file_bytes))
        all_text_parts = [f"\n### ã‚·ãƒ¼ãƒˆ: {name}\n{pd.read_excel(xls, sheet_name=name, header=None).to_string(header=False, index=False, na_rep='')}" for name in xls.sheet_names if not pd.read_excel(xls, sheet_name=name, header=None).empty]
        if not all_text_parts: return "[Excelãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¤±æ•—: å†…å®¹ãŒç©ºã§ã™]"
        return clean_and_format_text("".join(all_text_parts))
    except Exception as e: return f"[Excelãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}]"

def get_email_contents(msg):
    subject = str(make_header(decode_header(msg["subject"]))) if msg["subject"] else ""
    from_ = str(make_header(decode_header(msg["from"]))) if msg["from"] else ""
    received_at = parsedate_to_datetime(msg["Date"]) if msg["Date"] else None
    body_text, attachments = "", []
    if msg.is_multipart():
        for part in msg.walk():
            ctype, cdisp = part.get_content_type(), str(part.get("Content-Disposition"))
            if 'text/plain' in ctype and 'attachment' not in cdisp:
                charset = part.get_content_charset()
                try: body_text += part.get_payload(decode=True).decode(charset or 'utf-8', errors='ignore')
                except: body_text += part.get_payload(decode=True).decode('utf-8', errors='ignore')
            if 'attachment' in cdisp and (fname := part.get_filename()):
                filename = str(make_header(decode_header(fname)))
                #print(f"  > æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚")
                fb, lfname = part.get_payload(decode=True), filename.lower()
                content = ""
                if lfname.endswith(".pdf"): content = extract_text_from_pdf(fb)
                elif lfname.endswith(".docx"): content = extract_text_from_docx(fb)
                elif lfname.endswith((".xlsx", ".xls")): content = extract_text_from_excel(fb)
                elif lfname.endswith(".txt"): content = fb.decode('utf-8', errors='ignore')
                else: print(f"  > â„¹ï¸ æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã¯æœªå¯¾å¿œå½¢å¼ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã€‚")
                if content: attachments.append({"filename": filename, "content": content})
    else:
        charset = msg.get_content_charset()
        try: body_text = msg.get_payload(decode=True).decode(charset or 'utf-8', errors='ignore')
        except: body_text = msg.get_payload(decode=True).decode('utf-8', errors='ignore')
    return {"subject": subject, "from": from_, "received_at": received_at, "body": body_text.strip(), "attachments": attachments}

# --- LLMãƒ»DBå‡¦ç†é–¢é€£ ---


# â–¼â–¼â–¼ã€ã“ã“ã«é–¢æ•°ã‚’è¿½åŠ ã€‘â–¼â–¼â–¼
def extract_keywords_with_llm(text_content: str, item_type: str, count: int = 20) -> list:
    """
    AI(LLM)ã‚’ä½¿ã£ã¦ã€ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æœ€ã‚‚é‡è¦ãªã‚¹ã‚­ãƒ«ã‚’ã€Œæœ€å¤§countå€‹ã®ãƒªã‚¹ãƒˆã€ã¨ã—ã¦æŠ½å‡ºã™ã‚‹ã€‚
    """
    try:
        if item_type == 'job':
            instruction = f"ä»¥ä¸‹ã®æ¡ˆä»¶æƒ…å ±ã‹ã‚‰ã€æŠ€è¡“è€…ã‚’æ¢ã™ä¸Šã§æœ€ã‚‚é‡è¦åº¦ãŒé«˜ã„ã¨æ€ã‚ã‚Œã‚‹ã€Œå¿…é ˆã‚¹ã‚­ãƒ«ã€ã‚’ã€é‡è¦ãªã‚‚ã®ã‹ã‚‰é †ç•ªã«æœ€å¤§{count}å€‹æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚"
        else: # item_type == 'engineer'
            instruction = f"ä»¥ä¸‹ã®æŠ€è¡“è€…æƒ…å ±ã‹ã‚‰ã€ãã®äººã®ã‚­ãƒ£ãƒªã‚¢ã§æœ€ã‚‚æ ¸ã¨ãªã£ã¦ã„ã‚‹ã€Œã‚³ã‚¢ã‚¹ã‚­ãƒ«ã€ã‚’ã€å¾—æ„ãªã‚‚ã®ã‹ã‚‰é †ç•ªã«æœ€å¤§{count}å€‹æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚"
        
        prompt = f"""
        ã‚ãªãŸã¯ã€ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æœ€ã‚‚é‡è¦ãªæ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
        # çµ¶å¯¾çš„ãªãƒ«ãƒ¼ãƒ«:
        - æŠ½å‡ºã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ã€å¿…ãš**{count}å€‹ä»¥å†…**ã«å³é¸ã—ã¦ãã ã•ã„ã€‚
        - å‡ºåŠ›ã¯ã€**ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®å˜èªãƒªã‚¹ãƒˆã®ã¿**ã¨ã—ã€ä»–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚
        # æŒ‡ç¤º:
        {instruction}
        ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚„çµŒé¨“å¹´æ•°ãªã©ã®ä»˜éšæƒ…å ±ã¯å«ã‚ãšã€æŠ€è¡“åã‚„å½¹è·åãªã©ã®å˜èªã®ã¿ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
        # æœ¬ç•ª:
        å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ: ---
        {text_content}
        ---
        å‡ºåŠ›:
        """
        
        model = genai.GenerativeModel('models/gemini-2.5-flash-lite')

        #request_options = {"timeout": 10}

        response = model.generate_content(prompt)
        
        keywords = [kw.strip().lower() for kw in response.text.strip().split(',') if kw.strip()]
        
        if not keywords:
            print("  > âš ï¸ AIã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¿”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚")
            return []
            
        return keywords[:count]

    except Exception as e:
        print(f"  > âŒ LLMã«ã‚ˆã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return []
# â–²â–²â–²ã€è¿½åŠ ã“ã“ã¾ã§ã€‘â–²â–²â–²



def get_extraction_prompt(doc_type, text_content):
    """
    LLMã«ä¸ãˆã‚‹ã€æƒ…å ±æŠ½å‡ºç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚
    """
    if doc_type == 'engineer':
        return f"""
            ã‚ãªãŸã¯ã€ITäººæã®ã€Œã‚¹ã‚­ãƒ«ã‚·ãƒ¼ãƒˆã€ã‚„ã€Œè·å‹™çµŒæ­´æ›¸ã€ã‚’èª­ã¿è§£ãå°‚é–€å®¶ã§ã™ã€‚
            ã‚ãªãŸã®ä»•äº‹ã¯ã€ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰**å˜ä¸€ã®æŠ€è¡“è€…æƒ…å ±**ã‚’æŠ½å‡ºã—ã€æŒ‡å®šã•ã‚ŒãŸJSONå½¢å¼ã§æ•´ç†ã™ã‚‹ã“ã¨ã§ã™ã€‚
            # çµ¶å¯¾çš„ãªãƒ«ãƒ¼ãƒ«
            - å‡ºåŠ›ã¯ã€æŒ‡å®šã•ã‚ŒãŸJSONå½¢å¼ã®æ–‡å­—åˆ—ã®ã¿ã¨ã—ã€å‰å¾Œã«è§£èª¬ã‚„```json ```ã®ã‚ˆã†ãªã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®å›²ã¿ã‚’å«ã‚ãªã„ã§ãã ã•ã„ã€‚
            # æŒ‡ç¤º
            - ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã¯ã€ä¸€äººã®æŠ€è¡“è€…ã®æƒ…å ±ã§ã™ã€‚è¤‡æ•°ã®æ¥­å‹™çµŒæ­´ãŒå«ã¾ã‚Œã¦ã„ã¦ã‚‚ã€ãã‚Œã‚‰ã¯ã™ã¹ã¦ã“ã®ä¸€äººã®æŠ€è¡“è€…ã®çµŒæ­´ã¨ã—ã¦è¦ç´„ã—ã¦ãã ã•ã„ã€‚
            - `document`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ã¯ã€æŠ€è¡“è€…ã®ã‚¹ã‚­ãƒ«ã€çµŒé¨“ã€è‡ªå·±PRãªã©ã‚’ç·åˆçš„ã«è¦ç´„ã—ãŸã€æ¤œç´¢ã—ã‚„ã™ã„è‡ªç„¶ãªæ–‡ç« ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
            - `document`ã®æ–‡ç« ã®å…ˆé ­ã«ã¯ã€å¿…ãšæŠ€è¡“è€…åã‚’å«ã‚ã¦ãã ã•ã„ã€‚ä¾‹ï¼šã€Œå®Ÿå‹™çµŒé¨“15å¹´ã®TKæ°ã€‚Java(SpringBoot)ã‚’ä¸»è»¸ã«...ã€
            # å…·ä½“ä¾‹
            ## å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ:
            æ°å: å±±ç”° å¤ªéƒ
            å¹´é½¢: 35æ­³
            å¾—æ„æŠ€è¡“: Java, Spring
            è‡ªå·±PR: Webã‚¢ãƒ—ãƒªé–‹ç™ºãŒå¾—æ„ã§ã™ã€‚
            ## å‡ºåŠ›JSON:
            {{"engineers": [{{"name": "å±±ç”° å¤ªéƒ", "document": "35æ­³ã®å±±ç”°å¤ªéƒæ°ã€‚Java, Springã‚’ç”¨ã„ãŸWebã‚¢ãƒ—ãƒªé–‹ç™ºãŒå¾—æ„ã€‚", "main_skills": "Java, Spring"}}]}}
            # JSONå‡ºåŠ›å½¢å¼
            {{"engineers": [{{"name": "æŠ€è¡“è€…ã®æ°åã‚’æŠ½å‡º", "document": "æŠ€è¡“è€…ã®ã‚¹ã‚­ãƒ«ã‚„çµŒæ­´ã®è©³ç´°ã‚’ã€æ¤œç´¢ã—ã‚„ã™ã„ã‚ˆã†ã«è¦ç´„", "nationality": "å›½ç±ã‚’æŠ½å‡º", "availability_date": "ç¨¼åƒå¯èƒ½æ—¥ã‚’æŠ½å‡º", "desired_location": "å¸Œæœ›å‹¤å‹™åœ°ã‚’æŠ½å‡º", "desired_salary": "å¸Œæœ›å˜ä¾¡ã‚’æŠ½å‡º", "main_skills": "ä¸»è¦ãªã‚¹ã‚­ãƒ«ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§æŠ½å‡º"}}]}}
            # æœ¬ç•ª: ä»¥ä¸‹ã®ã‚¹ã‚­ãƒ«ã‚·ãƒ¼ãƒˆã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„
            ---
            {text_content}
        """
    elif doc_type == 'job':
        return f"""
            ã‚ãªãŸã¯ã€ITæ¥­ç•Œã®ã€Œæ¡ˆä»¶å®šç¾©æ›¸ã€ã‚’èª­ã¿è§£ãå°‚é–€å®¶ã§ã™ã€‚
            ã‚ãªãŸã®ä»•äº‹ã¯ã€ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰**æ¡ˆä»¶æƒ…å ±**ã‚’æŠ½å‡ºã—ã€æŒ‡å®šã•ã‚ŒãŸJSONå½¢å¼ã§æ•´ç†ã™ã‚‹ã“ã¨ã§ã™ã€‚
            ãƒ†ã‚­ã‚¹ãƒˆå†…ã«è¤‡æ•°ã®æ¡ˆä»¶æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãã‚Œãã‚Œã‚’å€‹åˆ¥ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦ãƒªã‚¹ãƒˆã«ã—ã¦ãã ã•ã„ã€‚
            # çµ¶å¯¾çš„ãªãƒ«ãƒ¼ãƒ«
            - å‡ºåŠ›ã¯ã€æŒ‡å®šã•ã‚ŒãŸJSONå½¢å¼ã®æ–‡å­—åˆ—ã®ã¿ã¨ã—ã€å‰å¾Œã«è§£èª¬ã‚„```json ```ã®ã‚ˆã†ãªã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®å›²ã¿ã‚’å«ã‚ãªã„ã§ãã ã•ã„ã€‚
            # æŒ‡ç¤º
            - `document`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ã¯ã€æ¡ˆä»¶ã®ã‚¹ã‚­ãƒ«ã‚„æ¥­å‹™å†…å®¹ã®è©³ç´°ã‚’ã€å¾Œã§æ¤œç´¢ã—ã‚„ã™ã„ã‚ˆã†ã«è‡ªç„¶ãªæ–‡ç« ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚
            - `document`ã®æ–‡ç« ã®å…ˆé ­ã«ã¯ã€å¿…ãšãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’å«ã‚ã¦ãã ã•ã„ã€‚ä¾‹ï¼šã€Œç¤¾å†…SEãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å¢—å“¡æ¡ˆä»¶ã€‚è¨­è¨ˆã€ãƒ†ã‚¹ãƒˆ...ã€
            # JSONå‡ºåŠ›å½¢å¼
            {{"jobs": [{{"project_name": "æ¡ˆä»¶åã‚’æŠ½å‡º", "document": "æ¡ˆä»¶ã®ã‚¹ã‚­ãƒ«ã‚„æ¥­å‹™å†…å®¹ã®è©³ç´°ã‚’ã€æ¤œç´¢ã—ã‚„ã™ã„ã‚ˆã†ã«è¦ç´„", "nationality_requirement": "å›½ç±è¦ä»¶ã‚’æŠ½å‡º", "start_date": "é–‹å§‹æ™‚æœŸã‚’æŠ½å‡º", "location": "å‹¤å‹™åœ°ã‚’æŠ½å‡º", "unit_price": "å˜ä¾¡ã‚„äºˆç®—ã‚’æŠ½å‡º", "required_skills": "å¿…é ˆã‚¹ã‚­ãƒ«ã‚„æ­“è¿ã‚¹ã‚­ãƒ«ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§æŠ½å‡º"}}]}}
            # æœ¬ç•ª: ä»¥ä¸‹ã®æ¡ˆä»¶æƒ…å ±ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„
            ---
            {text_content}
        """
    return ""



def _build_meta_info_string(item_type, item_data):
    meta_fields = []
    if item_type == 'job':
        meta_fields = [["å›½ç±è¦ä»¶", "nationality_requirement"], ["é–‹å§‹æ™‚æœŸ", "start_date"], ["å‹¤å‹™åœ°", "location"], ["å˜ä¾¡", "unit_price"], ["å¿…é ˆã‚¹ã‚­ãƒ«", "required_skills"]]
    elif item_type == 'engineer':
        meta_fields = [["å›½ç±", "nationality"], ["ç¨¼åƒå¯èƒ½æ—¥", "availability_date"], ["å¸Œæœ›å‹¤å‹™åœ°", "desired_location"], ["å¸Œæœ›å˜ä¾¡", "desired_salary"], ["ä¸»è¦ã‚¹ã‚­ãƒ«", "main_skills"]]
    if not meta_fields: return "\n---\n"
    meta_parts = [f"[{display_name}: {item_data.get(key, 'ä¸æ˜')}]" for display_name, key in meta_fields]
    return " ".join(meta_parts) + "\n---\n"

# â–¼â–¼â–¼ã€ã“ã“ãŒä»Šå›ã®ä¿®æ­£ã®æ ¸ã¨ãªã‚‹é–¢æ•°ã€‘â–¼â–¼â–¼
def split_text_with_llm(text_content: str) -> (dict | None, list):
    logs = []
    classification_prompt = f"""
        ã‚ãªãŸã¯ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆãŒã€Œæ¡ˆä»¶æƒ…å ±ã€ã€ŒæŠ€è¡“è€…æƒ…å ±ã€ã€Œãã®ä»–ã€ã®ã©ã‚Œã«æœ€ã‚‚å½“ã¦ã¯ã¾ã‚‹ã‹åˆ¤æ–­ã—ã€æŒ‡å®šã•ã‚ŒãŸå˜èªä¸€ã¤ã ã‘ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
        # åˆ¤æ–­åŸºæº–
        - ã€Œã‚¹ã‚­ãƒ«ã‚·ãƒ¼ãƒˆã€ã€Œè·å‹™çµŒæ­´æ›¸ã€ã€Œæ°åã€ã€Œå¹´é½¢ã€ã¨ã„ã£ãŸå˜èªãŒå«ã¾ã‚Œã¦ã„ã‚Œã°ã€ŒæŠ€è¡“è€…æƒ…å ±ã€ã®å¯èƒ½æ€§ãŒé«˜ã„ã€‚
        - ã€Œå‹Ÿé›†ã€ã€Œå¿…é ˆã‚¹ã‚­ãƒ«ã€ã€Œæ­“è¿ã‚¹ã‚­ãƒ«ã€ã€Œæ±‚ã‚ã‚‹äººç‰©åƒã€ã¨ã„ã£ãŸå˜èªãŒå«ã¾ã‚Œã¦ã„ã‚Œã°ã€Œæ¡ˆä»¶æƒ…å ±ã€ã®å¯èƒ½æ€§ãŒé«˜ã„ã€‚
        # å›ç­”å½¢å¼
        - `æ¡ˆä»¶æƒ…å ±`
        - `æŠ€è¡“è€…æƒ…å ±`
        - `ãã®ä»–`
        # åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
        ---
        {text_content[:2000]}
        ---
    """
    try:
        model = genai.GenerativeModel('models/gemini-2.5-flash-lite')
        logs.append("  > ğŸ“„ æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã‚’åˆ†é¡ä¸­...")

        #request_options = {"timeout": 10}

        response = model.generate_content(classification_prompt)
        doc_type = response.text.strip()
        logs.append(f"  > âœ… AIã«ã‚ˆã‚‹åˆ†é¡çµæœ: {doc_type}")
    except Exception as e:
        logs.append(f"  > âŒ æ–‡æ›¸ã®åˆ†é¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None, logs

    if "æŠ€è¡“è€…æƒ…å ±" in doc_type:
        extraction_prompt = get_extraction_prompt('engineer', text_content)
    elif "æ¡ˆä»¶æƒ…å ±" in doc_type:
        extraction_prompt = get_extraction_prompt('job', text_content)
    else:
        logs.append("  > âš ï¸ ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã¯æ¡ˆä»¶æƒ…å ±ã¾ãŸã¯æŠ€è¡“è€…æƒ…å ±ã¨ã—ã¦åˆ†é¡ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None, logs

    generation_config = {"response_mime_type": "application/json"}
    safety_settings = {'HARM_CATEGORY_HARASSMENT': 'BLOCK_NONE', 'HARM_CATEGORY_HATE_SPEECH': 'BLOCK_NONE', 'HARM_CATEGORY_SEXUALLY_EXPLICIT': 'BLOCK_NONE', 'HARM_CATEGORY_DANGEROUS_CONTENT': 'BLOCK_NONE'}
    
    try:
        logs.append("  > ğŸ¤– AIãŒæƒ…å ±ã‚’æ§‹é€ åŒ–ä¸­...")

        #request_options = {"timeout": 20}

        response = model.generate_content(extraction_prompt, generation_config=generation_config, safety_settings=safety_settings)
        raw_text = response.text
        
        parsed_json = None
        start_index = raw_text.find('{')
        if start_index == -1:
            logs.append("  > âŒ LLMå¿œç­”ã‹ã‚‰JSONé–‹å§‹æ–‡å­—'{'ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return None, logs

        brace_counter, end_index = 0, -1
        for i in range(start_index, len(raw_text)):
            char = raw_text[i]
            if char == '{': brace_counter += 1
            elif char == '}': brace_counter -= 1
            if brace_counter == 0:
                end_index = i
                break
        
        if end_index == -1:
            logs.append("  > âŒ LLMå¿œç­”ã®JSONæ§‹é€ ãŒå£Šã‚Œã¦ã„ã¾ã™ï¼ˆæ‹¬å¼§ã®å¯¾å¿œãŒå–ã‚Œã¾ã›ã‚“ï¼‰ã€‚")
            return None, logs

        json_str = raw_text[start_index : end_index + 1]
        try:
            parsed_json = json.loads(json_str)
            logs.append("  > âœ… JSONã®ãƒ‘ãƒ¼ã‚¹ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
        except json.JSONDecodeError as e:
            logs.append(f"  > âš ï¸ JSONãƒ‘ãƒ¼ã‚¹å¤±æ•—ã€‚ä¿®å¾©è©¦è¡Œ... (ã‚¨ãƒ©ãƒ¼: {e})")
            repaired_text = re.sub(r',\s*([\}\]])', r'\1', re.sub(r'(?<!\\)\n', r'\\n', json_str))
            try:
                parsed_json = json.loads(repaired_text)
                logs.append("  > âœ… JSONã®ä¿®å¾©ã¨å†ãƒ‘ãƒ¼ã‚¹ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
            except json.JSONDecodeError as final_e:
                logs.append(f"  > âŒ JSONä¿®å¾©å¾Œã‚‚ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {final_e}")
                return None, logs

        if "æŠ€è¡“è€…æƒ…å ±" in doc_type: parsed_json["jobs"] = []
        elif "æ¡ˆä»¶æƒ…å ±" in doc_type: parsed_json["engineers"] = []
        return parsed_json, logs

    except Exception as e:
        logs.append(f"  > âŒ LLMã«ã‚ˆã‚‹æ§‹é€ åŒ–å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None, logs


# run_email_processor.py å†…

# ãƒ•ã‚¡ã‚¤ãƒ«ã®å†’é ­ã§ã€å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
from datetime import datetime
import pytz # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³å‡¦ç†ã«å¿…è¦
import json
# ... ä»–ã®å¿…è¦ãªimportæ–‡

def process_single_email_core(source_data: dict) -> (bool, list):
    """
    ã€å®Œæˆç‰ˆã€‘
    å˜ä¸€ã®ãƒ¡ãƒ¼ãƒ«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è§£æã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã€DBã«ç™»éŒ²ã™ã‚‹ã€‚
    ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³å‡¦ç†ã‚’å¼·åŒ–ã€‚
    """
    logs = []
    if not source_data: 
        logs.append("âš ï¸ å‡¦ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")
        return False, logs

    # --- 1. ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æº–å‚™ ---
    valid_attachments_content = [f"\n\n--- æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«: {att['filename']} ---\n{att.get('content', '')}" for att in source_data.get('attachments', []) if att.get('content')]
    if valid_attachments_content: 
        logs.append(f"  > â„¹ï¸ {len(valid_attachments_content)}ä»¶ã®æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’è§£æã«å«ã‚ã¾ã™ã€‚")
    
    full_text_for_llm = source_data.get('body', '') + "".join(valid_attachments_content)
    if not full_text_for_llm.strip(): 
        logs.append("âš ï¸ è§£æå¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return False, logs

    # --- 2. AIã«ã‚ˆã‚‹æƒ…å ±æ§‹é€ åŒ– ---
    parsed_data, llm_logs = split_text_with_llm(full_text_for_llm)
    logs.extend(llm_logs)
    if not parsed_data: 
        return False, logs
    
    new_jobs = parsed_data.get("jobs", [])
    new_engineers = parsed_data.get("engineers", [])
    if not new_jobs and not new_engineers: 
        logs.append("âš ï¸ LLMã¯ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ¡ˆä»¶æƒ…å ±ã¾ãŸã¯æŠ€è¡“è€…æƒ…å ±ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        return False, logs
    
    # --- 3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®ä¿å­˜å‡¦ç† ---
    logs.append("  > âœ… æŠ½å‡ºã•ã‚ŒãŸæƒ…å ±ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã—ã¾ã™...")
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cursor:
                
                # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚’LAã«çµ±ä¸€ã—ã¦ã€ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ä»˜ãã®datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”Ÿæˆ
                target_tz = pytz.timezone('America/Los_Angeles')
                now_in_la = datetime.now(target_tz)
                
                received_at_dt = source_data.get('received_at')
                # JSONã¨ã—ã¦ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ãŸã‚ã«ã€datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ISOå½¢å¼ã®æ–‡å­—åˆ—ã«å¤‰æ›
                source_json_str = json.dumps({k: v.isoformat() if isinstance(v, datetime) else v for k, v in source_data.items()}, ensure_ascii=False, indent=2)

                # --- jobs ã®å‡¦ç† ---
                for item_data in new_jobs:
                    name = item_data.get("project_name", "åç§°æœªå®šã®æ¡ˆä»¶")
                    full_document = _build_meta_info_string('job', item_data) + (item_data.get("document") or full_text_for_llm)
                    
                    logs.append(f"    -> æ¡ˆä»¶ã€{name}ã€ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºä¸­...")
                    keywords = extract_keywords_with_llm(full_document, 'job') # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«å†…ã«å®šç¾©ãŒå¿…è¦
                    logs.append(f"    -> æŠ½å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keywords}")

                    sql = """
                        INSERT INTO jobs (project_name, document, source_data_json, created_at, received_at, keywords) 
                        VALUES (%s, %s, %s, %s, %s, %s) RETURNING id
                    """
                    params = (name, full_document, source_json_str, now_in_la, received_at_dt, keywords)
                    
                    try:
                        log_query = cursor.mogrify(sql, params).decode('utf-8', 'ignore')
                        logs.append(f"    -> Executing SQL: {log_query[:500]}...")
                    except Exception as log_err:
                        logs.append(f"    -> Failed to mogrify query for logging: {log_err}")
                    
                    cursor.execute(sql, params)
                    
                    result = cursor.fetchone()
                    if result:
                        logs.append(f"    -> æ–°è¦æ¡ˆä»¶ã‚’ç™»éŒ²: ã€{name}ã€ (ID: {result['id']})")
                    else:
                        logs.append(f"    -> âš ï¸ æ¡ˆä»¶ã€{name}ã€ã®DBç™»éŒ²ã«å¤±æ•—ã€ã¾ãŸã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚")

                # --- engineers ã®å‡¦ç† ---
                for item_data in new_engineers:
                    name = item_data.get("name", "åç§°ä¸æ˜ã®æŠ€è¡“è€…")
                    full_document = _build_meta_info_string('engineer', item_data) + (item_data.get("document") or full_text_for_llm)

                    logs.append(f"    -> æŠ€è¡“è€…ã€{name}ã€ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºä¸­...")
                    keywords = extract_keywords_with_llm(full_document, 'engineer') # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«å†…ã«å®šç¾©ãŒå¿…è¦
                    logs.append(f"    -> æŠ½å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keywords}")

                    sql = """
                        INSERT INTO engineers (name, document, source_data_json, created_at, received_at, keywords) 
                        VALUES (%s, %s, %s, %s, %s, %s) RETURNING id
                    """
                    params = (name, full_document, source_json_str, now_in_la, received_at_dt, keywords)

                    try:
                        log_query = cursor.mogrify(sql, params).decode('utf-8', 'ignore')
                        logs.append(f"    -> Executing SQL: {log_query[:500]}...")
                    except Exception as log_err:
                        logs.append(f"    -> Failed to mogrify query for logging: {log_err}")

                    cursor.execute(sql, params)

                    result = cursor.fetchone()
                    if result:
                        logs.append(f"    -> æ–°è¦æŠ€è¡“è€…ã‚’ç™»éŒ²: ã€{name}ã€ (ID: {result['id']})")
                    else:
                        logs.append(f"    -> âš ï¸ æŠ€è¡“è€…ã€{name}ã€ã®DBç™»éŒ²ã«å¤±æ•—ã€ã¾ãŸã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚")
                
            conn.commit()
    except Exception as e:
        logs.append(f"âŒ DBä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        import traceback
        logs.append(traceback.format_exc())
        return False, logs
        
    logs.append("  > âœ… ä¿å­˜å®Œäº†ï¼")
    return True, logs



# ==============================================================================
# 2. ãƒãƒƒãƒå‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯
# ==============================================================================

def fetch_and_process_emails_batch():
    mail = None
    try:
        secrets, config = load_secrets(), load_app_config()
        if not secrets: return
        configure_genai()
        FETCH_LIMIT = config.get("email_processing", {}).get("fetch_limit", 10)
        SERVER, USER, PASSWORD = secrets.get("EMAIL_SERVER"), secrets.get("EMAIL_USER"), secrets.get("EMAIL_PASSWORD")
        if not all([SERVER, USER, PASSWORD]):
            print("âŒ ãƒ¡ãƒ¼ãƒ«ã‚µãƒ¼ãƒãƒ¼ã®æ¥ç¶šæƒ…å ±ãŒ secrets.toml ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return
        
        try:
            mail = imaplib.IMAP4_SSL(SERVER)
            mail.login(USER, PASSWORD)
            mail.select('inbox')
            print("âœ… ãƒ¡ãƒ¼ãƒ«ã‚µãƒ¼ãƒãƒ¼ã¸ã®æ¥ç¶šå®Œäº†")
        except Exception as e:
            print(f"âŒ ãƒ¡ãƒ¼ãƒ«ã‚µãƒ¼ãƒãƒ¼ã¸ã®æ¥ç¶šã¾ãŸã¯ãƒ­ã‚°ã‚¤ãƒ³ã«å¤±æ•—: {e}")
            return

        _, messages = mail.search(None, 'UNSEEN')
        email_ids = messages[0].split()
        
        if not email_ids:
            print("â„¹ï¸ å‡¦ç†å¯¾è±¡ã®æœªèª­ãƒ¡ãƒ¼ãƒ«ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            latest_ids = email_ids[::-1][:FETCH_LIMIT]
            checked_count, total_processed_count = len(latest_ids), 0
            print(f"â„¹ï¸ æœ€æ–°ã®æœªèª­ãƒ¡ãƒ¼ãƒ« {checked_count}ä»¶ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚ï¼ˆè¨­å®šä¸Šé™: {FETCH_LIMIT}ä»¶ï¼‰")

            for i, email_id in enumerate(latest_ids):
                print(f"\n--- ({i+1}/{checked_count}) ãƒ¡ãƒ¼ãƒ«ID {email_id.decode()} ã‚’å‡¦ç†ä¸­ ---")
                _, msg_data = mail.fetch(email_id, '(RFC822)')
                for response_part in msg_data:
                    if isinstance(response_part, tuple):
                        msg = email.message_from_bytes(response_part[1])
                        source_data = get_email_contents(msg)
                        success, logs = process_single_email_core(source_data)
                        for log_line in logs: print(log_line)
                        if success:
                            total_processed_count += 1
                            mail.store(email_id, '+FLAGS', '\\Seen')
            
            print(f"\n--- ãƒã‚§ãƒƒã‚¯å®Œäº† ---")
            print(f"â–¶ï¸ å‡¦ç†æ¸ˆã¿ãƒ¡ãƒ¼ãƒ«: {total_processed_count}ä»¶ / ãƒã‚§ãƒƒã‚¯ã—ãŸãƒ¡ãƒ¼ãƒ«: {checked_count}ä»¶")

    except Exception as e:
        print(f"âŒ ãƒ¡ãƒ¼ãƒ«å‡¦ç†å…¨ä½“ã§äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        traceback.print_exc()
    finally:
        if mail and mail.state == 'SELECTED':
            mail.close()
            mail.logout()
            print("â„¹ï¸ ãƒ¡ãƒ¼ãƒ«ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰åˆ‡æ–­ã—ã¾ã—ãŸã€‚")

# ==============================================================================
# 3. ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
# ==============================================================================

def main():
    start_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print(f"--- [ {start_time} ] å®šæœŸãƒ¡ãƒ¼ãƒ«å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ ---")
    fetch_and_process_emails_batch()
    end_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print(f"--- [ {end_time} ] å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ ---")

if __name__ == "__main__":
    main()
