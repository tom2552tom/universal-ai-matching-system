import streamlit as st
import psycopg2
from psycopg2.extras import DictCursor
import faiss
from sentence_transformers import SentenceTransformer
import numpy as np
import os
import google.generativeai as genai
import json
from datetime import datetime
import imaplib
import email
from email.header import decode_header, make_header
from email.utils import parsedate_to_datetime
import io
import contextlib
import toml
import fitz
import docx
import re # ã‚¹ã‚­ãƒ«æŠ½å‡ºã®ãŸã‚ã« re ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import json


# --- 1. åˆæœŸè¨­å®šã¨å®šæ•° (å¤‰æ›´ãªã—) ---
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=API_KEY)
except (KeyError, Exception):
    st.error("`secrets.toml` ã« `GOOGLE_API_KEY` ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    st.stop()

JOB_INDEX_FILE = "backend_job_index.faiss"
ENGINEER_INDEX_FILE = "backend_engineer_index.faiss"
MODEL_NAME = 'intfloat/multilingual-e5-large'
TOP_K_CANDIDATES = 500
MIN_SCORE_THRESHOLD = 70.0

# --- é–¢æ•°å®šç¾© ---
@st.cache_data
def load_app_config():
    try:
        with open("config.toml", "r", encoding="utf-8") as f:
            return toml.load(f)
    except FileNotFoundError:
        return {"app": {"title": "Universal AI Agent (Default)"}, "messages": {"sales_staff_notice": ""}}
    except Exception as e:
        print(f"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return {"app": {"title": "Universal AI Agent (Error)"}, "messages": {"sales_staff_notice": ""}}

@st.cache_resource
def load_embedding_model():
    try:
        return SentenceTransformer(MODEL_NAME)
    except Exception as e:
        st.error(f"åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ« '{MODEL_NAME}' ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}"); return None

def get_db_connection():
    try:
        db_url = st.secrets["DATABASE_URL"]
        return psycopg2.connect(db_url, cursor_factory=DictCursor)
    except KeyError:
        st.error("`secrets.toml` ã« `DATABASE_URL` ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"); st.stop()
    except psycopg2.OperationalError as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ: {e}"); st.stop()
    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"); st.stop()

def init_database():
    conn = get_db_connection()
    try:
        with conn.cursor() as cursor:
            cursor.execute('CREATE TABLE IF NOT EXISTS jobs (id SERIAL PRIMARY KEY, project_name TEXT, document TEXT NOT NULL, source_data_json TEXT, created_at TEXT, assigned_user_id INTEGER, is_hidden INTEGER NOT NULL DEFAULT 0, received_at TIMESTAMP WITH TIME ZONE)')
            cursor.execute('CREATE TABLE IF NOT EXISTS engineers (id SERIAL PRIMARY KEY, name TEXT, document TEXT NOT NULL, source_data_json TEXT, created_at TEXT, assigned_user_id INTEGER, is_hidden INTEGER NOT NULL DEFAULT 0, received_at TIMESTAMP WITH TIME ZONE)')
            cursor.execute("CREATE TABLE IF NOT EXISTS users (id SERIAL PRIMARY KEY, username TEXT NOT NULL UNIQUE, email TEXT, created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP);")
            cursor.execute('''CREATE TABLE IF NOT EXISTS matching_results (id SERIAL PRIMARY KEY, job_id INTEGER NOT NULL, engineer_id INTEGER NOT NULL, score REAL NOT NULL, created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP, is_hidden INTEGER DEFAULT 0, grade TEXT, positive_points TEXT, concern_points TEXT, proposal_text TEXT, status TEXT DEFAULT 'æ–°è¦', FOREIGN KEY (job_id) REFERENCES jobs (id) ON DELETE CASCADE, FOREIGN KEY (engineer_id) REFERENCES engineers (id) ON DELETE CASCADE, UNIQUE (job_id, engineer_id))''')
            cursor.execute("SELECT COUNT(*) FROM users")
            if cursor.fetchone()[0] == 0:
                print("åˆå›èµ·å‹•ã®ãŸã‚ã€ãƒ†ã‚¹ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’è¿½åŠ ã—ã¾ã™...")
                users_to_add = [('ç†Šå´', 'yamada@example.com'), ('å²©æœ¬', 'suzuki@example.com'), ('å°é–¢', 'sato@example.com'), ('å†…å±±', 'sato@example.com'), ('å³¶ç”°', 'sato@example.com'), ('é•·è°·å·', 'sato@example.com'), ('åŒ—å³¶', 'sato@example.com'), ('å²©å´', 'sato@example.com'), ('æ ¹å²¸', 'sato@example.com'), ('æ·»ç”°', 'sato@example.com'), ('å±±æµ¦', 'sato@example.com'), ('ç¦ç”°', 'sato@example.com')]
                cursor.executemany("INSERT INTO users (username, email) VALUES (%s, %s)", users_to_add)
                print(" -> ãƒ†ã‚¹ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚")
            def get_columns(table_name):
                cursor.execute("SELECT column_name FROM information_schema.columns WHERE table_schema = 'public' AND table_name = %s", (table_name,))
                return [row['column_name'] for row in cursor.fetchall()]
            job_columns = get_columns('jobs')
            if 'assigned_user_id' not in job_columns: cursor.execute("ALTER TABLE jobs ADD COLUMN assigned_user_id INTEGER REFERENCES users(id)")
            if 'is_hidden' not in job_columns: cursor.execute("ALTER TABLE jobs ADD COLUMN is_hidden INTEGER NOT NULL DEFAULT 0")
            if 'received_at' not in job_columns: cursor.execute("ALTER TABLE jobs ADD COLUMN received_at TIMESTAMP WITH TIME ZONE")
            engineer_columns = get_columns('engineers')
            if 'assigned_user_id' not in engineer_columns: cursor.execute("ALTER TABLE engineers ADD COLUMN assigned_user_id INTEGER REFERENCES users(id)")
            if 'is_hidden' not in engineer_columns: cursor.execute("ALTER TABLE engineers ADD COLUMN is_hidden INTEGER NOT NULL DEFAULT 0")
            if 'received_at' not in engineer_columns: cursor.execute("ALTER TABLE engineers ADD COLUMN received_at TIMESTAMP WITH TIME ZONE")
            match_columns = get_columns('matching_results')
            if 'positive_points' not in match_columns: cursor.execute("ALTER TABLE matching_results ADD COLUMN positive_points TEXT")
            if 'concern_points' not in match_columns: cursor.execute("ALTER TABLE matching_results ADD COLUMN concern_points TEXT")
            if 'status' not in match_columns: cursor.execute("ALTER TABLE matching_results ADD COLUMN status TEXT DEFAULT 'æ–°è¦'")
        conn.commit()
        print("Database initialized and schema verified successfully for PostgreSQL.")
    except (Exception, psycopg2.Error) as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"); conn.rollback()
    finally:
        if conn: conn.close()

def get_extraction_prompt(doc_type, text_content):
    if doc_type == 'engineer':
        return f"""
            ã‚ãªãŸã¯ã€ITäººæã®ã€Œã‚¹ã‚­ãƒ«ã‚·ãƒ¼ãƒˆã€ã‚„ã€Œè·å‹™çµŒæ­´æ›¸ã€ã‚’èª­ã¿è§£ãå°‚é–€å®¶ã§ã™ã€‚
            ã‚ãªãŸã®ä»•äº‹ã¯ã€ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰**å˜ä¸€ã®æŠ€è¡“è€…æƒ…å ±**ã‚’æŠ½å‡ºã—ã€æŒ‡å®šã•ã‚ŒãŸJSONå½¢å¼ã§æ•´ç†ã™ã‚‹ã“ã¨ã§ã™ã€‚
            # çµ¶å¯¾çš„ãªãƒ«ãƒ¼ãƒ«
            - å‡ºåŠ›ã¯ã€æŒ‡å®šã•ã‚ŒãŸJSONå½¢å¼ã®æ–‡å­—åˆ—ã®ã¿ã¨ã—ã€å‰å¾Œã«è§£èª¬ã‚„```json ```ã®ã‚ˆã†ãªã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®å›²ã¿ã‚’å«ã‚ãªã„ã§ãã ã•ã„ã€‚
            # æŒ‡ç¤º
            - ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã¯ã€ä¸€äººã®æŠ€è¡“è€…ã®æƒ…å ±ã§ã™ã€‚è¤‡æ•°ã®æ¥­å‹™çµŒæ­´ãŒå«ã¾ã‚Œã¦ã„ã¦ã‚‚ã€ãã‚Œã‚‰ã¯ã™ã¹ã¦ã“ã®ä¸€äººã®æŠ€è¡“è€…ã®çµŒæ­´ã¨ã—ã¦è¦ç´„ã—ã¦ãã ã•ã„ã€‚
            - `document`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ã¯ã€æŠ€è¡“è€…ã®ã‚¹ã‚­ãƒ«ã€çµŒé¨“ã€è‡ªå·±PRãªã©ã‚’ç·åˆçš„ã«è¦ç´„ã—ãŸã€æ¤œç´¢ã—ã‚„ã™ã„è‡ªç„¶ãªæ–‡ç« ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
            - `document`ã®æ–‡ç« ã®å…ˆé ­ã«ã¯ã€å¿…ãšæŠ€è¡“è€…åã‚’å«ã‚ã¦ãã ã•ã„ã€‚ä¾‹ï¼šã€Œå®Ÿå‹™çµŒé¨“15å¹´ã®TKæ°ã€‚Java(SpringBoot)ã‚’ä¸»è»¸ã«...ã€
            # å…·ä½“ä¾‹
            ## å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ:
            æ°å: å±±ç”° å¤ªéƒ
            å¹´é½¢: 35æ­³
            å¾—æ„æŠ€è¡“: Java, Spring
            è‡ªå·±PR: Webã‚¢ãƒ—ãƒªé–‹ç™ºãŒå¾—æ„ã§ã™ã€‚
            ## å‡ºåŠ›JSON:
            {{"engineers": [{{"name": "å±±ç”° å¤ªéƒ", "document": "35æ­³ã®å±±ç”°å¤ªéƒæ°ã€‚Java, Springã‚’ç”¨ã„ãŸWebã‚¢ãƒ—ãƒªé–‹ç™ºãŒå¾—æ„ã€‚", "main_skills": "Java, Spring"}}]}}
            # JSONå‡ºåŠ›å½¢å¼
            {{"engineers": [{{"name": "æŠ€è¡“è€…ã®æ°åã‚’æŠ½å‡º", "document": "æŠ€è¡“è€…ã®ã‚¹ã‚­ãƒ«ã‚„çµŒæ­´ã®è©³ç´°ã‚’ã€æ¤œç´¢ã—ã‚„ã™ã„ã‚ˆã†ã«è¦ç´„", "nationality": "å›½ç±ã‚’æŠ½å‡º", "availability_date": "ç¨¼åƒå¯èƒ½æ—¥ã‚’æŠ½å‡º", "desired_location": "å¸Œæœ›å‹¤å‹™åœ°ã‚’æŠ½å‡º", "desired_salary": "å¸Œæœ›å˜ä¾¡ã‚’æŠ½å‡º", "main_skills": "ä¸»è¦ãªã‚¹ã‚­ãƒ«ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§æŠ½å‡º"}}]}}
            # æœ¬ç•ª: ä»¥ä¸‹ã®ã‚¹ã‚­ãƒ«ã‚·ãƒ¼ãƒˆã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„
            ---
            {text_content}
        """
    elif doc_type == 'job':
        return f"""
            ã‚ãªãŸã¯ã€ITæ¥­ç•Œã®ã€Œæ¡ˆä»¶å®šç¾©æ›¸ã€ã‚’èª­ã¿è§£ãå°‚é–€å®¶ã§ã™ã€‚
            ã‚ãªãŸã®ä»•äº‹ã¯ã€ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰**æ¡ˆä»¶æƒ…å ±**ã‚’æŠ½å‡ºã—ã€æŒ‡å®šã•ã‚ŒãŸJSONå½¢å¼ã§æ•´ç†ã™ã‚‹ã“ã¨ã§ã™ã€‚
            ãƒ†ã‚­ã‚¹ãƒˆå†…ã«è¤‡æ•°ã®æ¡ˆä»¶æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãã‚Œãã‚Œã‚’å€‹åˆ¥ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦ãƒªã‚¹ãƒˆã«ã—ã¦ãã ã•ã„ã€‚
            # çµ¶å¯¾çš„ãªãƒ«ãƒ¼ãƒ«
            - å‡ºåŠ›ã¯ã€æŒ‡å®šã•ã‚ŒãŸJSONå½¢å¼ã®æ–‡å­—åˆ—ã®ã¿ã¨ã—ã€å‰å¾Œã«è§£èª¬ã‚„```json ```ã®ã‚ˆã†ãªã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®å›²ã¿ã‚’å«ã‚ãªã„ã§ãã ã•ã„ã€‚
            # æŒ‡ç¤º
            - `document`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ã¯ã€æ¡ˆä»¶ã®ã‚¹ã‚­ãƒ«ã‚„æ¥­å‹™å†…å®¹ã®è©³ç´°ã‚’ã€å¾Œã§æ¤œç´¢ã—ã‚„ã™ã„ã‚ˆã†ã«è‡ªç„¶ãªæ–‡ç« ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚
            - `document`ã®æ–‡ç« ã®å…ˆé ­ã«ã¯ã€å¿…ãšãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’å«ã‚ã¦ãã ã•ã„ã€‚ä¾‹ï¼šã€Œç¤¾å†…SEãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å¢—å“¡æ¡ˆä»¶ã€‚è¨­è¨ˆã€ãƒ†ã‚¹ãƒˆ...ã€
            # JSONå‡ºåŠ›å½¢å¼
            {{"jobs": [{{"project_name": "æ¡ˆä»¶åã‚’æŠ½å‡º", "document": "æ¡ˆä»¶ã®ã‚¹ã‚­ãƒ«ã‚„æ¥­å‹™å†…å®¹ã®è©³ç´°ã‚’ã€æ¤œç´¢ã—ã‚„ã™ã„ã‚ˆã†ã«è¦ç´„", "nationality_requirement": "å›½ç±è¦ä»¶ã‚’æŠ½å‡º", "start_date": "é–‹å§‹æ™‚æœŸã‚’æŠ½å‡º", "location": "å‹¤å‹™åœ°ã‚’æŠ½å‡º", "unit_price": "å˜ä¾¡ã‚„äºˆç®—ã‚’æŠ½å‡º", "required_skills": "å¿…é ˆã‚¹ã‚­ãƒ«ã‚„æ­“è¿ã‚¹ã‚­ãƒ«ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§æŠ½å‡º"}}]}}
            # æœ¬ç•ª: ä»¥ä¸‹ã®æ¡ˆä»¶æƒ…å ±ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„
            ---
            {text_content}
        """
    return ""

# â–¼â–¼â–¼ã€ã“ã“ãŒä¿®æ­£ç®‡æ‰€ã€‘â–¼â–¼â–¼
def split_text_with_llm(text_content):
    """ã€äºŒæ®µéšå‡¦ç†ã€‘1. æ–‡æ›¸ã‚’åˆ†é¡ã—ã€2. åˆ†é¡çµæœã«å¿œã˜ã¦å°‚ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§æƒ…å ±æŠ½å‡ºã‚’è¡Œã†ã€‚"""
    classification_prompt = f"""
        ã‚ãªãŸã¯ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆãŒã€Œæ¡ˆä»¶æƒ…å ±ã€ã€ŒæŠ€è¡“è€…æƒ…å ±ã€ã€Œãã®ä»–ã€ã®ã©ã‚Œã«æœ€ã‚‚å½“ã¦ã¯ã¾ã‚‹ã‹åˆ¤æ–­ã—ã€æŒ‡å®šã•ã‚ŒãŸå˜èªä¸€ã¤ã ã‘ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
        # åˆ¤æ–­åŸºæº–
        - ã€Œã‚¹ã‚­ãƒ«ã‚·ãƒ¼ãƒˆã€ã€Œè·å‹™çµŒæ­´æ›¸ã€ã€Œæ°åã€ã€Œå¹´é½¢ã€ã¨ã„ã£ãŸå˜èªãŒå«ã¾ã‚Œã¦ã„ã‚Œã°ã€ŒæŠ€è¡“è€…æƒ…å ±ã€ã®å¯èƒ½æ€§ãŒé«˜ã„ã€‚
        - ã€Œå‹Ÿé›†ã€ã€Œå¿…é ˆã‚¹ã‚­ãƒ«ã€ã€Œæ­“è¿ã‚¹ã‚­ãƒ«ã€ã€Œæ±‚ã‚ã‚‹äººç‰©åƒã€ã¨ã„ã£ãŸå˜èªãŒå«ã¾ã‚Œã¦ã„ã‚Œã°ã€Œæ¡ˆä»¶æƒ…å ±ã€ã®å¯èƒ½æ€§ãŒé«˜ã„ã€‚
        # å›ç­”å½¢å¼
        - `æ¡ˆä»¶æƒ…å ±`
        - `æŠ€è¡“è€…æƒ…å ±`
        - `ãã®ä»–`
        # åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
        ---
        {text_content[:2000]}
        ---
    """
    try:
        model = genai.GenerativeModel('models/gemini-2.5-flash-lite')
        st.write("ğŸ“„ æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã‚’åˆ†é¡ä¸­...")
        response = model.generate_content(classification_prompt)
        doc_type = response.text.strip()
        st.write(f"âœ… AIã«ã‚ˆã‚‹åˆ†é¡çµæœ: **{doc_type}**")
    except Exception as e:
        st.error(f"æ–‡æ›¸ã®åˆ†é¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"); return None

    if "æŠ€è¡“è€…æƒ…å ±" in doc_type:
        extraction_prompt = get_extraction_prompt('engineer', text_content)
    elif "æ¡ˆä»¶æƒ…å ±" in doc_type:
        extraction_prompt = get_extraction_prompt('job', text_content)
    else:
        st.warning("ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã¯æ¡ˆä»¶æƒ…å ±ã¾ãŸã¯æŠ€è¡“è€…æƒ…å ±ã¨ã—ã¦åˆ†é¡ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"); return None

    generation_config = {"response_mime_type": "application/json"}
    safety_settings = {'HARM_CATEGORY_HARASSMENT': 'BLOCK_NONE', 'HARM_CATEGORY_HATE_SPEECH': 'BLOCK_NONE', 'HARM_CATEGORY_SEXUALLY_EXPLICIT': 'BLOCK_NONE', 'HARM_CATEGORY_DANGEROUS_CONTENT': 'BLOCK_NONE'}
    
    try:
        with st.spinner("AIãŒæƒ…å ±ã‚’æ§‹é€ åŒ–ä¸­..."):
            response = model.generate_content(extraction_prompt, generation_config=generation_config, safety_settings=safety_settings)
        
        raw_text = response.text
        
        # â–¼â–¼â–¼ã€ã“ã“ã‹ã‚‰ãŒJSONæŠ½å‡ºãƒ»ä¿®å¾©ãƒ­ã‚¸ãƒƒã‚¯ã§ã™ã€‘â–¼â–¼â–¼
        json_str = None
        try:
            # 1. ```json ... ``` å½¢å¼ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¢ã™
            match = re.search(r'```json\s*(\{.*?\})\s*```', raw_text, re.DOTALL)
            if match:
                json_str = match.group(1)
            else:
                # 2. ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãŒãªã„å ´åˆã€æœ€ã‚‚å¤§ããªæ³¢æ‹¬å¼§ã®ãƒšã‚¢ã‚’æ¢ã™
                start_index = raw_text.find('{')
                end_index = raw_text.rfind('}')
                if start_index != -1 and end_index != -1 and start_index < end_index:
                    json_str = raw_text[start_index : end_index + 1]
                else:
                    st.error("LLMã®å¿œç­”ã‹ã‚‰æœ‰åŠ¹ãªJSONæ§‹é€ ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                    st.code(raw_text, language='text')
                    return None
            
            # æŠ½å‡ºã—ãŸæ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹
            parsed_json = json.loads(json_str)

        except json.JSONDecodeError as e:
            # ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã€ä¿®å¾©ã‚’è©¦ã¿ã‚‹
            print(f"WARN: JSONã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã€‚ä¿®å¾©ã‚’è©¦ã¿ã¾ã™ã€‚ã‚¨ãƒ©ãƒ¼: {e}")
            
            repaired_text = json_str or raw_text
            # æ–‡å­—åˆ—å†…ã®ä¸æ­£ãªæ”¹è¡Œã‚’ç½®æ›
            repaired_text = re.sub(r'(?<!\\)\n', r'\\n', repaired_text)
            
            try:
                # ä¿®å¾©ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã§å†åº¦ãƒ‘ãƒ¼ã‚¹ã‚’è©¦ã¿ã‚‹
                print("INFO: ä¿®å¾©å¾Œã®JSONã§å†ãƒ‘ãƒ¼ã‚¹ã‚’è©¦ã¿ã¾ã™ã€‚")
                parsed_json = json.loads(repaired_text)
            except json.JSONDecodeError as final_e:
                st.error(f"JSONã®ä¿®å¾©å¾Œã‚‚ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ: {final_e}")
                st.write("ä¿®å¾©ã‚’è©¦ã¿ãŸJSONæ–‡å­—åˆ—:")
                st.code(repaired_text, language='json')
                return None
        # â–²â–²â–²ã€JSONæŠ½å‡ºãƒ»ä¿®å¾©ãƒ­ã‚¸ãƒƒã‚¯ã“ã“ã¾ã§ã€‘â–²â–²â–²

        if "æŠ€è¡“è€…æƒ…å ±" in doc_type: parsed_json["jobs"] = []
        elif "æ¡ˆä»¶æƒ…å ±" in doc_type: parsed_json["engineers"] = []
        return parsed_json
        
    except Exception as e:
        st.error(f"LLMã«ã‚ˆã‚‹æ§‹é€ åŒ–å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}");
        try: st.code(response.text, language='text')
        except NameError: st.text("ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å–å¾—ã«ã‚‚å¤±æ•—ã—ã¾ã—ãŸã€‚")
        return None
    

@st.cache_data
def get_match_summary_with_llm(job_doc, engineer_doc):
    model = genai.GenerativeModel('models/gemini-2.5-flash-lite')
    # â–¼â–¼â–¼ å¤‰æ›´ç‚¹ 1: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å¼·åŒ– â–¼â–¼â–¼
    prompt = f"""
        ã‚ãªãŸã¯ã€çµŒé¨“è±Šå¯ŒãªITäººæç´¹ä»‹ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚
        ã‚ãªãŸã®ä»•äº‹ã¯ã€æç¤ºã•ã‚ŒãŸã€Œæ¡ˆä»¶æƒ…å ±ã€ã¨ã€ŒæŠ€è¡“è€…æƒ…å ±ã€ã‚’æ¯”è¼ƒã—ã€å®¢è¦³çš„ã‹ã¤å…·ä½“çš„ãªãƒãƒƒãƒãƒ³ã‚°è©•ä¾¡ã‚’è¡Œã†ã“ã¨ã§ã™ã€‚
        
        # çµ¶å¯¾çš„ãªãƒ«ãƒ¼ãƒ«
        - å‡ºåŠ›ã¯ã€å¿…ãšæŒ‡å®šã•ã‚ŒãŸJSONå½¢å¼ã®æ–‡å­—åˆ—ã®ã¿ã¨ã—ã¦ãã ã•ã„ã€‚è§£èª¬ã‚„ ```json ``` ã®ã‚ˆã†ãªå›²ã¿ã¯çµ¶å¯¾ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚
        - JSONå†…ã®ã™ã¹ã¦ã®æ–‡å­—åˆ—ã¯ã€å¿…ãšãƒ€ãƒ–ãƒ«ã‚¯ã‚©ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ `"` ã§å›²ã£ã¦ãã ã•ã„ã€‚
        - æ–‡å­—åˆ—ã®é€”ä¸­ã§æ”¹è¡Œã—ãªã„ã§ãã ã•ã„ã€‚æ”¹è¡ŒãŒå¿…è¦ãªå ´åˆã¯ `\\n` ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
        - `summary`ã¯æœ€ã‚‚é‡è¦ãªé …ç›®ã§ã™ã€‚çµ¶å¯¾ã«çœç•¥ã›ãšã€å¿…ãšS, A, B, C, Dã®ã„ãšã‚Œã‹ã®æ–‡å­—åˆ—ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
        
        # æŒ‡ç¤º
        ä»¥ä¸‹ã®2ã¤ã®æƒ…å ±ã‚’åˆ†æã—ã€ãƒã‚¸ãƒ†ã‚£ãƒ–ãªç‚¹ã¨æ‡¸å¿µç‚¹ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ãã ã•ã„ã€‚æœ€çµ‚çš„ã«ã€ç·åˆè©•ä¾¡ï¼ˆsummaryï¼‰ã‚’S, A, B, C, Dã®5æ®µéšã§åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
        - S: å®Œç’§ãªãƒãƒƒãƒ, A: éå¸¸ã«è‰¯ã„ãƒãƒƒãƒ, B: è‰¯ã„ãƒãƒƒãƒ, C: æ¤œè¨ã®ä½™åœ°ã‚ã‚Š, D: ãƒŸã‚¹ãƒãƒƒãƒ
        
        # JSONå‡ºåŠ›å½¢å¼
        {{"summary": "S, A, B, C, Dã®ã„ãšã‚Œã‹", "positive_points": ["ã‚¹ã‚­ãƒ«é¢ã§ã®åˆè‡´ç‚¹"], "concern_points": ["ã‚¹ã‚­ãƒ«é¢ã§ã®æ‡¸å¿µç‚¹"]}}
        ---
        # æ¡ˆä»¶æƒ…å ±
        {job_doc}
        ---
        # æŠ€è¡“è€…æƒ…å ±
        {engineer_doc}
        ---
    """
    # â–²â–²â–² å¤‰æ›´ç‚¹ 1 ã“ã“ã¾ã§ â–²â–²â–²

    generation_config = {"response_mime_type": "application/json"}
    safety_settings = {'HARM_CATEGORY_HARASSMENT': 'BLOCK_NONE', 'HARM_CATEGORY_HATE_SPEECH': 'BLOCK_NONE', 'HARM_CATEGORY_SEXUALLY_EXPLICIT': 'BLOCK_NONE', 'HARM_CATEGORY_DANGEROUS_CONTENT': 'BLOCK_NONE'}
    try:
        with st.spinner("AIãŒãƒãƒƒãƒãƒ³ã‚°æ ¹æ‹ ã‚’åˆ†æä¸­..."):
            response = model.generate_content(prompt, generation_config=generation_config, safety_settings=safety_settings)
        raw_text = response.text
        start_index = raw_text.find('{'); end_index = raw_text.rfind('}')
        if start_index != -1 and end_index != -1 and start_index < end_index:
            json_str = raw_text[start_index : end_index + 1]
            return json.loads(json_str)
        else:
            st.error("è©•ä¾¡ã®åˆ†æä¸­ã«LLMãŒæœ‰åŠ¹ãªJSONã‚’è¿”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚"); st.code(raw_text); return None
    except Exception as e:
        st.error(f"æ ¹æ‹ ã®åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}"); return None

def update_index(index_path, items):
    embedding_model = load_embedding_model()
    if not embedding_model or not items: return
    dimension = embedding_model.get_sentence_embedding_dimension()
    index_map = faiss.IndexIDMap(faiss.IndexFlatIP(dimension))
    ids = np.array([item['id'] for item in items], dtype=np.int64)
    bodies = [str(item['document']).split('\n---\n', 1)[-1] for item in items]
    texts_with_prefix = ["passage: " + body for body in bodies]
    embeddings = embedding_model.encode(texts_with_prefix, normalize_embeddings=True, show_progress_bar=False)
    index_map.add_with_ids(embeddings, ids)
    faiss.write_index(index_map, index_path)

def search(query_text, index_path, top_k=5):
    embedding_model = load_embedding_model()
    if not embedding_model or not os.path.exists(index_path): return [], []
    index = faiss.read_index(index_path)
    if index.ntotal == 0: return [], []
    query_body = query_text.split('\n---\n', 1)[-1]
    prefixed_query = "query: " + query_body
    query_vector = embedding_model.encode([prefixed_query], normalize_embeddings=True).reshape(1, -1)
    similarities, ids = index.search(query_vector, min(top_k, index.ntotal))
    valid_ids = [int(i) for i in ids[0] if i != -1]
    valid_similarities = [similarities[0][j] for j, i in enumerate(ids[0]) if i != -1]
    return valid_similarities, valid_ids

def get_records_by_ids(table_name, ids):
    if not ids: return []
    with get_db_connection() as conn:
        with conn.cursor() as cursor:
            query = f"SELECT * FROM {table_name} WHERE id = ANY(%s)"
            cursor.execute(query, (ids,))
            results = cursor.fetchall()
            results_map = {res['id']: res for res in results}
            return [results_map[id] for id in ids if id in results_map]


def _extract_skills_from_document(document: str, item_type: str) -> set:
    """
    documentã®ãƒ¡ã‚¿æƒ…å ±ã‹ã‚‰ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆã‚’æŠ½å‡ºã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã€‚
    """
    if not document:
        return set()

    # æ¡ˆä»¶ã®å ´åˆã¯ã€Œå¿…é ˆã‚¹ã‚­ãƒ«ã€ã€æŠ€è¡“è€…ã®å ´åˆã¯ã€Œä¸»è¦ã‚¹ã‚­ãƒ«ã€ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«ã™ã‚‹
    key = "å¿…é ˆã‚¹ã‚­ãƒ«" if item_type == 'job' else "ä¸»è¦ã‚¹ã‚­ãƒ«"
    
    # [ã‚­ãƒ¼: å€¤] ã®å½¢å¼ã§ãƒ¡ã‚¿æƒ…å ±ã‚’æ­£è¦è¡¨ç¾ã§æ¤œç´¢
    match = re.search(rf"\[{key}:\s*([^\]]+)\]", document)
    if not match:
        return set()

    # æŠ½å‡ºã—ãŸã‚¹ã‚­ãƒ«æ–‡å­—åˆ—ã‚’æ•´å½¢
    skills_str = match.group(1).strip()
    if not skills_str or skills_str.lower() in ['ä¸æ˜', 'none']:
        return set()
    
    # ã‚«ãƒ³ãƒã‚„å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã§åŒºåˆ‡ã‚Šã€å„ã‚¹ã‚­ãƒ«ã‚’å°æ–‡å­—åŒ–ãƒ»ç©ºç™½é™¤å»ã—ã¦ã‚»ãƒƒãƒˆã«æ ¼ç´
    skills = {skill.strip().lower() for skill in re.split(r'[,ã€ï¼Œ\s]+', skills_str) if skill.strip()}
    return skills

# backend.py ã® run_matching_for_item é–¢æ•°ã‚’ä¿®æ­£

def run_matching_for_item(item_data, item_type, conn, now_str):
    # â–¼â–¼â–¼ã€ã“ã®é–¢æ•°å…¨ä½“ã‚’ç½®ãæ›ãˆã¦ãã ã•ã„ã€‘â–¼â–¼â–¼
    with conn.cursor() as cursor:
        # 1. æ¤œç´¢å¯¾è±¡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€ãƒ†ãƒ¼ãƒ–ãƒ«ã€åç§°ã‚’æ±ºå®š
        if item_type == 'job':
            query_text, index_path = item_data['document'], ENGINEER_INDEX_FILE
            target_table_name = 'engineers'
            source_name = item_data.get('project_name', f"æ¡ˆä»¶ID:{item_data['id']}")
        else:
            query_text, index_path = item_data['document'], JOB_INDEX_FILE
            target_table_name = 'jobs'
            source_name = item_data.get('name', f"æŠ€è¡“è€…ID:{item_data['id']}")

        search_limit = TOP_K_CANDIDATES * 2

        # 2. Faissã«ã‚ˆã‚‹é¡ä¼¼åº¦æ¤œç´¢ã‚’å®Ÿè¡Œ
        similarities, ids = search(query_text, index_path, top_k=search_limit)
        if not ids:
            st.write(f"â–¶ ã€{source_name}ã€(ID:{item_data['id']}, {item_type}) ã®é¡ä¼¼å€™è£œã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return

        # 3. æ¤œç´¢çµæœã®å€™è£œãƒ‡ãƒ¼ã‚¿ã‚’DBã‹ã‚‰ä¸€æ‹¬å–å¾—
        candidate_records = get_records_by_ids(target_table_name, ids)
        candidate_map = {record['id']: record for record in candidate_records}

        # â–¼â–¼â–¼ å¤‰æ›´ç‚¹ 1: æœ€åˆã®ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¿®æ­£ â–¼â–¼â–¼
        st.write(f"â–¶ ã€{source_name}ã€(ID:{item_data['id']}, {item_type}) ã®é¡ä¼¼å€™è£œ **{len(ids)}ä»¶** ã‚’ç™ºè¦‹ã€‚ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¾ã™...")
        # â–²â–²â–² å¤‰æ›´ç‚¹ 1 ã“ã“ã¾ã§ â–²â–²â–²

        source_skills = _extract_skills_from_document(item_data['document'], item_type)
        if not source_skills:
            st.write(f"  - æ¤œç´¢å…ƒã€{source_name}ã€ã®ã‚¹ã‚­ãƒ«æƒ…å ±ãŒæŠ½å‡ºã§ããªã‹ã£ãŸãŸã‚ã€äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

        # ã‚¹ãƒ†ãƒƒãƒ—A: ã‚¹ã‚­ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’è¡Œã„ã€æœ‰åŠ¹ãªå€™è£œãƒªã‚¹ãƒˆã‚’ä½œæˆã™ã‚‹
        valid_candidates = []
        for sim, candidate_id in zip(similarities, ids):
            if len(valid_candidates) >= TOP_K_CANDIDATES:
                break

            candidate_record = candidate_map.get(candidate_id)
            if not candidate_record:
                continue
            
            candidate_name = candidate_record.get('project_name') or candidate_record.get('name') or f"ID:{candidate_id}"

            if source_skills:
                candidate_item_type = 'engineer' if item_type == 'job' else 'job'
                candidate_skills = _extract_skills_from_document(candidate_record['document'], candidate_item_type)
                
                if not source_skills.intersection(candidate_skills):
                    # ã“ã®ãƒ­ã‚°ã¯è©³ç´°ã™ãã‚‹å ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦ã‚‚è‰¯ã„
                    # st.write(f"  - å€™è£œ: ã€{candidate_name}ã€ -> ã‚¹ã‚­ãƒ«ä¸ä¸€è‡´ã®ãŸã‚äº‹å‰é™¤å¤–ã€‚")
                    continue
            
            valid_candidates.append({
                'sim': sim,
                'id': candidate_id,
                'record': candidate_record,
                'name': candidate_name
            })
        
        # â–¼â–¼â–¼ å¤‰æ›´ç‚¹ 2: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®çµæœãƒ­ã‚°ã‚’ä¿®æ­£ â–¼â–¼â–¼
        if not valid_candidates:
            st.write(f"âœ… ã‚¹ã‚­ãƒ«ãŒä¸€è‡´ã™ã‚‹æœ‰åŠ¹ãªå€™è£œã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            return

        st.write(f"âœ… ã‚¹ã‚­ãƒ«ãŒä¸€è‡´ã—ãŸæœ‰åŠ¹ãªå€™è£œ **{len(valid_candidates)}ä»¶** ã«çµã‚Šè¾¼ã¿ã¾ã—ãŸã€‚AIè©•ä¾¡ã‚’é–‹å§‹ã—ã¾ã™...")
        # â–²â–²â–² å¤‰æ›´ç‚¹ 2 ã“ã“ã¾ã§ â–²â–²â–²

        # ã‚¹ãƒ†ãƒƒãƒ—B: æœ‰åŠ¹ãªå€™è£œãƒªã‚¹ãƒˆã«å¯¾ã—ã¦AIè©•ä¾¡ã¨DBä¿å­˜ã‚’è¡Œã†
        for candidate_info in valid_candidates:
            score = float(candidate_info['sim']) * 100

            if score < MIN_SCORE_THRESHOLD:
                continue

            # 5. LLMè©•ä¾¡ã®ãŸã‚ã®æ¡ˆä»¶ãƒ»æŠ€è¡“è€…æƒ…å ±ã‚’æº–å‚™
            if item_type == 'job':
                job_doc, engineer_doc = item_data['document'], candidate_info['record']['document']
                job_id, engineer_id = item_data['id'], candidate_info['id']
            else:
                job_doc, engineer_doc = candidate_info['record']['document'], item_data['document']
                job_id, engineer_id = candidate_info['id'], item_data['id']

            # 6. LLMã«ã‚ˆã‚‹ãƒãƒƒãƒãƒ³ã‚°è©•ä¾¡ã‚’å®Ÿè¡Œ
            llm_result = get_match_summary_with_llm(job_doc, engineer_doc)

            # 7. LLMã®è©•ä¾¡çµæœã«åŸºã¥ã„ã¦DBã¸ã®ä¿å­˜ã‚’åˆ¤æ–­
            if llm_result and 'summary' in llm_result:
                grade = llm_result.get('summary')
                positive_points = json.dumps(llm_result.get('positive_points', []), ensure_ascii=False)
                concern_points = json.dumps(llm_result.get('concern_points', []), ensure_ascii=False)

                if grade in ['S', 'A', 'B']:
                    try:
                        cursor.execute(
                            'INSERT INTO matching_results (job_id, engineer_id, score, created_at, grade, positive_points, concern_points) VALUES (%s, %s, %s, %s, %s, %s, %s) ON CONFLICT (job_id, engineer_id) DO NOTHING',
                            (job_id, engineer_id, score, now_str, grade, positive_points, concern_points)
                        )
                        st.write(f"  - å€™è£œ: ã€{candidate_info['name']}ã€ -> ãƒãƒƒãƒãƒ³ã‚°è©•ä¾¡: **{grade}** (ã‚¹ã‚³ã‚¢: {score:.2f}) ... âœ… DBã«ä¿å­˜")
                    except Exception as e:
                        st.write(f"  - DBä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                else:
                    st.write(f"  - å€™è£œ: ã€{candidate_info['name']}ã€ -> ãƒãƒƒãƒãƒ³ã‚°è©•ä¾¡: **{grade}** (ã‚¹ã‚³ã‚¢: {score:.2f}) ... âŒ ã‚¹ã‚­ãƒƒãƒ—")
            else:
                st.write(f"  - å€™è£œ: ã€{candidate_info['name']}ã€ -> LLMè©•ä¾¡å¤±æ•—ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")



# backend.py

def process_single_content(source_data: dict, progress_bar, base_progress: float, progress_per_email: float):
    """
    å˜ä¸€ã®ãƒ¡ãƒ¼ãƒ«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å‡¦ç†ã—ã€é€²æ—ãƒãƒ¼ã‚’æ›´æ–°ã™ã‚‹ã€‚
    
    Args:
        source_data (dict): ãƒ¡ãƒ¼ãƒ«ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã€‚
        progress_bar: Streamlitã®ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
        base_progress (float): ã“ã®ãƒ¡ãƒ¼ãƒ«å‡¦ç†é–‹å§‹å‰ã®é€²æ—å€¤ã€‚
        progress_per_email (float): ã“ã®ãƒ¡ãƒ¼ãƒ«1ä»¶ã‚ãŸã‚Šã®é€²æ—ã®é‡ã¿ã€‚
    """
    if not source_data: 
        st.warning("å‡¦ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")
        return False

    # ã‚¹ãƒ†ãƒƒãƒ—1: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è§£æ (LLM) - ã“ã®ãƒ¡ãƒ¼ãƒ«å‡¦ç†ã®50%ã‚’å ã‚ã‚‹ã¨ä»®å®š
    valid_attachments_content = [f"\n\n--- æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«: {att['filename']} ---\n{att.get('content', '')}" for att in source_data.get('attachments', []) if att.get('content') and not att.get('content', '').startswith("[") and not att.get('content', '').endswith("]")]
    if valid_attachments_content: 
        st.write(f"â„¹ï¸ {len(valid_attachments_content)}ä»¶ã®æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’è§£æã«å«ã‚ã¾ã™ã€‚")
    full_text_for_llm = source_data.get('body', '') + "".join(valid_attachments_content)
    if not full_text_for_llm.strip(): 
        st.warning("è§£æå¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return False
    
    # split_text_with_llm ã¯å†…éƒ¨ã§ã‚¹ãƒ”ãƒŠãƒ¼ã‚„ãƒ­ã‚°ã‚’è¡¨ç¤ºã™ã‚‹
    parsed_data = split_text_with_llm(full_text_for_llm)
    
    # é€²æ—ãƒãƒ¼ã‚’æ›´æ–° (ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è§£æå®Œäº†)
    # ã“ã®ãƒ¡ãƒ¼ãƒ«ã«å‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸé€²æ—ã®ã†ã¡ã€50%ãŒå®Œäº†ã—ãŸã¨ã¿ãªã™
    current_progress = base_progress + (progress_per_email * 0.5)
    progress_bar.progress(current_progress, text="ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è§£æå®Œäº†")

    if not parsed_data: 
        return False
    
    new_jobs_data, new_engineers_data = parsed_data.get("jobs", []), parsed_data.get("engineers", [])
    if not new_jobs_data and not new_engineers_data: 
        st.warning("LLMã¯ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ¡ˆä»¶æƒ…å ±ã¾ãŸã¯æŠ€è¡“è€…æƒ…å ±ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        return False
    
    # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒãƒƒãƒãƒ³ã‚°å‡¦ç† - ã“ã®ãƒ¡ãƒ¼ãƒ«å‡¦ç†ã®æ®‹ã‚Šã®50%
    st.write("ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ›´æ–°ã—ã€ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
    with get_db_connection() as conn:
        with conn.cursor() as cursor:
            now_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            received_at_dt = source_data.get('received_at')
            json_data_to_store = source_data.copy()
            if isinstance(json_data_to_store.get('received_at'), datetime):
                json_data_to_store['received_at'] = json_data_to_store['received_at'].isoformat()
            source_json_str = json.dumps(json_data_to_store, ensure_ascii=False, indent=2)

            newly_added_jobs, newly_added_engineers = [], []
            
            for item_data in new_jobs_data:
                doc = item_data.get("document") or full_text_for_llm
                project_name = item_data.get("project_name", "åç§°æœªå®šã®æ¡ˆä»¶")
                meta_info = _build_meta_info_string('job', item_data)
                full_document = meta_info + doc
                cursor.execute('INSERT INTO jobs (project_name, document, source_data_json, created_at, received_at) VALUES (%s, %s, %s, %s, %s) RETURNING id', (project_name, full_document, source_json_str, now_str, received_at_dt))
                item_data['id'] = cursor.fetchone()[0]
                item_data['document'] = full_document
                newly_added_jobs.append(item_data)
            
            for item_data in new_engineers_data:
                doc = item_data.get("document") or full_text_for_llm
                engineer_name = item_data.get("name", "åç§°ä¸æ˜ã®æŠ€è¡“è€…")
                meta_info = _build_meta_info_string('engineer', item_data)
                full_document = meta_info + doc
                cursor.execute('INSERT INTO engineers (name, document, source_data_json, created_at, received_at) VALUES (%s, %s, %s, %s, %s) RETURNING id', (engineer_name, full_document, source_json_str, now_str, received_at_dt))
                item_data['id'] = cursor.fetchone()[0]
                item_data['document'] = full_document
                newly_added_engineers.append(item_data)
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°
            cursor.execute('SELECT id, document FROM jobs WHERE is_hidden = 0'); all_active_jobs = cursor.fetchall()
            cursor.execute('SELECT id, document FROM engineers WHERE is_hidden = 0'); all_active_engineers = cursor.fetchall()
            if all_active_jobs: update_index(JOB_INDEX_FILE, all_active_jobs)
            if all_active_engineers: update_index(ENGINEER_INDEX_FILE, all_active_engineers)
            
            # å†ãƒãƒƒãƒãƒ³ã‚° (run_matching_for_item ã¯å†…éƒ¨ã§ãƒ­ã‚°ã‚’å‡ºåŠ›ã™ã‚‹)
            for new_job in newly_added_jobs:
                run_matching_for_item(new_job, 'job', conn, now_str)
            for new_engineer in newly_added_engineers:
                run_matching_for_item(new_engineer, 'engineer', conn, now_str)
        conn.commit()

    # é€²æ—ãƒãƒ¼ã‚’æ›´æ–° (ã“ã®ãƒ¡ãƒ¼ãƒ«ã®å‡¦ç†ãŒ100%å®Œäº†)
    current_progress = base_progress + progress_per_email
    progress_bar.progress(current_progress, text="ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†å®Œäº†")
    
    return True





def extract_text_from_pdf(file_bytes):
    try:
        with fitz.open(stream=file_bytes, filetype="pdf") as doc:
            text = "".join(page.get_text() for page in doc)
        return text if text.strip() else "[PDFãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¤±æ•—: å†…å®¹ãŒç©ºã¾ãŸã¯ç”»åƒPDF]"
    except Exception as e:
        return f"[PDFãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}]"

def extract_text_from_docx(file_bytes):
    try:
        doc = docx.Document(io.BytesIO(file_bytes))
        text = "\n".join([para.text for para in doc.paragraphs])
        return text if text.strip() else "[DOCXãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¤±æ•—: å†…å®¹ãŒç©º]"
    except Exception as e:
        return f"[DOCXãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}]"

def get_email_contents(msg) -> dict:
    subject = str(make_header(decode_header(msg["subject"]))) if msg["subject"] else ""
    from_ = str(make_header(decode_header(msg["from"]))) if msg["from"] else ""
    received_at = parsedate_to_datetime(msg["Date"]) if msg["Date"] else None

    body_text, attachments = "", []
    if msg.is_multipart():
        for part in msg.walk():
            content_type, content_disposition = part.get_content_type(), str(part.get("Content-Disposition"))
            if 'text/plain' in content_type and 'attachment' not in content_disposition:
                charset = part.get_content_charset()
                try: body_text += part.get_payload(decode=True).decode(charset or 'utf-8', errors='ignore')
                except Exception: body_text += part.get_payload(decode=True).decode('utf-8', errors='ignore')
            if 'attachment' in content_disposition and (raw_filename := part.get_filename()):
                filename = str(make_header(decode_header(raw_filename)))
                st.write(f"ğŸ“„ æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚")
                file_bytes, lower_filename = part.get_payload(decode=True), filename.lower()
                if lower_filename.endswith(".pdf"): attachments.append({"filename": filename, "content": extract_text_from_pdf(file_bytes)})
                elif lower_filename.endswith(".docx"): attachments.append({"filename": filename, "content": extract_text_from_docx(file_bytes)})
                elif lower_filename.endswith(".txt"): attachments.append({"filename": filename, "content": file_bytes.decode('utf-8', errors='ignore')})
                else: st.write(f"â„¹ï¸ æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã¯æœªå¯¾å¿œã®å½¢å¼ã®ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
    else:
        charset = msg.get_content_charset()
        try: body_text = msg.get_payload(decode=True).decode(charset or 'utf-8', errors='ignore')
        except Exception: body_text = msg.get_payload(decode=True).decode('utf-8', errors='ignore')
    
    return {"subject": subject, "from": from_, "received_at": received_at, "body": body_text.strip(), "attachments": attachments}




# backend.py

def fetch_and_process_emails():
    try:
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®åˆæœŸåŒ–ã¨é‡ã¿ä»˜ã‘å®šç¾©
        progress_bar = st.progress(0, text="å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
        
        WEIGHT_CONNECT = 0.05  # ã‚µãƒ¼ãƒãƒ¼æ¥ç¶šã«5%
        WEIGHT_FETCH_IDS = 0.05 # ãƒ¡ãƒ¼ãƒ«IDãƒªã‚¹ãƒˆå–å¾—ã«5%
        WEIGHT_LOOP = 0.90     # ãƒ¡ãƒ¼ãƒ«ã”ã¨ã®ãƒ«ãƒ¼ãƒ—å‡¦ç†å…¨ä½“ã§90%

        # ãƒ¡ãƒ¼ãƒ«ã‚µãƒ¼ãƒãƒ¼æ¥ç¶š
        try:
            SERVER, USER, PASSWORD = st.secrets["EMAIL_SERVER"], st.secrets["EMAIL_USER"], st.secrets["EMAIL_PASSWORD"]
        except KeyError as e:
            st.error(f"ãƒ¡ãƒ¼ãƒ«ã‚µãƒ¼ãƒãƒ¼ã®æ¥ç¶šæƒ…å ±ãŒSecretsã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“: {e}")
            return False, ""
        
        try:
            mail = imaplib.IMAP4_SSL(SERVER)
            mail.login(USER, PASSWORD)
            mail.select('inbox')
            progress_bar.progress(WEIGHT_CONNECT, text="ãƒ¡ãƒ¼ãƒ«ã‚µãƒ¼ãƒãƒ¼æ¥ç¶šå®Œäº†")
        except Exception as e:
            st.error(f"ãƒ¡ãƒ¼ãƒ«ã‚µãƒ¼ãƒãƒ¼ã¸ã®æ¥ç¶šã¾ãŸã¯ãƒ­ã‚°ã‚¤ãƒ³ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return False, ""
        
        total_processed_count, checked_count = 0, 0
        try:
            with st.status("æœ€æ–°ã®æœªèª­ãƒ¡ãƒ¼ãƒ«ã‚’å–å¾—ãƒ»å‡¦ç†ä¸­...", expanded=True) as status:
                _, messages = mail.search(None, 'UNSEEN')
                email_ids = messages[0].split()
                
                progress_bar.progress(WEIGHT_CONNECT + WEIGHT_FETCH_IDS, text="æœªèª­ãƒ¡ãƒ¼ãƒ«IDãƒªã‚¹ãƒˆå–å¾—å®Œäº†")
                
                if not email_ids:
                    st.write("å‡¦ç†å¯¾è±¡ã®æœªèª­ãƒ¡ãƒ¼ãƒ«ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                else:
                    latest_ids = email_ids[::-1][:10]
                    checked_count = len(latest_ids)
                    st.write(f"æœ€æ–°ã®æœªèª­ãƒ¡ãƒ¼ãƒ« {checked_count}ä»¶ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚")

                    # ãƒ¡ãƒ¼ãƒ«1ä»¶ã‚ãŸã‚Šã®é€²æ—ã®å‰²åˆã‚’è¨ˆç®—
                    progress_per_email = WEIGHT_LOOP / checked_count if checked_count > 0 else 0
                    
                    for i, email_id in enumerate(latest_ids):
                        # ã“ã®ãƒ«ãƒ¼ãƒ—é–‹å§‹æ™‚ç‚¹ã§ã®ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹é€²æ—
                        base_progress_for_this_email = (WEIGHT_CONNECT + WEIGHT_FETCH_IDS) + (i * progress_per_email)
                        
                        # ãƒ¡ãƒ¼ãƒ«å†…å®¹å–å¾—ã®é€²æ—
                        progress_bar.progress(base_progress_for_this_email, text=f"ãƒ¡ãƒ¼ãƒ«({i+1}/{checked_count})ã®å†…å®¹ã‚’å–å¾—ä¸­...")
                        
                        _, msg_data = mail.fetch(email_id, '(RFC822)')
                        for response_part in msg_data:
                            if isinstance(response_part, tuple):
                                msg = email.message_from_bytes(response_part[1])
                                source_data = get_email_contents(msg)
                                
                                # ãƒ¡ãƒ¼ãƒ«å†…å®¹å–å¾—å®Œäº†å¾Œã®é€²æ— (ãƒ¡ãƒ¼ãƒ«1ä»¶ã®å‡¦ç†ã®20%ã‚’å‰²ã‚Šå½“ã¦)
                                fetch_complete_progress = base_progress_for_this_email + (progress_per_email * 0.2)
                                progress_bar.progress(fetch_complete_progress, text=f"ãƒ¡ãƒ¼ãƒ«({i+1}/{checked_count})ã®å†…å®¹å–å¾—å®Œäº†")

                                if source_data['body'] or source_data['attachments']:
                                    st.write("---")
                                    st.write(f"âœ… ãƒ¡ãƒ¼ãƒ«ID {email_id.decode()} ã‚’å‡¦ç†ã—ã¾ã™ã€‚")
                                    received_at_str = source_data['received_at'].strftime('%Y-%m-%d %H:%M:%S') if source_data.get('received_at') else 'å–å¾—ä¸å¯'
                                    st.write(f"   å—ä¿¡æ—¥æ™‚: {received_at_str}")
                                    st.write(f"   å·®å‡ºäºº: {source_data.get('from', 'å–å¾—ä¸å¯')}")
                                    st.write(f"   ä»¶å: {source_data.get('subject', 'å–å¾—ä¸å¯')}")
                                    
                                    # process_single_content ã«é€²æ—ç®¡ç†æƒ…å ±ã‚’æ¸¡ã™
                                    # æ®‹ã‚Šã®80%ã®é€²æ—ã‚’ã“ã®é–¢æ•°ã«å§”ã­ã‚‹
                                    if process_single_content(source_data, progress_bar, fetch_complete_progress, progress_per_email * 0.8):
                                        total_processed_count += 1
                                        mail.store(email_id, '+FLAGS', '\\Seen')
                                else:
                                    st.write(f"âœ–ï¸ ãƒ¡ãƒ¼ãƒ«ID {email_id.decode()} ã¯æœ¬æ–‡ã‚‚æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ç„¡ã„ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                                    # ã‚¹ã‚­ãƒƒãƒ—ã—ãŸå ´åˆã§ã‚‚ã€ã“ã®ãƒ¡ãƒ¼ãƒ«ã®é€²æ—ã¯å®Œäº†ã—ãŸã“ã¨ã«ã™ã‚‹
                                    final_progress_for_this_email = base_progress_for_this_email + progress_per_email
                                    progress_bar.progress(final_progress_for_this_email, text=f"ãƒ¡ãƒ¼ãƒ«({i+1}/{checked_count}) ã‚¹ã‚­ãƒƒãƒ—å®Œäº†")
                        
                        st.write(f"({i+1}/{checked_count}) ãƒã‚§ãƒƒã‚¯å®Œäº†")
                
                status.update(label="ãƒ¡ãƒ¼ãƒ«ãƒã‚§ãƒƒã‚¯å®Œäº†", state="complete")
        finally:
            mail.close()
            mail.logout()
    
        # æœ€çµ‚çš„ã«ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’100%ã«ã™ã‚‹
        progress_bar.progress(1.0, text="å…¨å‡¦ç†å®Œäº†ï¼")
        
        # å‡¦ç†å®Œäº†å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        if checked_count > 0:
            if total_processed_count > 0:
                st.success(f"ãƒã‚§ãƒƒã‚¯ã—ãŸ {checked_count} ä»¶ã®ãƒ¡ãƒ¼ãƒ«ã®ã†ã¡ã€{total_processed_count} ä»¶ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã€ä¿å­˜ã—ã¾ã—ãŸã€‚")
                st.balloons()
            else:
                st.warning(f"ãƒ¡ãƒ¼ãƒ«ã‚’ {checked_count} ä»¶ãƒã‚§ãƒƒã‚¯ã—ã¾ã—ãŸãŒã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã§ãã‚‹æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            st.info("å‡¦ç†å¯¾è±¡ã¨ãªã‚‹æ–°ã—ã„æœªèª­ãƒ¡ãƒ¼ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            
        return True, "" # ãƒ­ã‚°ã‚¹ãƒˆãƒªãƒ¼ãƒ ã¯ä½¿ã‚ãªã„ã®ã§ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™
    except Exception as e:
        st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return False, ""





# --- æ®‹ã‚Šã®é–¢æ•° (å¤‰æ›´ãªã—) ---
def hide_match(result_id):
    if not result_id: st.warning("IDãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"); return False
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute('UPDATE matching_results SET is_hidden = 1 WHERE id = %s', (result_id,))
                if cursor.rowcount > 0: st.toast(f"ãƒãƒƒãƒãƒ³ã‚°çµæœ (ID: {result_id}) ã‚’éè¡¨ç¤ºã«ã—ã¾ã—ãŸã€‚"); conn.commit(); return True
                else: st.warning(f"ãƒãƒƒãƒãƒ³ã‚°çµæœ (ID: {result_id}) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"); return False
    except (Exception, psycopg2.Error) as e: st.error(f"DBæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}"); return False

def get_all_users():
    with get_db_connection() as conn:
        with conn.cursor() as cursor:
            cursor.execute("SELECT id, username FROM users ORDER BY id"); return cursor.fetchall()

def assign_user_to_job(job_id, user_id):
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cur: cur.execute("UPDATE jobs SET assigned_user_id = %s WHERE id = %s", (user_id, job_id))
            conn.commit(); return True
        except (Exception, psycopg2.Error) as e: print(f"æ‹…å½“è€…å‰²ã‚Šå½“ã¦ã‚¨ãƒ©ãƒ¼: {e}"); conn.rollback(); return False

def set_job_visibility(job_id, is_hidden):
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cur: cur.execute("UPDATE jobs SET is_hidden = %s WHERE id = %s", (is_hidden, job_id))
            conn.commit(); return True
        except (Exception, psycopg2.Error) as e: print(f"è¡¨ç¤ºçŠ¶æ…‹ã®æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}"); conn.rollback(); return False

def assign_user_to_engineer(engineer_id, user_id):
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cur: cur.execute("UPDATE engineers SET assigned_user_id = %s WHERE id = %s", (user_id, engineer_id))
            conn.commit(); return True
        except (Exception, psycopg2.Error) as e: print(f"æŠ€è¡“è€…ã¸ã®æ‹…å½“è€…å‰²ã‚Šå½“ã¦ã‚¨ãƒ©ãƒ¼: {e}"); conn.rollback(); return False

def set_engineer_visibility(engineer_id, is_hidden):
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cur: cur.execute("UPDATE engineers SET is_hidden = %s WHERE id = %s", (is_hidden, engineer_id))
            conn.commit(); return True
        except (Exception, psycopg2.Error) as e: print(f"æŠ€è¡“è€…ã®è¡¨ç¤ºçŠ¶æ…‹ã®æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}"); conn.rollback(); return False

def update_engineer_source_json(engineer_id, new_json_str):
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cur: cur.execute("UPDATE engineers SET source_data_json = %s WHERE id = %s", (new_json_str, engineer_id))
            conn.commit(); return True
        except (Exception, psycopg2.Error) as e: print(f"æŠ€è¡“è€…ã®JSONãƒ‡ãƒ¼ã‚¿æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}"); conn.rollback(); return False

def generate_proposal_reply_with_llm(job_summary, engineer_summary, engineer_name, project_name):
    if not all([job_summary, engineer_summary, engineer_name, project_name]): return "æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€ææ¡ˆãƒ¡ãƒ¼ãƒ«ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
    prompt = f"""
        ã‚ãªãŸã¯ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«å„ªç§€ãªæŠ€è¡“è€…ã‚’ææ¡ˆã™ã‚‹ã€çµŒé¨“è±Šå¯ŒãªITå–¶æ¥­æ‹…å½“è€…ã§ã™ã€‚
        ä»¥ä¸‹ã®æ¡ˆä»¶æƒ…å ±ã¨æŠ€è¡“è€…æƒ…å ±ã‚’ã‚‚ã¨ã«ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å¿ƒã«éŸ¿ãã€ä¸å¯§ã§èª¬å¾—åŠ›ã®ã‚ã‚‹ææ¡ˆãƒ¡ãƒ¼ãƒ«ã®æ–‡é¢ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
        # å½¹å‰²
        - å„ªç§€ãªITå–¶æ¥­æ‹…å½“è€…
        # æŒ‡ç¤º
        - æœ€åˆã«ã€ææ¡ˆã™ã‚‹æŠ€è¡“è€…åã¨æ¡ˆä»¶åã‚’è¨˜è¼‰ã—ãŸä»¶åã‚’ä½œæˆã—ã¦ãã ã•ã„ (ä¾‹: ä»¶å: ã€ã€‡ã€‡æ§˜ã®ã”ææ¡ˆã€‘ã€‡ã€‡ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä»¶)ã€‚
        - æŠ€è¡“è€…ã®ã‚¹ã‚­ãƒ«ã‚„çµŒé¨“ãŒã€æ¡ˆä»¶ã®ã©ã®è¦ä»¶ã«å…·ä½“çš„ã«ãƒãƒƒãƒã—ã¦ã„ã‚‹ã‹ã‚’æ˜ç¢ºã«ç¤ºã—ã¦ãã ã•ã„ã€‚
        - ãƒã‚¸ãƒ†ã‚£ãƒ–ãªç‚¹ï¼ˆé©åˆã‚¹ã‚­ãƒ«ï¼‰ã‚’å¼·èª¿ã—ã€æŠ€è¡“è€…ã®é­…åŠ›ã‚’æœ€å¤§é™ã«ä¼ãˆã¦ãã ã•ã„ã€‚
        - æ‡¸å¿µç‚¹ï¼ˆã‚¹ã‚­ãƒ«ãƒŸã‚¹ãƒãƒƒãƒã‚„çµŒé¨“ä¸è¶³ï¼‰ãŒã‚ã‚‹å ´åˆã¯ã€æ­£ç›´ã«è§¦ã‚Œã¤ã¤ã‚‚ã€å­¦ç¿’æ„æ¬²ã‚„é¡ä¼¼çµŒé¨“ã€ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ãªã©ã§ã©ã®ã‚ˆã†ã«ã‚«ãƒãƒ¼ã§ãã‚‹ã‹ã‚’å‰å‘ãã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
        - å…¨ä½“ã¨ã—ã¦ã€ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã‹ã¤ä¸å¯§ãªãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒ¼ãƒ«ã®ãƒˆãƒ¼ãƒ³ã‚’ç¶­æŒã—ã¦ãã ã•ã„ã€‚
        - æœ€å¾Œã«ã€ãœã²ä¸€åº¦ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§ã®é¢è«‡ã®æ©Ÿä¼šã‚’è¨­ã‘ã¦ã„ãŸã ã‘ã¾ã™ã‚ˆã†ãŠé¡˜ã„ã™ã‚‹ä¸€æ–‡ã§ç· ã‚ããã£ã¦ãã ã•ã„ã€‚
        - å‡ºåŠ›ã¯ã€ä»¶åã¨æœ¬æ–‡ã‚’å«ã‚“ã ãƒ¡ãƒ¼ãƒ«å½¢å¼ã®ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã¨ã—ã¦ãã ã•ã„ã€‚ä½™è¨ˆãªè§£èª¬ã¯ä¸è¦ã§ã™ã€‚
        # æ¡ˆä»¶æƒ…å ±
        {job_summary}
        # æŠ€è¡“è€…æƒ…å ±
        {engineer_summary}
        # ææ¡ˆã™ã‚‹æŠ€è¡“è€…ã®åå‰
        {engineer_name}
        # æ¡ˆä»¶å
        {project_name}
        ---
        ãã‚Œã§ã¯ã€ä¸Šè¨˜ã®æŒ‡ç¤ºã«åŸºã¥ã„ã¦ã€æœ€é©ãªææ¡ˆãƒ¡ãƒ¼ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
    """
    try:
        model = genai.GenerativeModel('models/gemini-2.5-flash-lite')
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        print(f"Error generating proposal reply with LLM: {e}"); return f"ææ¡ˆãƒ¡ãƒ¼ãƒ«ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

def save_match_grade(match_id, grade):
    if not grade: return False
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cursor: cursor.execute("UPDATE matching_results SET grade = %s WHERE id = %s", (grade, match_id))
            conn.commit(); return True
        except (Exception, psycopg2.Error) as e: print(f"Error saving match grade for match_id {match_id}: {e}"); conn.rollback(); return False

def get_evaluation_html(grade, font_size='2.5em'):
    if not grade: return ""
    color_map = {'S': '#00b894', 'A': '#28a745', 'B': '#17a2b8', 'C': '#ffc107', 'D': '#fd7e14', 'E': '#dc3545'}
    color = color_map.get(grade.upper(), '#6c757d') 
    style = f"color: {color}; font-size: {font_size}; font-weight: bold; text-align: center; line-height: 1; padding-top: 10px;"
    html_code = f"<div style='text-align: center; margin-bottom: 5px;'><span style='{style}'>{grade.upper()}</span></div><div style='text-align: center; font-size: 0.8em; color: #888;'>åˆ¤å®š</div>"
    return html_code

def get_matching_result_details(result_id):
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cursor:
                cursor.execute("SELECT * FROM matching_results WHERE id = %s", (result_id,))
                match_result = cursor.fetchone()
                if not match_result: return None
                cursor.execute("SELECT j.*, u.username as assignee_name FROM jobs j LEFT JOIN users u ON j.assigned_user_id = u.id WHERE j.id = %s", (match_result['job_id'],))
                job_data = cursor.fetchone()
                cursor.execute("SELECT e.*, u.username as assignee_name FROM engineers e LEFT JOIN users u ON e.assigned_user_id = u.id WHERE e.id = %s", (match_result['engineer_id'],))
                engineer_data = cursor.fetchone()
                return {"match_result": dict(match_result), "job_data": dict(job_data) if job_data else None, "engineer_data": dict(engineer_data) if engineer_data else None}
        except (Exception, psycopg2.Error) as e:
            print(f"ãƒãƒƒãƒãƒ³ã‚°è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}"); return None


def re_evaluate_existing_matches_for_engineer(engineer_id):
    """
    ã€ãƒ‘ã‚¿ãƒ¼ãƒ³Aã€‘
    æŒ‡å®šã•ã‚ŒãŸæŠ€è¡“è€…ã®æ—¢å­˜ã®ãƒãƒƒãƒãƒ³ã‚°çµæœã™ã¹ã¦ã«å¯¾ã—ã¦ã€AIè©•ä¾¡ã®ã¿ã‚’å†å®Ÿè¡Œã—ã€DBã‚’æ›´æ–°ã™ã‚‹ã€‚
    æ–°ã—ã„ãƒãƒƒãƒãƒ³ã‚°ã¯è¡Œã‚ãªã„ã€‚
    """
    if not engineer_id:
        st.error("æŠ€è¡“è€…IDãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return False

    conn = get_db_connection()
    try:
        with conn.cursor() as cursor:
            # 1. æŠ€è¡“è€…ã®æœ€æ–°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
            cursor.execute("SELECT document FROM engineers WHERE id = %s", (engineer_id,))
            engineer_record = cursor.fetchone()
            if not engineer_record:
                st.error(f"æŠ€è¡“è€…ID:{engineer_id} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                return False
            engineer_doc = engineer_record['document']

            # 2. ã“ã®æŠ€è¡“è€…ã«é–¢é€£ã™ã‚‹ã€è¡¨ç¤ºä¸­ã®ãƒãƒƒãƒãƒ³ã‚°çµæœã‚’å–å¾—
            cursor.execute(
                """
                SELECT r.id as match_id, j.id as job_id, j.document as job_document, j.project_name
                FROM matching_results r
                JOIN jobs j ON r.job_id = j.id
                WHERE r.engineer_id = %s AND r.is_hidden = 0 AND j.is_hidden = 0
                """,
                (engineer_id,)
            )
            existing_matches = cursor.fetchall()

            if not existing_matches:
                st.info("ã“ã®æŠ€è¡“è€…ã«ã¯å†è©•ä¾¡å¯¾è±¡ã®ãƒãƒƒãƒãƒ³ã‚°çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                return True # å‡¦ç†å¯¾è±¡ãŒãªã„ã®ã§æˆåŠŸã¨ã¿ãªã™

            st.write(f"{len(existing_matches)}ä»¶ã®æ—¢å­˜ãƒãƒƒãƒãƒ³ã‚°ã«å¯¾ã—ã¦å†è©•ä¾¡ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
            
            # 3. å„ãƒãƒƒãƒãƒ³ã‚°ã«å¯¾ã—ã¦AIè©•ä¾¡ã‚’å†å®Ÿè¡Œ
            success_count = 0
            for match in existing_matches:
                st.write(f"  - æ¡ˆä»¶ã€{match['project_name']}ã€ã¨ã®ãƒãƒƒãƒãƒ³ã‚°ã‚’å†è©•ä¾¡ä¸­...")
                
                # AIè©•ä¾¡ã‚’å‘¼ã³å‡ºã—
                llm_result = get_match_summary_with_llm(match['job_document'], engineer_doc)
                
                # DBã‚’æ›´æ–°
                if update_match_evaluation(match['match_id'], llm_result):
                    st.write(f"    -> æ–°ã—ã„è©•ä¾¡: **{llm_result.get('summary')}** ... âœ… æ›´æ–°å®Œäº†")
                    success_count += 1
                else:
                    st.write(f"    -> è©•ä¾¡ã¾ãŸã¯æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        
        # ã“ã®é–¢æ•°ã¯DBã®å¤‰æ›´ã‚’ä¼´ã‚ãªã„ã®ã§ã€conn.commit()ã¯ä¸è¦ (update_match_evaluationå†…ã§å®Œçµ)
        return success_count == len(existing_matches)

    except (Exception, psycopg2.Error) as e:
        st.error(f"å†è©•ä¾¡å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return False
    finally:
        if conn:
            conn.close()



def update_engineer_name(engineer_id, new_name):
    if not new_name or not new_name.strip(): return False
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cursor: cursor.execute("UPDATE engineers SET name = %s WHERE id = %s", (new_name.strip(), engineer_id))
            conn.commit(); return True
        except (Exception, psycopg2.Error) as e:
            print(f"æŠ€è¡“è€…æ°åã®æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}"); conn.rollback(); return False

def _build_meta_info_string(item_type, item_data):
    meta_fields = []
    if item_type == 'job':
        meta_fields = [["å›½ç±è¦ä»¶", "nationality_requirement"], ["é–‹å§‹æ™‚æœŸ", "start_date"], ["å‹¤å‹™åœ°", "location"], ["å˜ä¾¡", "unit_price"], ["å¿…é ˆã‚¹ã‚­ãƒ«", "required_skills"]]
    elif item_type == 'engineer':
        meta_fields = [["å›½ç±", "nationality"], ["ç¨¼åƒå¯èƒ½æ—¥", "availability_date"], ["å¸Œæœ›å‹¤å‹™åœ°", "desired_location"], ["å¸Œæœ›å˜ä¾¡", "desired_salary"], ["ä¸»è¦ã‚¹ã‚­ãƒ«", "main_skills"]]
    if not meta_fields: return "\n---\n"
    meta_parts = [f"[{display_name}: {item_data.get(key, 'ä¸æ˜')}]" for display_name, key in meta_fields]
    return " ".join(meta_parts) + "\n---\n"

def update_match_status(match_id, new_status):
    if not match_id or not new_status: return False
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cursor: cursor.execute("UPDATE matching_results SET status = %s WHERE id = %s", (new_status, match_id))
            conn.commit(); return True
        except (Exception, psycopg2.Error) as e:
            print(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}"); conn.rollback(); return False


def delete_job(job_id):
    """
    æŒ‡å®šã•ã‚ŒãŸæ¡ˆä»¶IDã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ jobs ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å‰Šé™¤ã™ã‚‹ã€‚
    ON DELETE CASCADE åˆ¶ç´„ã«ã‚ˆã‚Šã€é–¢é€£ã™ã‚‹ matching_results ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚‚è‡ªå‹•çš„ã«å‰Šé™¤ã•ã‚Œã‚‹ã€‚
    
    Args:
        job_id (int): å‰Šé™¤å¯¾è±¡ã®æ¡ˆä»¶IDã€‚
        
    Returns:
        bool: å‰Šé™¤ãŒæˆåŠŸã—ãŸå ´åˆã¯Trueã€å¤±æ•—ã—ãŸå ´åˆã¯Falseã€‚
    """
    if not job_id:
        print("å‰Šé™¤å¯¾è±¡ã®æ¡ˆä»¶IDãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return False
        
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cursor:
                # æ¡ˆä»¶è‡ªä½“ã‚’å‰Šé™¤ã™ã‚‹ (ON DELETE CASCADE ã«ã‚ˆã‚Šé–¢é€£ãƒ‡ãƒ¼ã‚¿ã‚‚å‰Šé™¤ã•ã‚Œã‚‹)
                cursor.execute("DELETE FROM jobs WHERE id = %s", (job_id,))
                deleted_rows = cursor.rowcount
                print(f"Deleted {deleted_rows} job record with id {job_id}.")
            
            conn.commit()
            
            # æ¡ˆä»¶ãŒ1ä»¶ä»¥ä¸Šå‰Šé™¤ã•ã‚ŒãŸã‚‰æˆåŠŸã¨ã¿ãªã™
            return deleted_rows > 0
            
        except (Exception, psycopg2.Error) as e:
            print(f"æ¡ˆä»¶å‰Šé™¤ä¸­ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            conn.rollback() # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯å¤‰æ›´ã‚’å…ƒã«æˆ»ã™
            return False
        

# backend.py ã®æœ«å°¾ã‚ãŸã‚Šã«è¿½åŠ 

def delete_engineer(engineer_id):
    """
    æŒ‡å®šã•ã‚ŒãŸæŠ€è¡“è€…IDã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ engineers ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å‰Šé™¤ã™ã‚‹ã€‚
    ON DELETE CASCADE åˆ¶ç´„ã«ã‚ˆã‚Šã€é–¢é€£ã™ã‚‹ matching_results ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚‚è‡ªå‹•çš„ã«å‰Šé™¤ã•ã‚Œã‚‹ã€‚
    
    Args:
        engineer_id (int): å‰Šé™¤å¯¾è±¡ã®æŠ€è¡“è€…IDã€‚
        
    Returns:
        bool: å‰Šé™¤ãŒæˆåŠŸã—ãŸå ´åˆã¯Trueã€å¤±æ•—ã—ãŸå ´åˆã¯Falseã€‚
    """
    if not engineer_id:
        print("å‰Šé™¤å¯¾è±¡ã®æŠ€è¡“è€…IDãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return False
        
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cursor:
                # æŠ€è¡“è€…è‡ªä½“ã‚’å‰Šé™¤ã™ã‚‹ (ON DELETE CASCADE ã«ã‚ˆã‚Šé–¢é€£ãƒ‡ãƒ¼ã‚¿ã‚‚å‰Šé™¤ã•ã‚Œã‚‹)
                cursor.execute("DELETE FROM engineers WHERE id = %s", (engineer_id,))
                deleted_rows = cursor.rowcount
                print(f"Deleted {deleted_rows} engineer record with id {engineer_id}.")
            
            conn.commit()
            
            # æŠ€è¡“è€…ãŒ1ä»¶ä»¥ä¸Šå‰Šé™¤ã•ã‚ŒãŸã‚‰æˆåŠŸã¨ã¿ãªã™
            return deleted_rows > 0
            
        except (Exception, psycopg2.Error) as e:
            print(f"æŠ€è¡“è€…å‰Šé™¤ä¸­ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            conn.rollback() # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯å¤‰æ›´ã‚’å…ƒã«æˆ»ã™
            return False

def update_job_source_json(job_id, new_json_str):
    """
    æ¡ˆä»¶ã®source_data_jsonã‚’æ›´æ–°ã™ã‚‹ã€‚
    """
    if not job_id or not new_json_str:
        return False
        
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cur:
                cur.execute("UPDATE jobs SET source_data_json = %s WHERE id = %s", (new_json_str, job_id))
            conn.commit()
            return True
        except (Exception, psycopg2.Error) as e:
            print(f"æ¡ˆä»¶ã®JSONãƒ‡ãƒ¼ã‚¿æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            conn.rollback()
            return False
        

def update_match_evaluation(match_id, llm_result):
    """
    æŒ‡å®šã•ã‚ŒãŸãƒãƒƒãƒãƒ³ã‚°IDã®è©•ä¾¡çµæœã‚’æ›´æ–°ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã€‚
    """
    if not llm_result or 'summary' not in llm_result:
        return False
        
    grade = llm_result.get('summary')
    positive_points = json.dumps(llm_result.get('positive_points', []), ensure_ascii=False)
    concern_points = json.dumps(llm_result.get('concern_points', []), ensure_ascii=False)
    
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cursor:
                cursor.execute(
                    "UPDATE matching_results SET grade = %s, positive_points = %s, concern_points = %s WHERE id = %s",
                    (grade, positive_points, concern_points, match_id)
                )
            conn.commit()
            return True
        except (Exception, psycopg2.Error) as e:
            print(f"ãƒãƒƒãƒãƒ³ã‚°è©•ä¾¡ã®æ›´æ–°ã‚¨ãƒ©ãƒ¼ (ID: {match_id}): {e}")
            conn.rollback()
            return False


def re_evaluate_and_match_single_engineer(engineer_id):
    """
    ã€ã‚¯ãƒªã‚¢ï¼†å†ãƒãƒƒãƒãƒ³ã‚°ã€‘
    æŒ‡å®šã•ã‚ŒãŸæŠ€è¡“è€…ã®documentã‚’æœ€æ–°åŒ–ã—ã€æ—¢å­˜ã®ãƒãƒƒãƒãƒ³ã‚°ã‚’ã‚¯ãƒªã‚¢å¾Œã€
    å†åº¦ã™ã¹ã¦ã®æ¡ˆä»¶ã¨ãƒãƒƒãƒãƒ³ã‚°ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
    """
    if not engineer_id:
        st.error("æŠ€è¡“è€…IDãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return False

    with get_db_connection() as conn:
        try:
            with conn.cursor() as cursor:
                # 1. æŠ€è¡“è€…ã®æœ€æ–°ã®source_data_jsonã‚’å–å¾—
                cursor.execute("SELECT source_data_json, name FROM engineers WHERE id = %s", (engineer_id,))
                engineer_record = cursor.fetchone()
                if not engineer_record or not engineer_record['source_data_json']:
                    st.error(f"æŠ€è¡“è€…ID:{engineer_id} ã®å…ƒæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                    return False
                
                st.write("ğŸ“„ å…ƒæƒ…å ±ã‹ã‚‰æœ€æ–°ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆã—ã¾ã™...")
                source_data = json.loads(engineer_record['source_data_json'])
                full_text_for_llm = source_data.get('body', '') + "".join([f"\n\n--- æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«: {att['filename']} ---\n{att.get('content', '')}" for att in source_data.get('attachments', []) if att.get('content') and not att.get('content', '').startswith("[") and not att.get('content', '').endswith("]")])
                
                # 2. split_text_with_llmã§documentã‚’å†ç”Ÿæˆ
                parsed_data = split_text_with_llm(full_text_for_llm)
                if not parsed_data or not parsed_data.get("engineers"):
                    st.error("LLMã«ã‚ˆã‚‹æƒ…å ±æŠ½å‡ºï¼ˆå†è©•ä¾¡ï¼‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                    return False
                
                item_data = parsed_data["engineers"][0]
                doc = item_data.get("document") or full_text_for_llm
                meta_info = _build_meta_info_string('engineer', item_data)
                new_full_document = meta_info + doc
                
                # 3. engineersãƒ†ãƒ¼ãƒ–ãƒ«ã®documentã‚’æ›´æ–°
                cursor.execute("UPDATE engineers SET document = %s WHERE id = %s", (new_full_document, engineer_id))
                st.write("âœ… æŠ€è¡“è€…ã®AIè¦ç´„æƒ…å ±ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚")

                # 4. æ—¢å­˜ã®ãƒãƒƒãƒãƒ³ã‚°çµæœã‚’å‰Šé™¤
                # ON DELETE CASCADEãŒè¨­å®šã•ã‚Œã¦ã„ã‚Œã°ä¸è¦ã ãŒã€å®‰å…¨ã®ãŸã‚æ˜ç¤ºçš„ã«å®Ÿè¡Œ
                cursor.execute("DELETE FROM matching_results WHERE engineer_id = %s", (engineer_id,))
                st.write(f"ğŸ—‘ï¸ æŠ€è¡“è€…ID:{engineer_id} ã®æ—¢å­˜ãƒãƒƒãƒãƒ³ã‚°çµæœã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")

                # 5. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å†æ§‹ç¯‰
                st.write("ğŸ”„ ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ›´æ–°ã—ã€å†ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
                cursor.execute('SELECT id, document FROM jobs WHERE is_hidden = 0'); all_active_jobs = cursor.fetchall()
                cursor.execute('SELECT id, document FROM engineers WHERE is_hidden = 0'); all_active_engineers = cursor.fetchall()
                if all_active_jobs: update_index(JOB_INDEX_FILE, all_active_jobs)
                if all_active_engineers: update_index(ENGINEER_INDEX_FILE, all_active_engineers)
                
                # 6. å†ãƒãƒƒãƒãƒ³ã‚°ã‚’å®Ÿè¡Œ
                now_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                engineer_data_for_matching = {
                    'id': engineer_id, 
                    'document': new_full_document, 
                    'name': engineer_record['name']
                }
                run_matching_for_item(engineer_data_for_matching, 'engineer', conn, now_str) # cursorã§ã¯ãªãconnã‚’æ¸¡ã™

            conn.commit()
            return True
        except (Exception, psycopg2.Error) as e:
            conn.rollback()
            st.error(f"å†è©•ä¾¡ãƒ»å†ãƒãƒƒãƒãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return False
        

def save_proposal_text(match_id, text):
    """
    æŒ‡å®šã•ã‚ŒãŸãƒãƒƒãƒãƒ³ã‚°IDã«å¯¾ã—ã¦ã€ç”Ÿæˆã•ã‚ŒãŸææ¡ˆãƒ¡ãƒ¼ãƒ«ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜ã—ã¾ã™ã€‚
    """
    if not match_id or text is None:
        return False
        
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cursor:
                cursor.execute(
                    "UPDATE matching_results SET proposal_text = %s WHERE id = %s",
                    (text, match_id)
                )
            conn.commit()
            return True
        except (Exception, psycopg2.Error) as e:
            print(f"Error saving proposal text for match_id {match_id}: {e}")
            conn.rollback()
            return False
        