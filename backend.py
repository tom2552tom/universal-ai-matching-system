import streamlit as st
import faiss
from sentence_transformers import SentenceTransformer
import numpy as np
import os
import google.generativeai as genai
import json
from datetime import datetime
import imaplib
import email
from email.header import decode_header
import io
import contextlib
import toml
import fitz
import docx
import psycopg2
from psycopg2.extras import DictCursor

# --- 1. åˆæœŸè¨­å®šã¨å®šæ•° ---
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=API_KEY)
except (KeyError, Exception):
    st.error("`secrets.toml` ã« `GOOGLE_API_KEY` ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    st.stop()

# Faissã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
JOB_INDEX_FILE = "backend_job_index.faiss"
ENGINEER_INDEX_FILE = "backend_engineer_index.faiss"
MODEL_NAME = 'intfloat/multilingual-e5-large'
TOP_K_CANDIDATES = 500
MIN_SCORE_THRESHOLD = 70.0

# --- 2. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã‚‹é–¢æ•° ---

@st.cache_data
def load_app_config():
    config_file_path = "config.toml"
    if not os.path.exists(config_file_path):
        st.warning(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« '{config_file_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return {}
    try:
        with open(config_file_path, "r", encoding="utf-8") as f:
            return toml.load(f)
    except Exception as e:
        st.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return {}

@st.cache_resource
def load_embedding_model():
    try:
        return SentenceTransformer(MODEL_NAME)
    except Exception as e:
        st.error(f"åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ« '{MODEL_NAME}' ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

# --- 3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š ---

def get_db_connection():
    """PostgreSQLãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®æ¥ç¶šã‚’å–å¾—ã—ã¾ã™ã€‚"""
    try:
        conn_string = st.secrets["DATABASE_URL"]
        conn = psycopg2.connect(conn_string)
        return conn
    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        st.info("Supabaseã®æ¥ç¶šæƒ…å ±ãŒStreamlitã®Secretsã«æ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.stop()

# --- 4. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ãƒ»ã‚¹ã‚­ãƒ¼ãƒç®¡ç† ---

def init_database():
    """PostgreSQLãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’åˆæœŸåŒ–ã™ã‚‹ã€‚"""
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cursor:
                
                def column_exists(table, column):
                    cursor.execute("""
                        SELECT 1 FROM information_schema.columns 
                        WHERE table_schema = 'public' AND table_name = %s AND column_name = %s
                    """, (table, column))
                    return cursor.fetchone() is not None

                # --- ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ ---
                cursor.execute('CREATE TABLE IF NOT EXISTS jobs (id SERIAL PRIMARY KEY, project_name TEXT, document TEXT NOT NULL, source_data_json TEXT, created_at TEXT, is_hidden INTEGER DEFAULT 0, assigned_user_id INTEGER)')
                cursor.execute('CREATE TABLE IF NOT EXISTS engineers (id SERIAL PRIMARY KEY, name TEXT, document TEXT NOT NULL, source_data_json TEXT, created_at TEXT, is_hidden INTEGER DEFAULT 0, assigned_user_id INTEGER)')
                cursor.execute('CREATE TABLE IF NOT EXISTS users (id SERIAL PRIMARY KEY, username TEXT NOT NULL UNIQUE, email TEXT, created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP)')
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS matching_results (
                        id SERIAL PRIMARY KEY,
                        job_id INTEGER REFERENCES jobs(id) ON DELETE CASCADE,
                        engineer_id INTEGER REFERENCES engineers(id) ON DELETE CASCADE,
                        score REAL,
                        created_at TEXT,
                        is_hidden INTEGER DEFAULT 0,
                        grade TEXT,
                        positive_points TEXT,
                        concern_points TEXT,
                        proposal_text TEXT,
                        status TEXT DEFAULT 'æ–°è¦',
                        UNIQUE (job_id, engineer_id)
                    )
                ''')

                # --- åˆå›ãƒ¦ãƒ¼ã‚¶ãƒ¼è¿½åŠ  ---
                cursor.execute("SELECT COUNT(*) FROM users")
                if cursor.fetchone()[0] == 0:
                    print("åˆå›èµ·å‹•ã®ãŸã‚ã€ãƒ†ã‚¹ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’è¿½åŠ ã—ã¾ã™...")
                    users_to_add = [
                        ('ç†Šå´', 'k@e.com'), ('å²©æœ¬', 'i@e.com'), ('å°é–¢', 'o@e.com'),
                        ('å†…å±±', 'u@e.com'), ('å³¶ç”°', 's@e.com'), ('é•·è°·å·', 'h@e.com'),
                        ('åŒ—å³¶', 'k@e.com'), ('å²©å´', 'i@e.com'), ('æ ¹å²¸', 'n@e.com'),
                        ('æ·»ç”°', 's@e.com'), ('å±±æµ¦', 'y@e.com'), ('ç¦ç”°', 'f@e.com')
                    ]
                    cursor.executemany("INSERT INTO users (username, email) VALUES (%s, %s)", users_to_add)
                    print(f" -> {len(users_to_add)}åã®ãƒ†ã‚¹ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚")
                
                # --- ã‚«ãƒ©ãƒ è¿½åŠ  (ä¸‹ä½äº’æ›æ€§ã®ãŸã‚) ---
                if not column_exists('jobs', 'assigned_user_id'): cursor.execute("ALTER TABLE jobs ADD COLUMN assigned_user_id INTEGER REFERENCES users(id)")
                if not column_exists('jobs', 'is_hidden'): cursor.execute("ALTER TABLE jobs ADD COLUMN is_hidden INTEGER NOT NULL DEFAULT 0")
                if not column_exists('engineers', 'assigned_user_id'): cursor.execute("ALTER TABLE engineers ADD COLUMN assigned_user_id INTEGER REFERENCES users(id)")
                if not column_exists('engineers', 'is_hidden'): cursor.execute("ALTER TABLE engineers ADD COLUMN is_hidden INTEGER NOT NULL DEFAULT 0")
                if not column_exists('matching_results', 'status'): cursor.execute("ALTER TABLE matching_results ADD COLUMN status TEXT DEFAULT 'æ–°è¦'")

            conn.commit()
            print("Database initialized and schema verified for PostgreSQL successfully.")
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

# --- 5. LLM & AIé–¢é€£ ---

def _build_meta_info_string(item_type, item_data):
    """ãƒ¡ã‚¿æƒ…å ±æ–‡å­—åˆ—ã‚’ç”Ÿæˆã™ã‚‹å…±é€šãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"""
    if item_type == 'job':
        meta_fields = [
            ["å›½ç±è¦ä»¶", "nationality_requirement"], ["é–‹å§‹æ™‚æœŸ", "start_date"], ["å‹¤å‹™åœ°", "location"],
            ["å˜ä¾¡", "unit_price"], ["å¿…é ˆã‚¹ã‚­ãƒ«", "required_skills"]
        ]
    elif item_type == 'engineer':
        meta_fields = [
            ["å›½ç±", "nationality"], ["ç¨¼åƒå¯èƒ½æ—¥", "availability_date"], ["å¸Œæœ›å‹¤å‹™åœ°", "desired_location"],
            ["å¸Œæœ›å˜ä¾¡", "desired_salary"], ["ä¸»è¦ã‚¹ã‚­ãƒ«", "main_skills"]
        ]
    else:
        return "\n---\n"
    meta_parts = [f"[{name}: {item_data.get(key, 'ä¸æ˜')}]" for name, key in meta_fields]
    return " ".join(meta_parts) + "\n---\n"

def split_text_with_llm(text_content):
    model = genai.GenerativeModel('models/gemini-2.5-flash-lite')
    try:
        with open('prompt.txt', 'r', encoding='utf-8') as f:
            prompt = f.read().replace('{text_content}', text_content)
    except FileNotFoundError:
        st.error("ã‚¨ãƒ©ãƒ¼: `prompt.txt` ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None
    
    generation_config = {"response_mime_type": "application/json"}
    safety_settings = {'HARM_CATEGORY_HARASSMENT': 'BLOCK_NONE', 'HARM_CATEGORY_HATE_SPEECH': 'BLOCK_NONE', 'HARM_CATEGORY_SEXUALLY_EXPLICIT': 'BLOCK_NONE', 'HARM_CATEGORY_DANGEROUS_CONTENT': 'BLOCK_NONE'}
    
    try:
        response = model.generate_content(prompt, generation_config=generation_config, safety_settings=safety_settings)
        raw_text = response.text
        start_index = raw_text.find('{')
        end_index = raw_text.rfind('}')
        if start_index != -1 and end_index != -1 and start_index < end_index:
            return json.loads(raw_text[start_index : end_index + 1])
        st.error("LLMã®å¿œç­”ã‹ã‚‰æœ‰åŠ¹ãªJSONå½¢å¼ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"); st.code(raw_text)
        return None
    except Exception as e:
        st.error(f"LLMã«ã‚ˆã‚‹æ§‹é€ åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

@st.cache_data
def get_match_summary_with_llm(_job_doc, _engineer_doc):
    model = genai.GenerativeModel('models/gemini-2.5-flash-lite')
    prompt = f"""
ã‚ãªãŸã¯ã€çµŒé¨“è±Šå¯Œã§éå¸¸ã«å„ªç§€ãªITäººæç´¹ä»‹ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚
ã‚ãªãŸã®ä»•äº‹ã¯ã€æç¤ºã•ã‚ŒãŸã€Œæ¡ˆä»¶æƒ…å ±ã€ã¨ã€ŒæŠ€è¡“è€…æƒ…å ±ã€ã‚’æ·±ãæ¯”è¼ƒåˆ†æã—ã€å˜ãªã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ä¸€è‡´ã§ã¯ãªãã€**å…·ä½“çš„ãªç†ç”±ã«åŸºã¥ã„ãŸ**å®¢è¦³çš„ãªãƒãƒƒãƒãƒ³ã‚°è©•ä¾¡ã‚’è¡Œã†ã“ã¨ã§ã™ã€‚
# çµ¶å¯¾çš„ãªãƒ«ãƒ¼ãƒ«
- **ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ç¾…åˆ—ã¯çµ¶å¯¾ã«ç¦æ­¢ã—ã¾ã™ã€‚** `positive_points` ã¨ `concern_points` ã«ã¯ã€å¿…ãš**å…·ä½“çš„ãªç†ç”±ã‚’èª¬æ˜ã™ã‚‹çŸ­ã„æ–‡ç« **ã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
- å„ãƒã‚¤ãƒ³ãƒˆã¯ã€ãªãœãã‚ŒãŒãƒã‚¸ãƒ†ã‚£ãƒ–ãªã®ã‹ã€ãªãœãã‚ŒãŒæ‡¸å¿µç‚¹ãªã®ã‹ãŒæ˜ç¢ºã«ã‚ã‹ã‚‹ã‚ˆã†ã«è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
- æœ€çµ‚çš„ãªç·åˆè©•ä¾¡ `summary` ã¯ã€S, A, B, C, Dã®ã„ãšã‚Œã‹ã®æ–‡å­—åˆ—ã‚’å¿…ãšè¿”ã—ã¦ãã ã•ã„ã€‚
- ãƒã‚¸ãƒ†ã‚£ãƒ–ãªç‚¹ã‚„æ‡¸å¿µç‚¹ãŒä¸€ã¤ã‚‚ãªã„å ´åˆã§ã‚‚ã€ãã®æ—¨ã‚’æ­£ç›´ã«è¨˜è¼‰ã™ã‚‹ã‹ã€ç©ºã®ãƒªã‚¹ãƒˆ `[]` ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
# è‰¯ã„ä¾‹ã¨æ‚ªã„ä¾‹
- **æ‚ªã„ä¾‹:** `{{"positive_points": ["Java, Spring Boot, AWS"]}}`
- **è‰¯ã„ä¾‹:** `{{"positive_points": ["æ¡ˆä»¶ã§å¿…é ˆã¨ãªã£ã¦ã„ã‚‹Javaã¨Spring Bootã§ã®é–‹ç™ºçµŒé¨“ãŒ5å¹´ä»¥ä¸Šã‚ã‚Šã€å³æˆ¦åŠ›ã¨ã—ã¦æœŸå¾…ã§ãã‚‹ã€‚", "AWSç’°å¢ƒã§ã®ã‚¤ãƒ³ãƒ•ãƒ©æ§‹ç¯‰çµŒé¨“ãŒã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®è¦ä»¶ã¨åˆè‡´ã—ã¦ã„ã‚‹ã€‚"]}}`
# æŒ‡ç¤º
ä»¥ä¸‹ã®2ã¤ã®æƒ…å ±ã‚’åˆ†æã—ã€ä¸Šè¨˜ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ãƒã‚¸ãƒ†ã‚£ãƒ–ãªç‚¹ã¨æ‡¸å¿µç‚¹ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ãã ã•ã„ã€‚æœ€çµ‚çš„ã«ã€ç·åˆè©•ä¾¡ï¼ˆsummaryï¼‰ã‚’S, A, B, C, Dã®5æ®µéšã§åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
- S: å®Œç’§ãªãƒãƒƒãƒã€å³æˆ¦åŠ›ã¨ã—ã¦å¼·ãæ¨è–¦
- A: éå¸¸ã«è‰¯ã„ãƒãƒƒãƒã€å¤šãã®è¦ä»¶ã‚’æº€ãŸã—ã¦ã„ã‚‹
- B: è‰¯ã„ãƒãƒƒãƒã€ä¸»è¦ãªè¦ä»¶ã¯æº€ãŸã—ã¦ã„ã‚‹
- C: æ¤œè¨ã®ä½™åœ°ã‚ã‚Šã€ã„ãã¤ã‹ã®æ‡¸å¿µç‚¹ãŒã‚ã‚‹
- D: ãƒŸã‚¹ãƒãƒƒãƒã€æ¨è–¦ã¯é›£ã—ã„
# JSONå‡ºåŠ›å½¢å¼
{{
  "summary": "S, A, B, C, Dã®ã„ãšã‚Œã‹",
  "positive_points": ["ï¼ˆã“ã“ã«å…·ä½“çš„ãªç†ç”±ã‚’èª¬æ˜ã™ã‚‹æ–‡ç« ã‚’è¨˜è¿°ï¼‰"],
  "concern_points": ["ï¼ˆã“ã“ã«å…·ä½“çš„ãªç†ç”±ã‚’èª¬æ˜ã™ã‚‹æ–‡ç« ã‚’è¨˜è¿°ï¼‰"]
}}
---
# æ¡ˆä»¶æƒ…å ±
{_job_doc}
---
# æŠ€è¡“è€…æƒ…å ±
{_engineer_doc}
---
"""
    generation_config = {"response_mime_type": "application/json"}
    safety_settings = {'HARM_CATEGORY_HARASSMENT': 'BLOCK_NONE', 'HARM_CATEGORY_HATE_SPEECH': 'BLOCK_NONE', 'HARM_CATEGORY_SEXUALLY_EXPLICIT': 'BLOCK_NONE', 'HARM_CATEGORY_DANGEROUS_CONTENT': 'BLOCK_NONE'}
    try:
        response = model.generate_content(prompt, generation_config=generation_config, safety_settings=safety_settings)
        raw_text = response.text
        start_index = raw_text.find('{')
        end_index = raw_text.rfind('}')
        if start_index != -1 and end_index != -1 and start_index < end_index:
            return json.loads(raw_text[start_index : end_index + 1])
        st.error("è©•ä¾¡ã®åˆ†æä¸­ã«LLMãŒæœ‰åŠ¹ãªJSONã‚’è¿”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚"); st.code(raw_text)
        return None
    except Exception as e:
        st.error(f"æ ¹æ‹ ã®åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def generate_proposal_reply_with_llm(job_summary, engineer_summary, engineer_name, project_name):
    if not all([job_summary, engineer_summary, engineer_name, project_name]):
        return "æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€ææ¡ˆãƒ¡ãƒ¼ãƒ«ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
    model = genai.GenerativeModel('models/gemini-2.5-flash-lite')
    prompt = f"""
ã‚ãªãŸã¯ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«å„ªç§€ãªæŠ€è¡“è€…ã‚’ææ¡ˆã™ã‚‹ã€çµŒé¨“è±Šå¯ŒãªITå–¶æ¥­æ‹…å½“è€…ã§ã™ã€‚
ä»¥ä¸‹ã®æ¡ˆä»¶æƒ…å ±ã¨æŠ€è¡“è€…æƒ…å ±ã‚’ã‚‚ã¨ã«ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å¿ƒã«éŸ¿ãã€ä¸å¯§ã§èª¬å¾—åŠ›ã®ã‚ã‚‹ææ¡ˆãƒ¡ãƒ¼ãƒ«ã®æ–‡é¢ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
# å½¹å‰²: å„ªç§€ãªITå–¶æ¥­æ‹…å½“è€…
# æŒ‡ç¤º
- æœ€åˆã«ã€ææ¡ˆã™ã‚‹æŠ€è¡“è€…åã¨æ¡ˆä»¶åã‚’è¨˜è¼‰ã—ãŸä»¶åã‚’ä½œæˆã—ã¦ãã ã•ã„ (ä¾‹: ä»¶å: ã€ã€‡ã€‡æ§˜ã®ã”ææ¡ˆã€‘ã€‡ã€‡ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä»¶)ã€‚
- æŠ€è¡“è€…ã®ã‚¹ã‚­ãƒ«ã‚„çµŒé¨“ãŒã€æ¡ˆä»¶ã®ã©ã®è¦ä»¶ã«å…·ä½“çš„ã«ãƒãƒƒãƒã—ã¦ã„ã‚‹ã‹ã‚’æ˜ç¢ºã«ç¤ºã—ã¦ãã ã•ã„ã€‚
- ãƒã‚¸ãƒ†ã‚£ãƒ–ãªç‚¹ï¼ˆé©åˆã‚¹ã‚­ãƒ«ï¼‰ã‚’å¼·èª¿ã—ã€æŠ€è¡“è€…ã®é­…åŠ›ã‚’æœ€å¤§é™ã«ä¼ãˆã¦ãã ã•ã„ã€‚
- æ‡¸å¿µç‚¹ï¼ˆã‚¹ã‚­ãƒ«ãƒŸã‚¹ãƒãƒƒãƒã‚„çµŒé¨“ä¸è¶³ï¼‰ãŒã‚ã‚‹å ´åˆã¯ã€æ­£ç›´ã«è§¦ã‚Œã¤ã¤ã‚‚ã€å­¦ç¿’æ„æ¬²ã‚„é¡ä¼¼çµŒé¨“ã€ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ãªã©ã§ã©ã®ã‚ˆã†ã«ã‚«ãƒãƒ¼ã§ãã‚‹ã‹ã‚’å‰å‘ãã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
- å…¨ä½“ã¨ã—ã¦ã€ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã‹ã¤ä¸å¯§ãªãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒ¼ãƒ«ã®ãƒˆãƒ¼ãƒ³ã‚’ç¶­æŒã—ã¦ãã ã•ã„ã€‚
- æœ€å¾Œã«ã€ãœã²ä¸€åº¦ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§ã®é¢è«‡ã®æ©Ÿä¼šã‚’è¨­ã‘ã¦ã„ãŸã ã‘ã¾ã™ã‚ˆã†ãŠé¡˜ã„ã™ã‚‹ä¸€æ–‡ã§ç· ã‚ããã£ã¦ãã ã•ã„ã€‚
- å‡ºåŠ›ã¯ã€ä»¶åã¨æœ¬æ–‡ã‚’å«ã‚“ã ãƒ¡ãƒ¼ãƒ«å½¢å¼ã®ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã¨ã—ã¦ãã ã•ã„ã€‚ä½™è¨ˆãªè§£èª¬ã¯ä¸è¦ã§ã™ã€‚
# æ¡ˆä»¶æƒ…å ±: {job_summary}
# æŠ€è¡“è€…æƒ…å ±: {engineer_summary}
# ææ¡ˆã™ã‚‹æŠ€è¡“è€…ã®åå‰: {engineer_name}
# æ¡ˆä»¶å: {project_name}
---
ãã‚Œã§ã¯ã€ä¸Šè¨˜ã®æŒ‡ç¤ºã«åŸºã¥ã„ã¦ã€æœ€é©ãªææ¡ˆãƒ¡ãƒ¼ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
"""
    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        print(f"Error generating proposal reply with LLM: {e}")
        return f"ææ¡ˆãƒ¡ãƒ¼ãƒ«ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

# --- 6. Faiss (ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢) é–¢é€£ ---

def update_index(index_path, items):
    embedding_model = load_embedding_model()
    if not embedding_model or not items: return
    dimension = embedding_model.get_sentence_embedding_dimension()
    index_map = faiss.IndexIDMap(faiss.IndexFlatIP(dimension))
    ids = np.array([item['id'] for item in items], dtype=np.int64)
    bodies = [str(item['document']).split('\n---\n', 1)[-1] for item in items]
    texts_with_prefix = ["passage: " + body for body in bodies]
    embeddings = embedding_model.encode(texts_with_prefix, normalize_embeddings=True, show_progress_bar=False)
    index_map.add_with_ids(embeddings, ids)
    faiss.write_index(index_map, index_path)

def search(query_text, index_path, top_k=5):
    embedding_model = load_embedding_model()
    if not embedding_model or not os.path.exists(index_path): return [], []
    index = faiss.read_index(index_path)
    if index.ntotal == 0: return [], []
    query_body = query_text.split('\n---\n', 1)[-1]
    prefixed_query = "query: " + query_body
    query_vector = embedding_model.encode([prefixed_query], normalize_embeddings=True).reshape(1, -1)
    similarities, ids = index.search(query_vector, min(top_k, index.ntotal))
    valid_ids = [int(i) for i in ids[0] if i != -1]
    valid_similarities = [s for s, i in zip(similarities[0], ids[0]) if i != -1]
    return valid_similarities, valid_ids

# --- 7. ãƒ‡ãƒ¼ã‚¿å‡¦ç† & ãƒãƒƒãƒãƒ³ã‚°å®Ÿè¡Œ ---

def get_records_by_ids(table_name, ids):
    if not ids: return []
    with get_db_connection() as conn:
        with conn.cursor(cursor_factory=DictCursor) as cursor:
            # INå¥ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã¯ã‚¿ãƒ—ãƒ«ã«ã™ã‚‹
            query = f"SELECT * FROM {table_name} WHERE id IN %s"
            cursor.execute(query, (tuple(ids),))
            results = cursor.fetchall()
            results_map = {res['id']: res for res in results}
            return [results_map[id] for id in ids if id in results_map]

def run_matching_for_item(item_data, item_type, cursor, now_str):
    if item_type == 'job':
        query_text, index_path, candidate_table = item_data['document'], ENGINEER_INDEX_FILE, 'engineers'
        source_name = item_data.get('project_name', f"æ¡ˆä»¶ID:{item_data['id']}")
    else:
        query_text, index_path, candidate_table = item_data['document'], JOB_INDEX_FILE, 'jobs'
        source_name = item_data.get('name', f"æŠ€è¡“è€…ID:{item_data['id']}")
    
    similarities, ids = search(query_text, index_path, top_k=TOP_K_CANDIDATES)
    if not ids:
        print(f"â–¶ ã€{source_name}ã€(ID:{item_data['id']}, {item_type}) ã®é¡ä¼¼å€™è£œã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    candidate_records = get_records_by_ids(candidate_table, ids)
    candidate_map = {record['id']: record for record in candidate_records}
    print(f"â–¶ ã€{source_name}ã€(ID:{item_data['id']}, {item_type}) ã¨ã®é¡ä¼¼å€™è£œ {len(ids)}ä»¶ã‚’è©•ä¾¡ã—ã¾ã™ã€‚")

    for sim, candidate_id in zip(similarities, ids):
        score = float(sim) * 100
        if score < MIN_SCORE_THRESHOLD: continue
        candidate_record = candidate_map.get(candidate_id)
        if not candidate_record: continue

        candidate_name = candidate_record.get('project_name' if candidate_table == 'jobs' else 'name', f"ID:{candidate_id}")
        
        job_doc, engineer_doc, job_id, engineer_id = (item_data['document'], candidate_record['document'], item_data['id'], candidate_id) if item_type == 'job' else (candidate_record['document'], item_data['document'], candidate_id, item_data['id'])
        
        llm_result = get_match_summary_with_llm(job_doc, engineer_doc)
        
        if llm_result and 'summary' in llm_result:
            grade = llm_result.get('summary')
            if grade in ['S', 'A', 'B', 'C']:
                cursor.execute(
                    'INSERT INTO matching_results (job_id, engineer_id, score, created_at, grade) VALUES (%s, %s, %s, %s, %s)',
                    (job_id, engineer_id, score, now_str, grade)
                )
                print(f"  - å€™è£œ: ã€{candidate_name}ã€(ID:{candidate_id}) -> è©•ä¾¡: {grade} (ã‚¹ã‚³ã‚¢: {score:.2f}) ... âœ… DBä¿å­˜")
            else:
                print(f"  - å€™è£œ: ã€{candidate_name}ã€(ID:{candidate_id}) -> è©•ä¾¡: {grade} (ã‚¹ã‚³ã‚¢: {score:.2f}) ... âŒ ã‚¹ã‚­ãƒƒãƒ—")
        else:
            print(f"  - å€™è£œ: ã€{candidate_name}ã€(ID:{candidate_id}) -> LLMè©•ä¾¡å¤±æ•—ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")

def process_single_content(source_data: dict):
    if not source_data:
        print("å‡¦ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")
        return False
    
    attachments = source_data.get('attachments', [])
    valid_attachments_content = [f"\n\n--- æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«: {att['filename']} ---\n{att['content']}" for att in attachments if att.get('content') and not att['content'].startswith("[")]
    full_text_for_llm = source_data.get('body', '') + "".join(valid_attachments_content)
    if not full_text_for_llm.strip():
        print("è§£æå¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return False
    
    parsed_data = split_text_with_llm(full_text_for_llm)
    if not parsed_data: return False
    
    new_jobs_data = parsed_data.get("jobs", [])
    new_engineers_data = parsed_data.get("engineers", [])
    if not new_jobs_data and not new_engineers_data:
        print("LLMã¯ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ¡ˆä»¶æƒ…å ±ã¾ãŸã¯æŠ€è¡“è€…æƒ…å ±ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        return False

    with get_db_connection() as conn:
        with conn.cursor() as cursor:
            now_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            source_json_str = json.dumps(source_data, ensure_ascii=False, indent=2)
            newly_added_jobs, newly_added_engineers = [], []

            for item_data in new_jobs_data:
                doc = item_data.get("document") or full_text_for_llm
                project_name = item_data.get("project_name", "åç§°æœªå®šã®æ¡ˆä»¶")
                meta_info = _build_meta_info_string('job', item_data)
                full_document = meta_info + doc
                cursor.execute('INSERT INTO jobs (project_name, document, source_data_json, created_at) VALUES (%s, %s, %s, %s) RETURNING id', (project_name, full_document, source_json_str, now_str))
                item_data['id'] = cursor.fetchone()[0]
                item_data['document'] = full_document
                newly_added_jobs.append(item_data)
            
            for item_data in new_engineers_data:
                doc = item_data.get("document") or full_text_for_llm
                engineer_name = item_data.get("name", "åç§°ä¸æ˜ã®æŠ€è¡“è€…")
                meta_info = _build_meta_info_string('engineer', item_data)
                full_document = meta_info + doc
                cursor.execute('INSERT INTO engineers (name, document, source_data_json, created_at) VALUES (%s, %s, %s, %s) RETURNING id', (engineer_name, full_document, source_json_str, now_str))
                item_data['id'] = cursor.fetchone()[0]
                item_data['document'] = full_document
                newly_added_engineers.append(item_data)

            print("ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ›´æ–°ã—ã€ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
            cursor.execute('SELECT id, document FROM jobs WHERE is_hidden = 0')
            all_active_jobs = cursor.fetchall()
            cursor.execute('SELECT id, document FROM engineers WHERE is_hidden = 0')
            all_active_engineers = cursor.fetchall()
            if all_active_jobs: update_index(JOB_INDEX_FILE, all_active_jobs)
            if all_active_engineers: update_index(ENGINEER_INDEX_FILE, all_active_engineers)

            for new_job in newly_added_jobs: run_matching_for_item(new_job, 'job', cursor, now_str)
            for new_engineer in newly_added_engineers: run_matching_for_item(new_engineer, 'engineer', cursor, now_str)
        conn.commit()
    return True

# --- 8. ãƒ¡ãƒ¼ãƒ«å–å¾—ãƒ»å‡¦ç† ---

def extract_text_from_pdf(file_bytes):
    try:
        with fitz.open(stream=file_bytes, filetype="pdf") as doc:
            text = "".join(page.get_text() for page in doc)
        return text.strip() or "[PDFãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¤±æ•—: å†…å®¹ãŒç©ºã¾ãŸã¯ç”»åƒPDF]"
    except Exception as e: return f"[PDFãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}]"

def extract_text_from_docx(file_bytes):
    try:
        doc = docx.Document(io.BytesIO(file_bytes))
        text = "\n".join([para.text for para in doc.paragraphs])
        return text.strip() or "[DOCXãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¤±æ•—: å†…å®¹ãŒç©º]"
    except Exception as e: return f"[DOCXãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}]"

def get_email_contents(msg):
    body_text, attachments = "", []
    if msg.is_multipart():
        for part in msg.walk():
            content_type = part.get_content_type()
            content_disposition = str(part.get("Content-Disposition"))
            if 'text/plain' in content_type and 'attachment' not in content_disposition:
                charset = part.get_content_charset() or 'utf-8'
                try: body_text += part.get_payload(decode=True).decode(charset, 'ignore')
                except Exception: body_text += part.get_payload(decode=True).decode('utf-8', 'ignore')
            if 'attachment' in content_disposition:
                raw_filename = part.get_filename()
                if raw_filename:
                    filename = "".join([s.decode(c or 'utf-8', 'ignore') if isinstance(s, bytes) else s for s, c in decode_header(raw_filename)])
                    print(f"ğŸ“„ æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚")
                    file_bytes = part.get_payload(decode=True)
                    lower_filename = filename.lower()
                    content = ""
                    if lower_filename.endswith(".pdf"): content = extract_text_from_pdf(file_bytes)
                    elif lower_filename.endswith(".docx"): content = extract_text_from_docx(file_bytes)
                    elif lower_filename.endswith(".txt"): content = file_bytes.decode('utf-8', 'ignore')
                    else: print(f"â„¹ï¸ æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã¯æœªå¯¾å¿œå½¢å¼ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã€‚")
                    if content: attachments.append({"filename": filename, "content": content})
    else:
        charset = msg.get_content_charset() or 'utf-8'
        try: body_text = msg.get_payload(decode=True).decode(charset, 'ignore')
        except Exception: body_text = msg.get_payload(decode=True).decode('utf-8', 'ignore')
    return {"body": body_text.strip(), "attachments": attachments}

def fetch_and_process_emails():
    log_stream = io.StringIO()
    try:
        with contextlib.redirect_stdout(log_stream):
            try:
                SERVER, USER, PASSWORD = st.secrets["EMAIL_SERVER"], st.secrets["EMAIL_USER"], st.secrets["EMAIL_PASSWORD"]
            except KeyError as e:
                print(f"ãƒ¡ãƒ¼ãƒ«ã‚µãƒ¼ãƒãƒ¼ã®æ¥ç¶šæƒ…å ±ãŒSecretsã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“: {e}")
                return False, log_stream.getvalue()
            
            with imaplib.IMAP4_SSL(SERVER) as mail:
                mail.login(USER, PASSWORD)
                mail.select('inbox')
                _, messages = mail.search(None, 'UNSEEN')
                email_ids = messages[0].split()
                if not email_ids:
                    print("å‡¦ç†å¯¾è±¡ã®æœªèª­ãƒ¡ãƒ¼ãƒ«ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                    return True, log_stream.getvalue()

                total_processed_count = 0
                latest_ids = email_ids[::-1][:10]
                print(f"æœ€æ–°ã®æœªèª­ãƒ¡ãƒ¼ãƒ« {len(latest_ids)}ä»¶ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚")
                for i, email_id in enumerate(latest_ids):
                    _, msg_data = mail.fetch(email_id, '(RFC822)')
                    for response_part in msg_data:
                        if isinstance(response_part, tuple):
                            msg = email.message_from_bytes(response_part[1])
                            source_data = get_email_contents(msg)
                            if source_data['body'] or source_data['attachments']:
                                print(f"âœ… ãƒ¡ãƒ¼ãƒ«ID {email_id.decode()} ã‚’å‡¦ç†ã—ã¾ã™...")
                                if process_single_content(source_data):
                                    total_processed_count += 1
                                    mail.store(email_id, '+FLAGS', '\\Seen')
                            else:
                                print(f"âœ–ï¸ ãƒ¡ãƒ¼ãƒ«ID {email_id.decode()} ã¯å†…å®¹ãŒãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    print(f"({i+1}/{len(latest_ids)}) ãƒã‚§ãƒƒã‚¯å®Œäº†")
        
        if total_processed_count > 0:
            st.success(f"{total_processed_count} ä»¶ã®ãƒ¡ãƒ¼ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã€ä¿å­˜ã—ã¾ã—ãŸã€‚")
            st.balloons()
        else:
            st.info("æ–°ã—ã„æœªèª­ãƒ¡ãƒ¼ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã—ãŸãŒã€å‡¦ç†å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return True, log_stream.getvalue()
    except Exception as e:
        print(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return False, log_stream.getvalue()

# --- 9. ãƒ‡ãƒ¼ã‚¿æ›´æ–°ãƒ»æ“ä½œ (CRUD) ---

def _update_single_field(table, field, value, record_id):
    """æ±ç”¨çš„ãªå˜ä¸€ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ›´æ–°é–¢æ•°"""
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cursor:
                # SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é˜²ããŸã‚ã€ãƒ†ãƒ¼ãƒ–ãƒ«åã¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã¯ãƒ—ãƒ­ã‚°ãƒ©ãƒ å´ã§å›ºå®š
                # å‹•çš„ã«ã™ã‚‹å ´åˆã¯ã€ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆã§å³å¯†ã«æ¤œè¨¼ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
                query = f"UPDATE {table} SET {field} = %s WHERE id = %s"
                cursor.execute(query, (value, record_id))
            conn.commit()
        return True
    except Exception as e:
        print(f"DBæ›´æ–°ã‚¨ãƒ©ãƒ¼ ({table}.{field}): {e}")
        return False

def update_job_source_json(job_id, new_json_str):
    return _update_single_field('jobs', 'source_data_json', new_json_str, job_id)

def update_engineer_source_json(engineer_id, new_json_str):
    return _update_single_field('engineers', 'source_data_json', new_json_str, engineer_id)

def update_engineer_name(engineer_id, new_name):
    return _update_single_field('engineers', 'name', new_name.strip(), engineer_id) if new_name and new_name.strip() else False

def assign_user_to_job(job_id, user_id):
    return _update_single_field('jobs', 'assigned_user_id', user_id, job_id)

def assign_user_to_engineer(engineer_id, user_id):
    return _update_single_field('engineers', 'assigned_user_id', user_id, engineer_id)

def set_job_visibility(job_id, is_hidden):
    return _update_single_field('jobs', 'is_hidden', is_hidden, job_id)

def set_engineer_visibility(engineer_id, is_hidden):
    return _update_single_field('engineers', 'is_hidden', is_hidden, engineer_id)

def hide_match(result_id):
    return _update_single_field('matching_results', 'is_hidden', 1, result_id)

def save_match_grade(match_id, grade):
    return _update_single_field('matching_results', 'grade', grade, match_id) if grade else False

def update_match_status(match_id, new_status):
    return _update_single_field('matching_results', 'status', new_status, match_id) if match_id and new_status else False

def get_all_users():
    """å…¨ã¦ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’å–å¾—ã™ã‚‹ã€‚"""
    with get_db_connection() as conn:
        with conn.cursor(cursor_factory=DictCursor) as cursor:
            cursor.execute("SELECT id, username FROM users ORDER BY id")
            return cursor.fetchall()

def get_matching_result_details(result_id):
    """æŒ‡å®šã•ã‚ŒãŸãƒãƒƒãƒãƒ³ã‚°çµæœIDã®è©³ç´°æƒ…å ±ã‚’å–å¾—ã™ã‚‹ã€‚"""
    with get_db_connection() as conn:
        with conn.cursor(cursor_factory=DictCursor) as cursor:
            cursor.execute("SELECT * FROM matching_results WHERE id = %s", (result_id,))
            match_result = cursor.fetchone()
            if not match_result: return None
            
            job_query = "SELECT j.*, u.username as assignee_name FROM jobs j LEFT JOIN users u ON j.assigned_user_id = u.id WHERE j.id = %s"
            cursor.execute(job_query, (match_result['job_id'],))
            job_data = cursor.fetchone()

            engineer_query = "SELECT e.*, u.username as assignee_name FROM engineers e LEFT JOIN users u ON e.assigned_user_id = u.id WHERE e.id = %s"
            cursor.execute(engineer_query, (match_result['engineer_id'],))
            engineer_data = cursor.fetchone()

            return {
                "match_result": match_result,
                "job_data": job_data,
                "engineer_data": engineer_data,
            }

# --- 10. å†è©•ä¾¡ãƒ»å†ãƒãƒƒãƒãƒ³ã‚° ---

def re_evaluate_and_match_single_item(item_type, item_id):
    """æ¡ˆä»¶ã¾ãŸã¯æŠ€è¡“è€…ã®å˜ä¸€ã‚¢ã‚¤ãƒ†ãƒ ã‚’å†è©•ä¾¡ãƒ»å†ãƒãƒƒãƒãƒ³ã‚°ã™ã‚‹å…±é€šé–¢æ•°"""
    table_name = 'jobs' if item_type == 'job' else 'engineers'
    
    with get_db_connection() as conn:
        with conn.cursor(cursor_factory=DictCursor) as cursor:
            cursor.execute(f"SELECT * FROM {table_name} WHERE id = %s", (item_id,))
            record = cursor.fetchone()
            if not record:
                print(f"ID:{item_id} ã® {item_type} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                return False

            source_data = json.loads(record['source_data_json'])
            full_text_for_llm = source_data.get('body', '')
            
            parsed_data = split_text_with_llm(full_text_for_llm)
            if not parsed_data or not parsed_data.get(f"{table_name}s"): # 'jobs' or 'engineers'
                print(f"LLMã«ã‚ˆã‚‹å†è©•ä¾¡ã§ã€{item_type}æƒ…å ±ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                return False

            item_data = parsed_data[f"{table_name}s"][0]
            doc = item_data.get("document") or full_text_for_llm
            meta_info = _build_meta_info_string(item_type, item_data)
            new_full_document = meta_info + doc
            
            if item_type == 'job':
                new_name = item_data.get('project_name', record['project_name'])
                cursor.execute("UPDATE jobs SET document = %s, project_name = %s WHERE id = %s", (new_full_document, new_name, item_id))
            else:
                new_name = item_data.get('name', record['name'])
                cursor.execute("UPDATE engineers SET document = %s, name = %s WHERE id = %s", (new_full_document, new_name, item_id))

            delete_query = f"DELETE FROM matching_results WHERE {item_type}_id = %s"
            cursor.execute(delete_query, (item_id,))
            print(f"{item_type} ID:{item_id} ã®æ—¢å­˜ãƒãƒƒãƒãƒ³ã‚°çµæœã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")

            print("ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ›´æ–°ã—ã€å†ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
            cursor.execute('SELECT id, document FROM jobs WHERE is_hidden = 0')
            all_active_jobs = cursor.fetchall()
            cursor.execute('SELECT id, document FROM engineers WHERE is_hidden = 0')
            all_active_engineers = cursor.fetchall()
            if all_active_jobs: update_index(JOB_INDEX_FILE, all_active_jobs)
            if all_active_engineers: update_index(ENGINEER_INDEX_FILE, all_active_engineers)

            now_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            data_for_matching = {'id': item_id, 'document': new_full_document}
            if item_type == 'job': data_for_matching['project_name'] = new_name
            else: data_for_matching['name'] = new_name
            
            run_matching_for_item(data_for_matching, item_type, cursor, now_str)
        conn.commit()
    return True

def re_evaluate_and_match_single_engineer(engineer_id):
    return re_evaluate_and_match_single_item('engineer', engineer_id)

def re_evaluate_and_match_single_job(job_id):
    return re_evaluate_and_match_single_item('job', job_id)
