import streamlit as st
import psycopg2
from psycopg2.extras import DictCursor
import faiss
from sentence_transformers import SentenceTransformer
import numpy as np
import os
import google.generativeai as genai
import json
from datetime import datetime
import imaplib
import email
from email.header import decode_header, make_header
from email.utils import parsedate_to_datetime
import io
import contextlib
import toml
import fitz
import docx

# --- 1. åˆæœŸè¨­å®šã¨å®šæ•° (å¤‰æ›´ãªã—) ---
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=API_KEY)
except (KeyError, Exception):
    st.error("`secrets.toml` ã« `GOOGLE_API_KEY` ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    st.stop()

JOB_INDEX_FILE = "backend_job_index.faiss"
ENGINEER_INDEX_FILE = "backend_engineer_index.faiss"
MODEL_NAME = 'intfloat/multilingual-e5-large'
TOP_K_CANDIDATES = 500
MIN_SCORE_THRESHOLD = 70.0

# --- é–¢æ•°å®šç¾© ---
@st.cache_data
def load_app_config():
    try:
        with open("config.toml", "r", encoding="utf-8") as f:
            return toml.load(f)
    except FileNotFoundError:
        return {"app": {"title": "Universal AI Agent (Default)"}, "messages": {"sales_staff_notice": ""}}
    except Exception as e:
        print(f"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return {"app": {"title": "Universal AI Agent (Error)"}, "messages": {"sales_staff_notice": ""}}

@st.cache_resource
def load_embedding_model():
    try:
        return SentenceTransformer(MODEL_NAME)
    except Exception as e:
        st.error(f"åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ« '{MODEL_NAME}' ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}"); return None

def get_db_connection():
    try:
        db_url = st.secrets["DATABASE_URL"]
        return psycopg2.connect(db_url, cursor_factory=DictCursor)
    except KeyError:
        st.error("`secrets.toml` ã« `DATABASE_URL` ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"); st.stop()
    except psycopg2.OperationalError as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ: {e}"); st.stop()
    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"); st.stop()

def init_database():
    conn = get_db_connection()
    try:
        with conn.cursor() as cursor:
            cursor.execute('CREATE TABLE IF NOT EXISTS jobs (id SERIAL PRIMARY KEY, project_name TEXT, document TEXT NOT NULL, source_data_json TEXT, created_at TEXT, assigned_user_id INTEGER, is_hidden INTEGER NOT NULL DEFAULT 0, received_at TIMESTAMP WITH TIME ZONE)')
            cursor.execute('CREATE TABLE IF NOT EXISTS engineers (id SERIAL PRIMARY KEY, name TEXT, document TEXT NOT NULL, source_data_json TEXT, created_at TEXT, assigned_user_id INTEGER, is_hidden INTEGER NOT NULL DEFAULT 0, received_at TIMESTAMP WITH TIME ZONE)')
            cursor.execute("CREATE TABLE IF NOT EXISTS users (id SERIAL PRIMARY KEY, username TEXT NOT NULL UNIQUE, email TEXT, created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP);")
            cursor.execute('''CREATE TABLE IF NOT EXISTS matching_results (id SERIAL PRIMARY KEY, job_id INTEGER NOT NULL, engineer_id INTEGER NOT NULL, score REAL NOT NULL, created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP, is_hidden INTEGER DEFAULT 0, grade TEXT, positive_points TEXT, concern_points TEXT, proposal_text TEXT, status TEXT DEFAULT 'æ–°è¦', FOREIGN KEY (job_id) REFERENCES jobs (id) ON DELETE CASCADE, FOREIGN KEY (engineer_id) REFERENCES engineers (id) ON DELETE CASCADE, UNIQUE (job_id, engineer_id))''')
            cursor.execute("SELECT COUNT(*) FROM users")
            if cursor.fetchone()[0] == 0:
                print("åˆå›èµ·å‹•ã®ãŸã‚ã€ãƒ†ã‚¹ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’è¿½åŠ ã—ã¾ã™...")
                users_to_add = [('ç†Šå´', 'yamada@example.com'), ('å²©æœ¬', 'suzuki@example.com'), ('å°é–¢', 'sato@example.com'), ('å†…å±±', 'sato@example.com'), ('å³¶ç”°', 'sato@example.com'), ('é•·è°·å·', 'sato@example.com'), ('åŒ—å³¶', 'sato@example.com'), ('å²©å´', 'sato@example.com'), ('æ ¹å²¸', 'sato@example.com'), ('æ·»ç”°', 'sato@example.com'), ('å±±æµ¦', 'sato@example.com'), ('ç¦ç”°', 'sato@example.com')]
                cursor.executemany("INSERT INTO users (username, email) VALUES (%s, %s)", users_to_add)
                print(" -> ãƒ†ã‚¹ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚")
            def get_columns(table_name):
                cursor.execute("SELECT column_name FROM information_schema.columns WHERE table_schema = 'public' AND table_name = %s", (table_name,))
                return [row['column_name'] for row in cursor.fetchall()]
            job_columns = get_columns('jobs')
            if 'assigned_user_id' not in job_columns: cursor.execute("ALTER TABLE jobs ADD COLUMN assigned_user_id INTEGER REFERENCES users(id)")
            if 'is_hidden' not in job_columns: cursor.execute("ALTER TABLE jobs ADD COLUMN is_hidden INTEGER NOT NULL DEFAULT 0")
            if 'received_at' not in job_columns: cursor.execute("ALTER TABLE jobs ADD COLUMN received_at TIMESTAMP WITH TIME ZONE")
            engineer_columns = get_columns('engineers')
            if 'assigned_user_id' not in engineer_columns: cursor.execute("ALTER TABLE engineers ADD COLUMN assigned_user_id INTEGER REFERENCES users(id)")
            if 'is_hidden' not in engineer_columns: cursor.execute("ALTER TABLE engineers ADD COLUMN is_hidden INTEGER NOT NULL DEFAULT 0")
            if 'received_at' not in engineer_columns: cursor.execute("ALTER TABLE engineers ADD COLUMN received_at TIMESTAMP WITH TIME ZONE")
            match_columns = get_columns('matching_results')
            if 'positive_points' not in match_columns: cursor.execute("ALTER TABLE matching_results ADD COLUMN positive_points TEXT")
            if 'concern_points' not in match_columns: cursor.execute("ALTER TABLE matching_results ADD COLUMN concern_points TEXT")
            if 'status' not in match_columns: cursor.execute("ALTER TABLE matching_results ADD COLUMN status TEXT DEFAULT 'æ–°è¦'")
        conn.commit()
        print("Database initialized and schema verified successfully for PostgreSQL.")
    except (Exception, psycopg2.Error) as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"); conn.rollback()
    finally:
        if conn: conn.close()

def get_extraction_prompt(doc_type, text_content):
    if doc_type == 'engineer':
        return f"""
            ã‚ãªãŸã¯ã€ITäººæã®ã€Œã‚¹ã‚­ãƒ«ã‚·ãƒ¼ãƒˆã€ã‚„ã€Œè·å‹™çµŒæ­´æ›¸ã€ã‚’èª­ã¿è§£ãå°‚é–€å®¶ã§ã™ã€‚
            ã‚ãªãŸã®ä»•äº‹ã¯ã€ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰**å˜ä¸€ã®æŠ€è¡“è€…æƒ…å ±**ã‚’æŠ½å‡ºã—ã€æŒ‡å®šã•ã‚ŒãŸJSONå½¢å¼ã§æ•´ç†ã™ã‚‹ã“ã¨ã§ã™ã€‚
            # çµ¶å¯¾çš„ãªãƒ«ãƒ¼ãƒ«
            - å‡ºåŠ›ã¯ã€æŒ‡å®šã•ã‚ŒãŸJSONå½¢å¼ã®æ–‡å­—åˆ—ã®ã¿ã¨ã—ã€å‰å¾Œã«è§£èª¬ã‚„```json ```ã®ã‚ˆã†ãªã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®å›²ã¿ã‚’å«ã‚ãªã„ã§ãã ã•ã„ã€‚
            # æŒ‡ç¤º
            - ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã¯ã€ä¸€äººã®æŠ€è¡“è€…ã®æƒ…å ±ã§ã™ã€‚è¤‡æ•°ã®æ¥­å‹™çµŒæ­´ãŒå«ã¾ã‚Œã¦ã„ã¦ã‚‚ã€ãã‚Œã‚‰ã¯ã™ã¹ã¦ã“ã®ä¸€äººã®æŠ€è¡“è€…ã®çµŒæ­´ã¨ã—ã¦è¦ç´„ã—ã¦ãã ã•ã„ã€‚
            - `document`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ã¯ã€æŠ€è¡“è€…ã®ã‚¹ã‚­ãƒ«ã€çµŒé¨“ã€è‡ªå·±PRãªã©ã‚’ç·åˆçš„ã«è¦ç´„ã—ãŸã€æ¤œç´¢ã—ã‚„ã™ã„è‡ªç„¶ãªæ–‡ç« ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
            - `document`ã®æ–‡ç« ã®å…ˆé ­ã«ã¯ã€å¿…ãšæŠ€è¡“è€…åã‚’å«ã‚ã¦ãã ã•ã„ã€‚ä¾‹ï¼šã€Œå®Ÿå‹™çµŒé¨“15å¹´ã®TKæ°ã€‚Java(SpringBoot)ã‚’ä¸»è»¸ã«...ã€
            # JSONå‡ºåŠ›å½¢å¼
            {{"engineers": [{{"name": "æŠ€è¡“è€…ã®æ°åã‚’æŠ½å‡º", "document": "æŠ€è¡“è€…ã®ã‚¹ã‚­ãƒ«ã‚„çµŒæ­´ã®è©³ç´°ã‚’ã€æ¤œç´¢ã—ã‚„ã™ã„ã‚ˆã†ã«è¦ç´„", "nationality": "å›½ç±ã‚’æŠ½å‡º", "availability_date": "ç¨¼åƒå¯èƒ½æ—¥ã‚’æŠ½å‡º", "desired_location": "å¸Œæœ›å‹¤å‹™åœ°ã‚’æŠ½å‡º", "desired_salary": "å¸Œæœ›å˜ä¾¡ã‚’æŠ½å‡º", "main_skills": "ä¸»è¦ãªã‚¹ã‚­ãƒ«ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§æŠ½å‡º"}}]}}
            # æœ¬ç•ª: ä»¥ä¸‹ã®ã‚¹ã‚­ãƒ«ã‚·ãƒ¼ãƒˆã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„
            ---
            {text_content}
        """
    elif doc_type == 'job':
        return f"""
            ã‚ãªãŸã¯ã€ITæ¥­ç•Œã®ã€Œæ¡ˆä»¶å®šç¾©æ›¸ã€ã‚’èª­ã¿è§£ãå°‚é–€å®¶ã§ã™ã€‚
            ã‚ãªãŸã®ä»•äº‹ã¯ã€ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰**æ¡ˆä»¶æƒ…å ±**ã‚’æŠ½å‡ºã—ã€æŒ‡å®šã•ã‚ŒãŸJSONå½¢å¼ã§æ•´ç†ã™ã‚‹ã“ã¨ã§ã™ã€‚
            ãƒ†ã‚­ã‚¹ãƒˆå†…ã«è¤‡æ•°ã®æ¡ˆä»¶æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãã‚Œãã‚Œã‚’å€‹åˆ¥ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦ãƒªã‚¹ãƒˆã«ã—ã¦ãã ã•ã„ã€‚
            # çµ¶å¯¾çš„ãªãƒ«ãƒ¼ãƒ«
            - å‡ºåŠ›ã¯ã€æŒ‡å®šã•ã‚ŒãŸJSONå½¢å¼ã®æ–‡å­—åˆ—ã®ã¿ã¨ã—ã€å‰å¾Œã«è§£èª¬ã‚„```json ```ã®ã‚ˆã†ãªã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®å›²ã¿ã‚’å«ã‚ãªã„ã§ãã ã•ã„ã€‚
            # æŒ‡ç¤º
            - `document`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ã¯ã€æ¡ˆä»¶ã®ã‚¹ã‚­ãƒ«ã‚„æ¥­å‹™å†…å®¹ã®è©³ç´°ã‚’ã€å¾Œã§æ¤œç´¢ã—ã‚„ã™ã„ã‚ˆã†ã«è‡ªç„¶ãªæ–‡ç« ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚
            - `document`ã®æ–‡ç« ã®å…ˆé ­ã«ã¯ã€å¿…ãšãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’å«ã‚ã¦ãã ã•ã„ã€‚ä¾‹ï¼šã€Œç¤¾å†…SEãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å¢—å“¡æ¡ˆä»¶ã€‚è¨­è¨ˆã€ãƒ†ã‚¹ãƒˆ...ã€
            # JSONå‡ºåŠ›å½¢å¼
            {{"jobs": [{{"project_name": "æ¡ˆä»¶åã‚’æŠ½å‡º", "document": "æ¡ˆä»¶ã®ã‚¹ã‚­ãƒ«ã‚„æ¥­å‹™å†…å®¹ã®è©³ç´°ã‚’ã€æ¤œç´¢ã—ã‚„ã™ã„ã‚ˆã†ã«è¦ç´„", "nationality_requirement": "å›½ç±è¦ä»¶ã‚’æŠ½å‡º", "start_date": "é–‹å§‹æ™‚æœŸã‚’æŠ½å‡º", "location": "å‹¤å‹™åœ°ã‚’æŠ½å‡º", "unit_price": "å˜ä¾¡ã‚„äºˆç®—ã‚’æŠ½å‡º", "required_skills": "å¿…é ˆã‚¹ã‚­ãƒ«ã‚„æ­“è¿ã‚¹ã‚­ãƒ«ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§æŠ½å‡º"}}]}}
            # æœ¬ç•ª: ä»¥ä¸‹ã®æ¡ˆä»¶æƒ…å ±ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„
            ---
            {text_content}
        """
    return ""

def split_text_with_llm(text_content):
    classification_prompt = f"""
        ã‚ãªãŸã¯ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆãŒã€Œæ¡ˆä»¶æƒ…å ±ã€ã€ŒæŠ€è¡“è€…æƒ…å ±ã€ã€Œãã®ä»–ã€ã®ã©ã‚Œã«æœ€ã‚‚å½“ã¦ã¯ã¾ã‚‹ã‹åˆ¤æ–­ã—ã€æŒ‡å®šã•ã‚ŒãŸå˜èªä¸€ã¤ã ã‘ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
        # åˆ¤æ–­åŸºæº–
        - ã€Œã‚¹ã‚­ãƒ«ã‚·ãƒ¼ãƒˆã€ã€Œè·å‹™çµŒæ­´æ›¸ã€ã€Œæ°åã€ã€Œå¹´é½¢ã€ã¨ã„ã£ãŸå˜èªãŒå«ã¾ã‚Œã¦ã„ã‚Œã°ã€ŒæŠ€è¡“è€…æƒ…å ±ã€ã®å¯èƒ½æ€§ãŒé«˜ã„ã€‚
        - ã€Œå‹Ÿé›†ã€ã€Œå¿…é ˆã‚¹ã‚­ãƒ«ã€ã€Œæ­“è¿ã‚¹ã‚­ãƒ«ã€ã€Œæ±‚ã‚ã‚‹äººç‰©åƒã€ã¨ã„ã£ãŸå˜èªãŒå«ã¾ã‚Œã¦ã„ã‚Œã°ã€Œæ¡ˆä»¶æƒ…å ±ã€ã®å¯èƒ½æ€§ãŒé«˜ã„ã€‚
        - ä¸Šè¨˜ã®ã©ã¡ã‚‰ã§ã‚‚ãªã„å ´åˆã¯ã€Œãã®ä»–ã€ã¨åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚
        # å›ç­”å½¢å¼
        - `æ¡ˆä»¶æƒ…å ±`
        - `æŠ€è¡“è€…æƒ…å ±`
        - `ãã®ä»–`
        # åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
        ---
        {text_content[:2000]}
        ---
    """
    try:
        model = genai.GenerativeModel('models/gemini-2.5-flash-lite')
        st.write("ğŸ“„ æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã‚’åˆ†é¡ä¸­...")
        response = model.generate_content(classification_prompt)
        doc_type = response.text.strip()
        st.write(f"âœ… AIã«ã‚ˆã‚‹åˆ†é¡çµæœ: **{doc_type}**")
    except Exception as e:
        st.error(f"æ–‡æ›¸ã®åˆ†é¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"); return None

    if "æŠ€è¡“è€…æƒ…å ±" in doc_type:
        extraction_prompt = get_extraction_prompt('engineer', text_content)
    elif "æ¡ˆä»¶æƒ…å ±" in doc_type:
        extraction_prompt = get_extraction_prompt('job', text_content)
    else:
        st.warning("ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã¯æ¡ˆä»¶æƒ…å ±ã¾ãŸã¯æŠ€è¡“è€…æƒ…å ±ã¨ã—ã¦åˆ†é¡ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"); return None

    generation_config = {"response_mime_type": "application/json"}
    safety_settings = {'HARM_CATEGORY_HARASSMENT': 'BLOCK_NONE', 'HARM_CATEGORY_HATE_SPEECH': 'BLOCK_NONE', 'HARM_CATEGORY_SEXUALLY_EXPLICIT': 'BLOCK_NONE', 'HARM_CATEGORY_DANGEROUS_CONTENT': 'BLOCK_NONE'}
    try:
        with st.spinner("AIãŒæƒ…å ±ã‚’æ§‹é€ åŒ–ä¸­..."):
            response = model.generate_content(extraction_prompt, generation_config=generation_config, safety_settings=safety_settings)
        raw_text = response.text
        start_index = raw_text.find('{'); end_index = raw_text.rfind('}')
        if start_index != -1 and end_index != -1 and start_index < end_index:
            json_str = raw_text[start_index : end_index + 1]
            parsed_json = json.loads(json_str)
            if "æŠ€è¡“è€…æƒ…å ±" in doc_type: parsed_json["jobs"] = []
            elif "æ¡ˆä»¶æƒ…å ±" in doc_type: parsed_json["engineers"] = []
            return parsed_json
        else:
            st.error(f"LLMã«ã‚ˆã‚‹æ§‹é€ åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"); st.code(raw_text, language='text'); return None
    except json.JSONDecodeError as e:
        st.error(f"LLMã«ã‚ˆã‚‹æ§‹é€ åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}"); st.code(raw_text, language='text'); return None
    except Exception as e:
        st.error(f"LLMã«ã‚ˆã‚‹æ§‹é€ åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}");
        try: st.code(response.text, language='text')
        except NameError: st.text("ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å–å¾—ã«ã‚‚å¤±æ•—ã—ã¾ã—ãŸã€‚")
        return None

@st.cache_data
def get_match_summary_with_llm(job_doc, engineer_doc):
    model = genai.GenerativeModel('models/gemini-2.5-flash-lite')
    prompt = f"""
        ã‚ãªãŸã¯ã€çµŒé¨“è±Šå¯ŒãªITäººæç´¹ä»‹ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚
        ã‚ãªãŸã®ä»•äº‹ã¯ã€æç¤ºã•ã‚ŒãŸã€Œæ¡ˆä»¶æƒ…å ±ã€ã¨ã€ŒæŠ€è¡“è€…æƒ…å ±ã€ã‚’æ¯”è¼ƒã—ã€å®¢è¦³çš„ã‹ã¤å…·ä½“çš„ãªãƒãƒƒãƒãƒ³ã‚°è©•ä¾¡ã‚’è¡Œã†ã“ã¨ã§ã™ã€‚
        # çµ¶å¯¾çš„ãªãƒ«ãƒ¼ãƒ«
        - `summary`ã¯æœ€ã‚‚é‡è¦ãªé …ç›®ã§ã™ã€‚çµ¶å¯¾ã«çœç•¥ã›ãšã€å¿…ãšS, A, B, C, Dã®ã„ãšã‚Œã‹ã®æ–‡å­—åˆ—ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
        # æŒ‡ç¤º
        ä»¥ä¸‹ã®2ã¤ã®æƒ…å ±ã‚’åˆ†æã—ã€ãƒã‚¸ãƒ†ã‚£ãƒ–ãªç‚¹ã¨æ‡¸å¿µç‚¹ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ãã ã•ã„ã€‚æœ€çµ‚çš„ã«ã€ç·åˆè©•ä¾¡ï¼ˆsummaryï¼‰ã‚’S, A, B, C, Dã®5æ®µéšã§åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
        - S: å®Œç’§ãªãƒãƒƒãƒ, A: éå¸¸ã«è‰¯ã„ãƒãƒƒãƒ, B: è‰¯ã„ãƒãƒƒãƒ, C: æ¤œè¨ã®ä½™åœ°ã‚ã‚Š, D: ãƒŸã‚¹ãƒãƒƒãƒ
        # JSONå‡ºåŠ›å½¢å¼
        {{"summary": "S, A, B, C, Dã®ã„ãšã‚Œã‹", "positive_points": ["ã‚¹ã‚­ãƒ«é¢ã§ã®åˆè‡´ç‚¹"], "concern_points": ["ã‚¹ã‚­ãƒ«é¢ã§ã®æ‡¸å¿µç‚¹"]}}
        ---
        # æ¡ˆä»¶æƒ…å ±
        {job_doc}
        ---
        # æŠ€è¡“è€…æƒ…å ±
        {engineer_doc}
        ---
    """
    generation_config = {"response_mime_type": "application/json"}
    safety_settings = {'HARM_CATEGORY_HARASSMENT': 'BLOCK_NONE', 'HARM_CATEGORY_HATE_SPEECH': 'BLOCK_NONE', 'HARM_CATEGORY_SEXUALLY_EXPLICIT': 'BLOCK_NONE', 'HARM_CATEGORY_DANGEROUS_CONTENT': 'BLOCK_NONE'}
    try:
        with st.spinner("AIãŒãƒãƒƒãƒãƒ³ã‚°æ ¹æ‹ ã‚’åˆ†æä¸­..."):
            response = model.generate_content(prompt, generation_config=generation_config, safety_settings=safety_settings)
        raw_text = response.text
        start_index = raw_text.find('{'); end_index = raw_text.rfind('}')
        if start_index != -1 and end_index != -1 and start_index < end_index:
            json_str = raw_text[start_index : end_index + 1]
            return json.loads(json_str)
        else:
            st.error("è©•ä¾¡ã®åˆ†æä¸­ã«LLMãŒæœ‰åŠ¹ãªJSONã‚’è¿”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚"); st.code(raw_text); return None
    except Exception as e:
        st.error(f"æ ¹æ‹ ã®åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}"); return None

def update_index(index_path, items):
    embedding_model = load_embedding_model()
    if not embedding_model or not items: return
    dimension = embedding_model.get_sentence_embedding_dimension()
    index_map = faiss.IndexIDMap(faiss.IndexFlatIP(dimension))
    ids = np.array([item['id'] for item in items], dtype=np.int64)
    bodies = [str(item['document']).split('\n---\n', 1)[-1] for item in items]
    texts_with_prefix = ["passage: " + body for body in bodies]
    embeddings = embedding_model.encode(texts_with_prefix, normalize_embeddings=True, show_progress_bar=False)
    index_map.add_with_ids(embeddings, ids)
    faiss.write_index(index_map, index_path)

def search(query_text, index_path, top_k=5):
    embedding_model = load_embedding_model()
    if not embedding_model or not os.path.exists(index_path): return [], []
    index = faiss.read_index(index_path)
    if index.ntotal == 0: return [], []
    query_body = query_text.split('\n---\n', 1)[-1]
    prefixed_query = "query: " + query_body
    query_vector = embedding_model.encode([prefixed_query], normalize_embeddings=True).reshape(1, -1)
    similarities, ids = index.search(query_vector, min(top_k, index.ntotal))
    valid_ids = [int(i) for i in ids[0] if i != -1]
    valid_similarities = [similarities[0][j] for j, i in enumerate(ids[0]) if i != -1]
    return valid_similarities, valid_ids

def get_records_by_ids(table_name, ids):
    if not ids: return []
    with get_db_connection() as conn:
        with conn.cursor() as cursor:
            query = f"SELECT * FROM {table_name} WHERE id = ANY(%s)"
            cursor.execute(query, (ids,))
            results = cursor.fetchall()
            results_map = {res['id']: res for res in results}
            return [results_map[id] for id in ids if id in results_map]

def run_matching_for_item(item_data, item_type, cursor, now_str):
    if item_type == 'job':
        query_text, index_path, candidate_table = item_data['document'], ENGINEER_INDEX_FILE, 'engineers'
        source_name = item_data.get('project_name', f"æ¡ˆä»¶ID:{item_data['id']}")
    else:
        query_text, index_path, candidate_table = item_data['document'], JOB_INDEX_FILE, 'jobs'
        source_name = item_data.get('name', f"æŠ€è¡“è€…ID:{item_data['id']}")
    similarities, ids = search(query_text, index_path, top_k=TOP_K_CANDIDATES)
    if not ids:
        st.write(f"â–¶ ã€{source_name}ã€(ID:{item_data['id']}) ã®é¡ä¼¼å€™è£œã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"); return
    candidate_records = get_records_by_ids(candidate_table, ids)
    candidate_map = {record['id']: record for record in candidate_records}
    st.write(f"â–¶ ã€{source_name}ã€(ID:{item_data['id']}) ã¨ã®é¡ä¼¼å€™è£œ {len(ids)}ä»¶ã‚’è©•ä¾¡ã—ã¾ã™ã€‚")
    for sim, candidate_id in zip(similarities, ids):
        score = float(sim) * 100
        if score < MIN_SCORE_THRESHOLD: continue
        candidate_record = candidate_map.get(candidate_id)
        if not candidate_record: continue
        candidate_name = candidate_record.get('project_name') or candidate_record.get('name') or f"ID:{candidate_id}"
        job_doc, engineer_doc = (item_data['document'], candidate_record['document']) if item_type == 'job' else (candidate_record['document'], item_data['document'])
        job_id, engineer_id = (item_data['id'], candidate_record['id']) if item_type == 'job' else (candidate_record['id'], item_data['id'])
        llm_result = get_match_summary_with_llm(job_doc, engineer_doc)
        if llm_result and 'summary' in llm_result:
            grade = llm_result.get('summary')
            if grade in ['S', 'A', 'B']:
                cursor.execute('INSERT INTO matching_results (job_id, engineer_id, score, created_at, grade) VALUES (%s, %s, %s, %s, %s) ON CONFLICT (job_id, engineer_id) DO NOTHING', (job_id, engineer_id, score, now_str, grade))
                st.write(f"  - å€™è£œ: ã€{candidate_name}ã€ -> è©•ä¾¡: {grade} (ã‚¹ã‚³ã‚¢: {score:.2f}) ... âœ… DBã«ä¿å­˜")
            else:
                st.write(f"  - å€™è£œ: ã€{candidate_name}ã€ -> è©•ä¾¡: {grade} (ã‚¹ã‚³ã‚¢: {score:.2f}) ... âŒ ã‚¹ã‚­ãƒƒãƒ—")
        else:
            st.write(f"  - å€™è£œ: ã€{candidate_name}ã€ -> LLMè©•ä¾¡å¤±æ•—ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")

# â–¼â–¼â–¼ã€ã“ã“ãŒä¿®æ­£ç®‡æ‰€ã€‘â–¼â–¼â–¼
def process_single_content(source_data: dict):
    if not source_data: st.warning("å‡¦ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚"); return False
    valid_attachments_content = [f"\n\n--- æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«: {att['filename']} ---\n{att.get('content', '')}" for att in source_data.get('attachments', []) if att.get('content') and not att.get('content', '').startswith("[") and not att.get('content', '').endswith("]")]
    if valid_attachments_content: st.write(f"â„¹ï¸ {len(valid_attachments_content)}ä»¶ã®æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’è§£æã«å«ã‚ã¾ã™ã€‚")
    full_text_for_llm = source_data.get('body', '') + "".join(valid_attachments_content)
    if not full_text_for_llm.strip(): st.warning("è§£æå¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚"); return False
    
    parsed_data = split_text_with_llm(full_text_for_llm)
    if not parsed_data: return False
    
    new_jobs_data, new_engineers_data = parsed_data.get("jobs", []), parsed_data.get("engineers", [])
    if not new_jobs_data and not new_engineers_data: st.warning("LLMã¯ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ¡ˆä»¶æƒ…å ±ã¾ãŸã¯æŠ€è¡“è€…æƒ…å ±ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"); return False
    
    with get_db_connection() as conn:
        with conn.cursor() as cursor:
            now_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            # --- JSONä¿å­˜ç”¨ã®å‡¦ç† ---
            # 1. å…ƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—
            received_at_dt = source_data.get('received_at')
            
            # 2. JSONä¿å­˜ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼
            json_data_to_store = source_data.copy()
            
            # 3. datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ–‡å­—åˆ—ã«å¤‰æ›ï¼ˆJSONã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºã®ãŸã‚ï¼‰
            if isinstance(json_data_to_store.get('received_at'), datetime):
                json_data_to_store['received_at'] = json_data_to_store['received_at'].isoformat()
            
            # 4. å¤§ããªãƒ‡ãƒ¼ã‚¿ï¼ˆæœ¬æ–‡ã¨æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã‚’å‰Šé™¤
            json_data_to_store.pop('body', None)
            json_data_to_store.pop('attachments', None)
            
            # 5. JSONæ–‡å­—åˆ—ã‚’ä½œæˆ
            source_json_str = json.dumps(json_data_to_store, ensure_ascii=False, indent=2)
            
            # --- DBç™»éŒ²å‡¦ç† ---
            newly_added_jobs, newly_added_engineers = [], []
            
            for item_data in new_jobs_data:
                doc = item_data.get("document") or full_text_for_llm
                project_name = item_data.get("project_name", "åç§°æœªå®šã®æ¡ˆä»¶")
                meta_info = _build_meta_info_string('job', item_data)
                full_document = meta_info + doc
                # DBã«ã¯datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ(received_at_dt)ã‚’æ¸¡ã™
                cursor.execute('INSERT INTO jobs (project_name, document, source_data_json, created_at, received_at) VALUES (%s, %s, %s, %s, %s) RETURNING id', 
                               (project_name, full_document, source_json_str, now_str, received_at_dt))
                item_data['id'] = cursor.fetchone()[0]; item_data['document'] = full_document; newly_added_jobs.append(item_data)
            
            for item_data in new_engineers_data:
                doc = item_data.get("document") or full_text_for_llm
                engineer_name = item_data.get("name", "åç§°ä¸æ˜ã®æŠ€è¡“è€…")
                meta_info = _build_meta_info_string('engineer', item_data)
                full_document = meta_info + doc
                # DBã«ã¯datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ(received_at_dt)ã‚’æ¸¡ã™
                cursor.execute('INSERT INTO engineers (name, document, source_data_json, created_at, received_at) VALUES (%s, %s, %s, %s, %s) RETURNING id', 
                               (engineer_name, full_document, source_json_str, now_str, received_at_dt))
                item_data['id'] = cursor.fetchone()[0]; item_data['document'] = full_document; newly_added_engineers.append(item_data)
            
            st.write("ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ›´æ–°ã—ã€ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
            cursor.execute('SELECT id, document FROM jobs WHERE is_hidden = 0'); all_active_jobs = cursor.fetchall()
            cursor.execute('SELECT id, document FROM engineers WHERE is_hidden = 0'); all_active_engineers = cursor.fetchall()
            
            if all_active_jobs: update_index(JOB_INDEX_FILE, all_active_jobs)
            if all_active_engineers: update_index(ENGINEER_INDEX_FILE, all_active_engineers)
            
            for new_job in newly_added_jobs: run_matching_for_item(new_job, 'job', cursor, now_str)
            for new_engineer in newly_added_engineers: run_matching_for_item(new_engineer, 'engineer', cursor, now_str)
        conn.commit()
    return True
# â–²â–²â–²ã€ä¿®æ­£ã“ã“ã¾ã§ã€‘â–²â–²â–²

def get_email_contents(msg) -> dict:
    subject = str(make_header(decode_header(msg["subject"]))) if msg["subject"] else ""
    from_ = str(make_header(decode_header(msg["from"]))) if msg["from"] else ""
    received_at = parsedate_to_datetime(msg["Date"]) if msg["Date"] else None

    body_text, attachments = "", []
    if msg.is_multipart():
        for part in msg.walk():
            content_type, content_disposition = part.get_content_type(), str(part.get("Content-Disposition"))
            if 'text/plain' in content_type and 'attachment' not in content_disposition:
                charset = part.get_content_charset()
                try: body_text += part.get_payload(decode=True).decode(charset or 'utf-8', errors='ignore')
                except Exception: body_text += part.get_payload(decode=True).decode('utf-8', errors='ignore')
            if 'attachment' in content_disposition and (raw_filename := part.get_filename()):
                filename = str(make_header(decode_header(raw_filename)))
                st.write(f"ğŸ“„ æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚")
                file_bytes, lower_filename = part.get_payload(decode=True), filename.lower()
                if lower_filename.endswith(".pdf"): attachments.append({"filename": filename, "content": extract_text_from_pdf(file_bytes)})
                elif lower_filename.endswith(".docx"): attachments.append({"filename": filename, "content": extract_text_from_docx(file_bytes)})
                elif lower_filename.endswith(".txt"): attachments.append({"filename": filename, "content": file_bytes.decode('utf-8', errors='ignore')})
                else: st.write(f"â„¹ï¸ æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã¯æœªå¯¾å¿œã®å½¢å¼ã®ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
    else:
        charset = msg.get_content_charset()
        try: body_text = msg.get_payload(decode=True).decode(charset or 'utf-8', errors='ignore')
        except Exception: body_text = msg.get_payload(decode=True).decode('utf-8', errors='ignore')
    
    return {"subject": subject, "from": from_, "received_at": received_at, "body": body_text.strip(), "attachments": attachments}

def fetch_and_process_emails():
    log_stream = io.StringIO()
    try:
        with contextlib.redirect_stdout(log_stream):
            try: SERVER, USER, PASSWORD = st.secrets["EMAIL_SERVER"], st.secrets["EMAIL_USER"], st.secrets["EMAIL_PASSWORD"]
            except KeyError as e: st.error(f"ãƒ¡ãƒ¼ãƒ«ã‚µãƒ¼ãƒãƒ¼ã®æ¥ç¶šæƒ…å ±ãŒSecretsã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“: {e}"); return False, log_stream.getvalue()
            try: mail = imaplib.IMAP4_SSL(SERVER); mail.login(USER, PASSWORD); mail.select('inbox')
            except Exception as e: st.error(f"ãƒ¡ãƒ¼ãƒ«ã‚µãƒ¼ãƒãƒ¼ã¸ã®æ¥ç¶šã¾ãŸã¯ãƒ­ã‚°ã‚¤ãƒ³ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}"); return False, log_stream.getvalue()
            total_processed_count, checked_count = 0, 0
            try:
                with st.status("æœ€æ–°ã®æœªèª­ãƒ¡ãƒ¼ãƒ«ã‚’å–å¾—ãƒ»å‡¦ç†ä¸­...", expanded=True) as status:
                    _, messages = mail.search(None, 'UNSEEN')
                    email_ids = messages[0].split()
                    if not email_ids: st.write("å‡¦ç†å¯¾è±¡ã®æœªèª­ãƒ¡ãƒ¼ãƒ«ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                    else:
                        latest_ids = email_ids[::-1][:10]; checked_count = len(latest_ids)
                        st.write(f"æœ€æ–°ã®æœªèª­ãƒ¡ãƒ¼ãƒ« {checked_count}ä»¶ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚")
                        for i, email_id in enumerate(latest_ids):
                            _, msg_data = mail.fetch(email_id, '(RFC822)')
                            for response_part in msg_data:
                                if isinstance(response_part, tuple):
                                    msg = email.message_from_bytes(response_part[1])
                                    source_data = get_email_contents(msg)
                                    if source_data['body'] or source_data['attachments']:
                                        st.write("---")
                                        st.write(f"âœ… ãƒ¡ãƒ¼ãƒ«ID {email_id.decode()} ã‚’å‡¦ç†ã—ã¾ã™ã€‚")
                                        received_at_str = source_data['received_at'].strftime('%Y-%m-%d %H:%M:%S') if source_data.get('received_at') else 'å–å¾—ä¸å¯'
                                        st.write(f"   å—ä¿¡æ—¥æ™‚: {received_at_str}")
                                        st.write(f"   å·®å‡ºäºº: {source_data.get('from', 'å–å¾—ä¸å¯')}")
                                        st.write(f"   ä»¶å: {source_data.get('subject', 'å–å¾—ä¸å¯')}")
                                        if process_single_content(source_data):
                                            total_processed_count += 1; mail.store(email_id, '+FLAGS', '\\Seen')
                                    else: st.write(f"âœ–ï¸ ãƒ¡ãƒ¼ãƒ«ID {email_id.decode()} ã¯æœ¬æ–‡ã‚‚æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ç„¡ã„ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                            st.write(f"({i+1}/{checked_count}) ãƒã‚§ãƒƒã‚¯å®Œäº†")
                    status.update(label="ãƒ¡ãƒ¼ãƒ«ãƒã‚§ãƒƒã‚¯å®Œäº†", state="complete")
            finally: mail.close(); mail.logout()
        if checked_count > 0:
            if total_processed_count > 0: st.success(f"ãƒã‚§ãƒƒã‚¯ã—ãŸ {checked_count} ä»¶ã®ãƒ¡ãƒ¼ãƒ«ã®ã†ã¡ã€{total_processed_count} ä»¶ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã€ä¿å­˜ã—ã¾ã—ãŸã€‚"); st.balloons()
            else: st.warning(f"ãƒ¡ãƒ¼ãƒ«ã‚’ {checked_count} ä»¶ãƒã‚§ãƒƒã‚¯ã—ã¾ã—ãŸãŒã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã§ãã‚‹æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else: st.info("å‡¦ç†å¯¾è±¡ã¨ãªã‚‹æ–°ã—ã„æœªèª­ãƒ¡ãƒ¼ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return True, log_stream.getvalue()
    except Exception as e:
        st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"); return False, log_stream.getvalue()

# --- æ®‹ã‚Šã®é–¢æ•° (å¤‰æ›´ãªã—) ---
def hide_match(result_id):
    if not result_id: st.warning("IDãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"); return False
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute('UPDATE matching_results SET is_hidden = 1 WHERE id = %s', (result_id,))
                if cursor.rowcount > 0: st.toast(f"ãƒãƒƒãƒãƒ³ã‚°çµæœ (ID: {result_id}) ã‚’éè¡¨ç¤ºã«ã—ã¾ã—ãŸã€‚"); conn.commit(); return True
                else: st.warning(f"ãƒãƒƒãƒãƒ³ã‚°çµæœ (ID: {result_id}) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"); return False
    except (Exception, psycopg2.Error) as e: st.error(f"DBæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}"); return False

def get_all_users():
    with get_db_connection() as conn:
        with conn.cursor() as cursor:
            cursor.execute("SELECT id, username FROM users ORDER BY id"); return cursor.fetchall()

def assign_user_to_job(job_id, user_id):
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cur: cur.execute("UPDATE jobs SET assigned_user_id = %s WHERE id = %s", (user_id, job_id))
            conn.commit(); return True
        except (Exception, psycopg2.Error) as e: print(f"æ‹…å½“è€…å‰²ã‚Šå½“ã¦ã‚¨ãƒ©ãƒ¼: {e}"); conn.rollback(); return False

def set_job_visibility(job_id, is_hidden):
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cur: cur.execute("UPDATE jobs SET is_hidden = %s WHERE id = %s", (is_hidden, job_id))
            conn.commit(); return True
        except (Exception, psycopg2.Error) as e: print(f"è¡¨ç¤ºçŠ¶æ…‹ã®æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}"); conn.rollback(); return False

def assign_user_to_engineer(engineer_id, user_id):
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cur: cur.execute("UPDATE engineers SET assigned_user_id = %s WHERE id = %s", (user_id, engineer_id))
            conn.commit(); return True
        except (Exception, psycopg2.Error) as e: print(f"æŠ€è¡“è€…ã¸ã®æ‹…å½“è€…å‰²ã‚Šå½“ã¦ã‚¨ãƒ©ãƒ¼: {e}"); conn.rollback(); return False

def set_engineer_visibility(engineer_id, is_hidden):
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cur: cur.execute("UPDATE engineers SET is_hidden = %s WHERE id = %s", (is_hidden, engineer_id))
            conn.commit(); return True
        except (Exception, psycopg2.Error) as e: print(f"æŠ€è¡“è€…ã®è¡¨ç¤ºçŠ¶æ…‹ã®æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}"); conn.rollback(); return False

def update_engineer_source_json(engineer_id, new_json_str):
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cur: cur.execute("UPDATE engineers SET source_data_json = %s WHERE id = %s", (new_json_str, engineer_id))
            conn.commit(); return True
        except (Exception, psycopg2.Error) as e: print(f"æŠ€è¡“è€…ã®JSONãƒ‡ãƒ¼ã‚¿æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}"); conn.rollback(); return False

def generate_proposal_reply_with_llm(job_summary, engineer_summary, engineer_name, project_name):
    if not all([job_summary, engineer_summary, engineer_name, project_name]): return "æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€ææ¡ˆãƒ¡ãƒ¼ãƒ«ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
    prompt = f"""
        ã‚ãªãŸã¯ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«å„ªç§€ãªæŠ€è¡“è€…ã‚’ææ¡ˆã™ã‚‹ã€çµŒé¨“è±Šå¯ŒãªITå–¶æ¥­æ‹…å½“è€…ã§ã™ã€‚
        ä»¥ä¸‹ã®æ¡ˆä»¶æƒ…å ±ã¨æŠ€è¡“è€…æƒ…å ±ã‚’ã‚‚ã¨ã«ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å¿ƒã«éŸ¿ãã€ä¸å¯§ã§èª¬å¾—åŠ›ã®ã‚ã‚‹ææ¡ˆãƒ¡ãƒ¼ãƒ«ã®æ–‡é¢ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
        # å½¹å‰²
        - å„ªç§€ãªITå–¶æ¥­æ‹…å½“è€…
        # æŒ‡ç¤º
        - æœ€åˆã«ã€ææ¡ˆã™ã‚‹æŠ€è¡“è€…åã¨æ¡ˆä»¶åã‚’è¨˜è¼‰ã—ãŸä»¶åã‚’ä½œæˆã—ã¦ãã ã•ã„ (ä¾‹: ä»¶å: ã€ã€‡ã€‡æ§˜ã®ã”ææ¡ˆã€‘ã€‡ã€‡ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä»¶)ã€‚
        - æŠ€è¡“è€…ã®ã‚¹ã‚­ãƒ«ã‚„çµŒé¨“ãŒã€æ¡ˆä»¶ã®ã©ã®è¦ä»¶ã«å…·ä½“çš„ã«ãƒãƒƒãƒã—ã¦ã„ã‚‹ã‹ã‚’æ˜ç¢ºã«ç¤ºã—ã¦ãã ã•ã„ã€‚
        - ãƒã‚¸ãƒ†ã‚£ãƒ–ãªç‚¹ï¼ˆé©åˆã‚¹ã‚­ãƒ«ï¼‰ã‚’å¼·èª¿ã—ã€æŠ€è¡“è€…ã®é­…åŠ›ã‚’æœ€å¤§é™ã«ä¼ãˆã¦ãã ã•ã„ã€‚
        - æ‡¸å¿µç‚¹ï¼ˆã‚¹ã‚­ãƒ«ãƒŸã‚¹ãƒãƒƒãƒã‚„çµŒé¨“ä¸è¶³ï¼‰ãŒã‚ã‚‹å ´åˆã¯ã€æ­£ç›´ã«è§¦ã‚Œã¤ã¤ã‚‚ã€å­¦ç¿’æ„æ¬²ã‚„é¡ä¼¼çµŒé¨“ã€ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ãªã©ã§ã©ã®ã‚ˆã†ã«ã‚«ãƒãƒ¼ã§ãã‚‹ã‹ã‚’å‰å‘ãã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
        - å…¨ä½“ã¨ã—ã¦ã€ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã‹ã¤ä¸å¯§ãªãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒ¼ãƒ«ã®ãƒˆãƒ¼ãƒ³ã‚’ç¶­æŒã—ã¦ãã ã•ã„ã€‚
        - æœ€å¾Œã«ã€ãœã²ä¸€åº¦ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§ã®é¢è«‡ã®æ©Ÿä¼šã‚’è¨­ã‘ã¦ã„ãŸã ã‘ã¾ã™ã‚ˆã†ãŠé¡˜ã„ã™ã‚‹ä¸€æ–‡ã§ç· ã‚ããã£ã¦ãã ã•ã„ã€‚
        - å‡ºåŠ›ã¯ã€ä»¶åã¨æœ¬æ–‡ã‚’å«ã‚“ã ãƒ¡ãƒ¼ãƒ«å½¢å¼ã®ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã¨ã—ã¦ãã ã•ã„ã€‚ä½™è¨ˆãªè§£èª¬ã¯ä¸è¦ã§ã™ã€‚
        # æ¡ˆä»¶æƒ…å ±
        {job_summary}
        # æŠ€è¡“è€…æƒ…å ±
        {engineer_summary}
        # ææ¡ˆã™ã‚‹æŠ€è¡“è€…ã®åå‰
        {engineer_name}
        # æ¡ˆä»¶å
        {project_name}
        ---
        ãã‚Œã§ã¯ã€ä¸Šè¨˜ã®æŒ‡ç¤ºã«åŸºã¥ã„ã¦ã€æœ€é©ãªææ¡ˆãƒ¡ãƒ¼ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
    """
    try:
        model = genai.GenerativeModel('models/gemini-2.5-flash-lite')
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        print(f"Error generating proposal reply with LLM: {e}"); return f"ææ¡ˆãƒ¡ãƒ¼ãƒ«ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

def save_match_grade(match_id, grade):
    if not grade: return False
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cursor: cursor.execute("UPDATE matching_results SET grade = %s WHERE id = %s", (grade, match_id))
            conn.commit(); return True
        except (Exception, psycopg2.Error) as e: print(f"Error saving match grade for match_id {match_id}: {e}"); conn.rollback(); return False

def get_evaluation_html(grade, font_size='2.5em'):
    if not grade: return ""
    color_map = {'S': '#00b894', 'A': '#28a745', 'B': '#17a2b8', 'C': '#ffc107', 'D': '#fd7e14', 'E': '#dc3545'}
    color = color_map.get(grade.upper(), '#6c757d') 
    style = f"color: {color}; font-size: {font_size}; font-weight: bold; text-align: center; line-height: 1; padding-top: 10px;"
    html_code = f"<div style='text-align: center; margin-bottom: 5px;'><span style='{style}'>{grade.upper()}</span></div><div style='text-align: center; font-size: 0.8em; color: #888;'>åˆ¤å®š</div>"
    return html_code

def get_matching_result_details(result_id):
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cursor:
                cursor.execute("SELECT * FROM matching_results WHERE id = %s", (result_id,))
                match_result = cursor.fetchone()
                if not match_result: return None
                cursor.execute("SELECT j.*, u.username as assignee_name FROM jobs j LEFT JOIN users u ON j.assigned_user_id = u.id WHERE j.id = %s", (match_result['job_id'],))
                job_data = cursor.fetchone()
                cursor.execute("SELECT e.*, u.username as assignee_name FROM engineers e LEFT JOIN users u ON e.assigned_user_id = u.id WHERE e.id = %s", (match_result['engineer_id'],))
                engineer_data = cursor.fetchone()
                return {"match_result": dict(match_result), "job_data": dict(job_data) if job_data else None, "engineer_data": dict(engineer_data) if engineer_data else None}
        except (Exception, psycopg2.Error) as e:
            print(f"ãƒãƒƒãƒãƒ³ã‚°è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}"); return None

def re_evaluate_and_match_single_engineer(engineer_id):
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cursor:
                cursor.execute("SELECT * FROM engineers WHERE id = %s", (engineer_id,))
                engineer_record = cursor.fetchone()
                if not engineer_record:
                    st.error(f"æŠ€è¡“è€…ID:{engineer_id} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"); return False
                source_data = json.loads(engineer_record['source_data_json'])
                full_text_for_llm = source_data.get('body', '') + "".join([f"\n\n--- æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«: {att['filename']} ---\n{att.get('content', '')}" for att in source_data.get('attachments', []) if att.get('content') and not att.get('content', '').startswith("[") and not att.get('content', '').endswith("]")])
                parsed_data = split_text_with_llm(full_text_for_llm)
                if not parsed_data or not parsed_data.get("engineers"):
                    st.error("LLMã«ã‚ˆã‚‹å†è©•ä¾¡ã§ã€æŠ€è¡“è€…æƒ…å ±ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸã€‚"); return False
                item_data = parsed_data["engineers"][0]
                doc = item_data.get("document") or full_text_for_llm
                meta_info = _build_meta_info_string('engineer', item_data)
                new_full_document = meta_info + doc
                cursor.execute("UPDATE engineers SET document = %s WHERE id = %s", (new_full_document, engineer_id))
                cursor.execute("DELETE FROM matching_results WHERE engineer_id = %s", (engineer_id,))
                st.write(f"æŠ€è¡“è€…ID:{engineer_id} ã®æ—¢å­˜ãƒãƒƒãƒãƒ³ã‚°çµæœã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")
                st.write("ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ›´æ–°ã—ã€å†ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
                cursor.execute('SELECT id, document FROM jobs WHERE is_hidden = 0'); all_jobs = cursor.fetchall()
                cursor.execute('SELECT id, document FROM engineers WHERE is_hidden = 0'); all_engineers = cursor.fetchall()
                if all_jobs: update_index(JOB_INDEX_FILE, all_jobs)
                if all_engineers: update_index(ENGINEER_INDEX_FILE, all_engineers)
                now_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                engineer_data_for_matching = {'id': engineer_id, 'document': new_full_document, 'name': engineer_record['name']}
                run_matching_for_item(engineer_data_for_matching, 'engineer', cursor, now_str)
            conn.commit()
            return True
        except (Exception, psycopg2.Error) as e:
            conn.rollback(); st.error(f"å†è©•ä¾¡ãƒ»å†ãƒãƒƒãƒãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"); return False

def update_engineer_name(engineer_id, new_name):
    if not new_name or not new_name.strip(): return False
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cursor: cursor.execute("UPDATE engineers SET name = %s WHERE id = %s", (new_name.strip(), engineer_id))
            conn.commit(); return True
        except (Exception, psycopg2.Error) as e:
            print(f"æŠ€è¡“è€…æ°åã®æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}"); conn.rollback(); return False

def _build_meta_info_string(item_type, item_data):
    meta_fields = []
    if item_type == 'job':
        meta_fields = [["å›½ç±è¦ä»¶", "nationality_requirement"], ["é–‹å§‹æ™‚æœŸ", "start_date"], ["å‹¤å‹™åœ°", "location"], ["å˜ä¾¡", "unit_price"], ["å¿…é ˆã‚¹ã‚­ãƒ«", "required_skills"]]
    elif item_type == 'engineer':
        meta_fields = [["å›½ç±", "nationality"], ["ç¨¼åƒå¯èƒ½æ—¥", "availability_date"], ["å¸Œæœ›å‹¤å‹™åœ°", "desired_location"], ["å¸Œæœ›å˜ä¾¡", "desired_salary"], ["ä¸»è¦ã‚¹ã‚­ãƒ«", "main_skills"]]
    if not meta_fields: return "\n---\n"
    meta_parts = [f"[{display_name}: {item_data.get(key, 'ä¸æ˜')}]" for display_name, key in meta_fields]
    return " ".join(meta_parts) + "\n---\n"

def update_match_status(match_id, new_status):
    if not match_id or not new_status: return False
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cursor: cursor.execute("UPDATE matching_results SET status = %s WHERE id = %s", (new_status, match_id))
            conn.commit(); return True
        except (Exception, psycopg2.Error) as e:
            print(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}"); conn.rollback(); return False
