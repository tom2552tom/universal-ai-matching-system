import streamlit as st
import psycopg2
from psycopg2.extras import DictCursor
import faiss
from sentence_transformers import SentenceTransformer
import numpy as np
import os
import google.generativeai as genai
import json

import imaplib
import email
from email.header import decode_header, make_header
from email.utils import parsedate_to_datetime
import io
import contextlib
import toml
import fitz
import docx
import re # ã‚¹ã‚­ãƒ«æŠ½å‡ºã®ãŸã‚ã« re ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import json
import pandas as pd

from datetime import datetime, date, time, timedelta
import pytz # â˜…â˜…â˜… ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚’æ‰±ã†ãŸã‚ã«è¿½åŠ  â˜…â˜…â˜…



# --- 1. åˆæœŸè¨­å®šã¨å®šæ•° (å¤‰æ›´ãªã—) ---
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=API_KEY)
except (KeyError, Exception):
    st.error("`secrets.toml` ã« `GOOGLE_API_KEY` ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    st.stop()

JOB_INDEX_FILE = "backend_job_index.faiss"
ENGINEER_INDEX_FILE = "backend_engineer_index.faiss"
MODEL_NAME = 'intfloat/multilingual-e5-large'
TOP_K_CANDIDATES = 500
MIN_SCORE_THRESHOLD = 70.0

# --- é–¢æ•°å®šç¾© ---
#@st.cache_data
def load_app_config():
    try:
        with open("config.toml", "r", encoding="utf-8") as f:
            return toml.load(f)
    except FileNotFoundError:
        return {
                "app": {"title": "Universal AI Agent (Default)"}, "messages": {"sales_staff_notice": ""},
                "email_processing": {"fetch_limit": 10} # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã«ã‚‚è¿½åŠ 
                }
    
    except Exception as e:
        print(f"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return {"app": {"title": "Universal AI Agent (Error)"}, "messages": {"sales_staff_notice": ""}}

@st.cache_resource
def load_embedding_model():
    try:
        return SentenceTransformer(MODEL_NAME)
    except Exception as e:
        st.error(f"åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ« '{MODEL_NAME}' ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}"); return None

def get_db_connection():
    try:
        db_url = st.secrets["DATABASE_URL"]
        return psycopg2.connect(db_url, cursor_factory=DictCursor)
    except KeyError:
        st.error("`secrets.toml` ã« `DATABASE_URL` ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"); st.stop()
    except psycopg2.OperationalError as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ: {e}"); st.stop()
    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"); st.stop()

def init_database():
    conn = get_db_connection()
    try:
        with conn.cursor() as cursor:
            cursor.execute('CREATE TABLE IF NOT EXISTS jobs (id SERIAL PRIMARY KEY, project_name TEXT, document TEXT NOT NULL, source_data_json TEXT, created_at TEXT, assigned_user_id INTEGER, is_hidden INTEGER NOT NULL DEFAULT 0, received_at TIMESTAMP WITH TIME ZONE)')
            cursor.execute('CREATE TABLE IF NOT EXISTS engineers (id SERIAL PRIMARY KEY, name TEXT, document TEXT NOT NULL, source_data_json TEXT, created_at TEXT, assigned_user_id INTEGER, is_hidden INTEGER NOT NULL DEFAULT 0, received_at TIMESTAMP WITH TIME ZONE)')
            cursor.execute("CREATE TABLE IF NOT EXISTS users (id SERIAL PRIMARY KEY, username TEXT NOT NULL UNIQUE, email TEXT, created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP);")
            cursor.execute('''CREATE TABLE IF NOT EXISTS matching_results (id SERIAL PRIMARY KEY, job_id INTEGER NOT NULL, engineer_id INTEGER NOT NULL, score REAL NOT NULL, created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP, is_hidden INTEGER DEFAULT 0, grade TEXT, positive_points TEXT, concern_points TEXT, proposal_text TEXT, status TEXT DEFAULT 'æ–°è¦', FOREIGN KEY (job_id) REFERENCES jobs (id) ON DELETE CASCADE, FOREIGN KEY (engineer_id) REFERENCES engineers (id) ON DELETE CASCADE, UNIQUE (job_id, engineer_id))''')
            cursor.execute("SELECT COUNT(*) FROM users")
            if cursor.fetchone()[0] == 0:
                print("åˆå›èµ·å‹•ã®ãŸã‚ã€ãƒ†ã‚¹ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’è¿½åŠ ã—ã¾ã™...")
                users_to_add = [('ç†Šå´', 'yamada@example.com'), ('å²©æœ¬', 'suzuki@example.com'), ('å°é–¢', 'sato@example.com'), ('å†…å±±', 'sato@example.com'), ('å³¶ç”°', 'sato@example.com'), ('é•·è°·å·', 'sato@example.com'), ('åŒ—å³¶', 'sato@example.com'), ('å²©å´', 'sato@example.com'), ('æ ¹å²¸', 'sato@example.com'), ('æ·»ç”°', 'sato@example.com'), ('å±±æµ¦', 'sato@example.com'), ('ç¦ç”°', 'sato@example.com')]
                cursor.executemany("INSERT INTO users (username, email) VALUES (%s, %s)", users_to_add)
                print(" -> ãƒ†ã‚¹ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚")
            def get_columns(table_name):
                cursor.execute("SELECT column_name FROM information_schema.columns WHERE table_schema = 'public' AND table_name = %s", (table_name,))
                return [row['column_name'] for row in cursor.fetchall()]
            job_columns = get_columns('jobs')
            if 'assigned_user_id' not in job_columns: cursor.execute("ALTER TABLE jobs ADD COLUMN assigned_user_id INTEGER REFERENCES users(id)")
            if 'is_hidden' not in job_columns: cursor.execute("ALTER TABLE jobs ADD COLUMN is_hidden INTEGER NOT NULL DEFAULT 0")
            if 'received_at' not in job_columns: cursor.execute("ALTER TABLE jobs ADD COLUMN received_at TIMESTAMP WITH TIME ZONE")
            engineer_columns = get_columns('engineers')
            if 'assigned_user_id' not in engineer_columns: cursor.execute("ALTER TABLE engineers ADD COLUMN assigned_user_id INTEGER REFERENCES users(id)")
            if 'is_hidden' not in engineer_columns: cursor.execute("ALTER TABLE engineers ADD COLUMN is_hidden INTEGER NOT NULL DEFAULT 0")
            if 'received_at' not in engineer_columns: cursor.execute("ALTER TABLE engineers ADD COLUMN received_at TIMESTAMP WITH TIME ZONE")
            match_columns = get_columns('matching_results')
            if 'positive_points' not in match_columns: cursor.execute("ALTER TABLE matching_results ADD COLUMN positive_points TEXT")
            if 'concern_points' not in match_columns: cursor.execute("ALTER TABLE matching_results ADD COLUMN concern_points TEXT")
            if 'status' not in match_columns: cursor.execute("ALTER TABLE matching_results ADD COLUMN status TEXT DEFAULT 'æ–°è¦'")
        conn.commit()
        print("Database initialized and schema verified successfully for PostgreSQL.")
    except (Exception, psycopg2.Error) as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"); conn.rollback()
    finally:
        if conn: conn.close()

def get_extraction_prompt(doc_type, text_content):
    if doc_type == 'engineer':
        return f"""
            ã‚ãªãŸã¯ã€ITäººæã®ã€Œã‚¹ã‚­ãƒ«ã‚·ãƒ¼ãƒˆã€ã‚„ã€Œè·å‹™çµŒæ­´æ›¸ã€ã‚’èª­ã¿è§£ãå°‚é–€å®¶ã§ã™ã€‚
            ã‚ãªãŸã®ä»•äº‹ã¯ã€ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰**å˜ä¸€ã®æŠ€è¡“è€…æƒ…å ±**ã‚’æŠ½å‡ºã—ã€æŒ‡å®šã•ã‚ŒãŸJSONå½¢å¼ã§æ•´ç†ã™ã‚‹ã“ã¨ã§ã™ã€‚
            # çµ¶å¯¾çš„ãªãƒ«ãƒ¼ãƒ«
            - å‡ºåŠ›ã¯ã€æŒ‡å®šã•ã‚ŒãŸJSONå½¢å¼ã®æ–‡å­—åˆ—ã®ã¿ã¨ã—ã€å‰å¾Œã«è§£èª¬ã‚„```json ```ã®ã‚ˆã†ãªã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®å›²ã¿ã‚’å«ã‚ãªã„ã§ãã ã•ã„ã€‚
            # æŒ‡ç¤º
            - ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã¯ã€ä¸€äººã®æŠ€è¡“è€…ã®æƒ…å ±ã§ã™ã€‚è¤‡æ•°ã®æ¥­å‹™çµŒæ­´ãŒå«ã¾ã‚Œã¦ã„ã¦ã‚‚ã€ãã‚Œã‚‰ã¯ã™ã¹ã¦ã“ã®ä¸€äººã®æŠ€è¡“è€…ã®çµŒæ­´ã¨ã—ã¦è¦ç´„ã—ã¦ãã ã•ã„ã€‚
            - `document`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ã¯ã€æŠ€è¡“è€…ã®ã‚¹ã‚­ãƒ«ã€çµŒé¨“ã€è‡ªå·±PRãªã©ã‚’ç·åˆçš„ã«è¦ç´„ã—ãŸã€æ¤œç´¢ã—ã‚„ã™ã„è‡ªç„¶ãªæ–‡ç« ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
            - `document`ã®æ–‡ç« ã®å…ˆé ­ã«ã¯ã€å¿…ãšæŠ€è¡“è€…åã‚’å«ã‚ã¦ãã ã•ã„ã€‚ä¾‹ï¼šã€Œå®Ÿå‹™çµŒé¨“15å¹´ã®TKæ°ã€‚Java(SpringBoot)ã‚’ä¸»è»¸ã«...ã€
            # å…·ä½“ä¾‹
            ## å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ:
            æ°å: å±±ç”° å¤ªéƒ
            å¹´é½¢: 35æ­³
            å¾—æ„æŠ€è¡“: Java, Spring
            è‡ªå·±PR: Webã‚¢ãƒ—ãƒªé–‹ç™ºãŒå¾—æ„ã§ã™ã€‚
            ## å‡ºåŠ›JSON:
            {{"engineers": [{{"name": "å±±ç”° å¤ªéƒ", "document": "35æ­³ã®å±±ç”°å¤ªéƒæ°ã€‚Java, Springã‚’ç”¨ã„ãŸWebã‚¢ãƒ—ãƒªé–‹ç™ºãŒå¾—æ„ã€‚", "main_skills": "Java, Spring"}}]}}
            # JSONå‡ºåŠ›å½¢å¼
            {{"engineers": [{{"name": "æŠ€è¡“è€…ã®æ°åã‚’æŠ½å‡º", "document": "æŠ€è¡“è€…ã®ã‚¹ã‚­ãƒ«ã‚„çµŒæ­´ã®è©³ç´°ã‚’ã€æ¤œç´¢ã—ã‚„ã™ã„ã‚ˆã†ã«è¦ç´„", "nationality": "å›½ç±ã‚’æŠ½å‡º", "availability_date": "ç¨¼åƒå¯èƒ½æ—¥ã‚’æŠ½å‡º", "desired_location": "å¸Œæœ›å‹¤å‹™åœ°ã‚’æŠ½å‡º", "desired_salary": "å¸Œæœ›å˜ä¾¡ã‚’æŠ½å‡º", "main_skills": "ä¸»è¦ãªã‚¹ã‚­ãƒ«ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§æŠ½å‡º"}}]}}
            # æœ¬ç•ª: ä»¥ä¸‹ã®ã‚¹ã‚­ãƒ«ã‚·ãƒ¼ãƒˆã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„
            ---
            {text_content}
        """
    elif doc_type == 'job':
        return f"""
            ã‚ãªãŸã¯ã€ITæ¥­ç•Œã®ã€Œæ¡ˆä»¶å®šç¾©æ›¸ã€ã‚’èª­ã¿è§£ãå°‚é–€å®¶ã§ã™ã€‚
            ã‚ãªãŸã®ä»•äº‹ã¯ã€ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰**æ¡ˆä»¶æƒ…å ±**ã‚’æŠ½å‡ºã—ã€æŒ‡å®šã•ã‚ŒãŸJSONå½¢å¼ã§æ•´ç†ã™ã‚‹ã“ã¨ã§ã™ã€‚
            ãƒ†ã‚­ã‚¹ãƒˆå†…ã«è¤‡æ•°ã®æ¡ˆä»¶æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãã‚Œãã‚Œã‚’å€‹åˆ¥ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦ãƒªã‚¹ãƒˆã«ã—ã¦ãã ã•ã„ã€‚
            # çµ¶å¯¾çš„ãªãƒ«ãƒ¼ãƒ«
            - å‡ºåŠ›ã¯ã€æŒ‡å®šã•ã‚ŒãŸJSONå½¢å¼ã®æ–‡å­—åˆ—ã®ã¿ã¨ã—ã€å‰å¾Œã«è§£èª¬ã‚„```json ```ã®ã‚ˆã†ãªã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®å›²ã¿ã‚’å«ã‚ãªã„ã§ãã ã•ã„ã€‚
            # æŒ‡ç¤º
            - `document`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ã¯ã€æ¡ˆä»¶ã®ã‚¹ã‚­ãƒ«ã‚„æ¥­å‹™å†…å®¹ã®è©³ç´°ã‚’ã€å¾Œã§æ¤œç´¢ã—ã‚„ã™ã„ã‚ˆã†ã«è‡ªç„¶ãªæ–‡ç« ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚
            - `document`ã®æ–‡ç« ã®å…ˆé ­ã«ã¯ã€å¿…ãšãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’å«ã‚ã¦ãã ã•ã„ã€‚ä¾‹ï¼šã€Œç¤¾å†…SEãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å¢—å“¡æ¡ˆä»¶ã€‚è¨­è¨ˆã€ãƒ†ã‚¹ãƒˆ...ã€
            # JSONå‡ºåŠ›å½¢å¼
            {{"jobs": [{{"project_name": "æ¡ˆä»¶åã‚’æŠ½å‡º", "document": "æ¡ˆä»¶ã®ã‚¹ã‚­ãƒ«ã‚„æ¥­å‹™å†…å®¹ã®è©³ç´°ã‚’ã€æ¤œç´¢ã—ã‚„ã™ã„ã‚ˆã†ã«è¦ç´„", "nationality_requirement": "å›½ç±è¦ä»¶ã‚’æŠ½å‡º", "start_date": "é–‹å§‹æ™‚æœŸã‚’æŠ½å‡º", "location": "å‹¤å‹™åœ°ã‚’æŠ½å‡º", "unit_price": "å˜ä¾¡ã‚„äºˆç®—ã‚’æŠ½å‡º", "required_skills": "å¿…é ˆã‚¹ã‚­ãƒ«ã‚„æ­“è¿ã‚¹ã‚­ãƒ«ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§æŠ½å‡º"}}]}}
            # æœ¬ç•ª: ä»¥ä¸‹ã®æ¡ˆä»¶æƒ…å ±ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„
            ---
            {text_content}
        """
    return ""


# backend.py

def split_text_with_llm(text_content: str) -> (dict | None, list):
    """
    ã€UIç”¨ãƒ»ä¿®æ­£ç‰ˆã€‘
    æ–‡æ›¸ã‚’åˆ†é¡ã—ã€æƒ…å ±æŠ½å‡ºã‚’è¡Œã†ã€‚é€²æ—ã‚’ st.write ã§è¡¨ç¤ºã—ã€
    æœ€çµ‚çš„ã« (çµæœ, ãƒ­ã‚°ãƒªã‚¹ãƒˆ) ã®ã‚¿ãƒ—ãƒ«ã‚’è¿”ã™ã€‚
    """
    logs = []

    conn = get_db_connection() # ã“ã®é–¢æ•°ã¯UIã‹ã‚‰å‘¼ã°ã‚Œã‚‹å‰æ
    if not conn:
        logs.append("âŒ DBæ¥ç¶šã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šã€å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã—ãŸã€‚")
        return None, logs
    
    with conn.cursor() as cur:
            # --- 1. æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã®åˆ†é¡ ---
            # â˜…â˜…â˜… AIã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãƒ­ã‚°ã‚’è¨˜éŒ² (åˆ†é¡) â˜…â˜…â˜…
            cur.execute("INSERT INTO ai_activity_log (activity_type) VALUES ('classification')")
            conn.commit()


    # ã“ã®é–¢æ•°å†…ã§ç™ºç”Ÿã—ãŸãƒ­ã‚°ã‚’åé›†ã™ã‚‹ãŸã‚ã®ãƒªã‚¹ãƒˆ
    # UIè¡¨ç¤ºã¨ã¯åˆ¥ã«ã€å‘¼ã³å‡ºã—å…ƒã«è¿”ã™
    logs_for_caller = []
    
    # --- 1. æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã®åˆ†é¡ ---
    classification_prompt = f"""
        ã‚ãªãŸã¯ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆãŒã€Œæ¡ˆä»¶æƒ…å ±ã€ã€ŒæŠ€è¡“è€…æƒ…å ±ã€ã€Œãã®ä»–ã€ã®ã©ã‚Œã«æœ€ã‚‚å½“ã¦ã¯ã¾ã‚‹ã‹åˆ¤æ–­ã—ã€æŒ‡å®šã•ã‚ŒãŸå˜èªä¸€ã¤ã ã‘ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
        # åˆ¤æ–­åŸºæº–
        - ã€Œã‚¹ã‚­ãƒ«ã‚·ãƒ¼ãƒˆã€ã€Œè·å‹™çµŒæ­´æ›¸ã€ã€Œæ°åã€ã€Œå¹´é½¢ã€ã¨ã„ã£ãŸå˜èªãŒå«ã¾ã‚Œã¦ã„ã‚Œã°ã€ŒæŠ€è¡“è€…æƒ…å ±ã€ã®å¯èƒ½æ€§ãŒé«˜ã„ã€‚
        - ã€Œå‹Ÿé›†ã€ã€Œå¿…é ˆã‚¹ã‚­ãƒ«ã€ã€Œæ­“è¿ã‚¹ã‚­ãƒ«ã€ã€Œæ±‚ã‚ã‚‹äººç‰©åƒã€ã¨ã„ã£ãŸå˜èªãŒå«ã¾ã‚Œã¦ã„ã‚Œã°ã€Œæ¡ˆä»¶æƒ…å ±ã€ã®å¯èƒ½æ€§ãŒé«˜ã„ã€‚
        # å›ç­”å½¢å¼
        - `æ¡ˆä»¶æƒ…å ±`
        - `æŠ€è¡“è€…æƒ…å ±`
        - `ãã®ä»–`
        # åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
        ---
        {text_content[:2000]}
        ---
    """
    try:
        model = genai.GenerativeModel('models/gemini-2.5-flash-lite')
        
        # UIã«ç›´æ¥é€²æ—ã‚’è¡¨ç¤º
        st.write("ğŸ“„ æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã‚’åˆ†é¡ä¸­...")
        logs_for_caller.append("ğŸ“„ æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã‚’åˆ†é¡ä¸­...") # å‘¼ã³å‡ºã—å…ƒç”¨ã®ãƒ­ã‚°ã«ã‚‚è¿½åŠ 

        response = model.generate_content(classification_prompt)
        doc_type = response.text.strip()

        st.write(f"âœ… AIã«ã‚ˆã‚‹åˆ†é¡çµæœ: **{doc_type}**")
        logs_for_caller.append(f"âœ… AIã«ã‚ˆã‚‹åˆ†é¡çµæœ: **{doc_type}**")

    except Exception as e:
        st.error(f"æ–‡æ›¸ã®åˆ†é¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logs_for_caller.append(f"âŒ æ–‡æ›¸ã®åˆ†é¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None, logs_for_caller # â˜… ä¿®æ­£: å¿…ãšã‚¿ãƒ—ãƒ«ã‚’è¿”ã™

    # --- 2. æŠ½å‡ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é¸æŠ ---
    if "æŠ€è¡“è€…æƒ…å ±" in doc_type:
        extraction_prompt = get_extraction_prompt('engineer', text_content)
    elif "æ¡ˆä»¶æƒ…å ±" in doc_type:
        extraction_prompt = get_extraction_prompt('job', text_content)
    else:
        st.warning("ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã¯æ¡ˆä»¶æƒ…å ±ã¾ãŸã¯æŠ€è¡“è€…æƒ…å ±ã¨ã—ã¦åˆ†é¡ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        logs_for_caller.append("âš ï¸ ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã¯æ¡ˆä»¶æƒ…å ±ã¾ãŸã¯æŠ€è¡“è€…æƒ…å ±ã¨ã—ã¦åˆ†é¡ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None, logs_for_caller # â˜… ä¿®æ­£: å¿…ãšã‚¿ãƒ—ãƒ«ã‚’è¿”ã™

    # --- 3. æ§‹é€ åŒ–å‡¦ç† ---
    generation_config = {"response_mime_type": "application/json"}
    safety_settings = {'HARM_CATEGORY_HARASSMENT': 'BLOCK_NONE', 'HARM_CATEGORY_HATE_SPEECH': 'BLOCK_NONE', 'HARM_CATEGORY_SEXUALLY_EXPLICIT': 'BLOCK_NONE', 'HARM_CATEGORY_DANGEROUS_CONTENT': 'BLOCK_NONE'}
    
    try:
        with st.spinner("AIãŒæƒ…å ±ã‚’æ§‹é€ åŒ–ä¸­..."):
            logs_for_caller.append("ğŸ¤– AIãŒæƒ…å ±ã‚’æ§‹é€ åŒ–ä¸­...")
            response = model.generate_content(extraction_prompt, generation_config=generation_config, safety_settings=safety_settings)
        
        raw_text = response.text
        
        # --- 4. JSONã®æŠ½å‡ºãƒ»ä¿®å¾© ---
        parsed_json = None
        start_index = raw_text.find('{')
        if start_index == -1:
            st.error("LLMå¿œç­”ã‹ã‚‰JSONé–‹å§‹æ–‡å­—'{'ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            logs_for_caller.append("âŒ LLMå¿œç­”ã‹ã‚‰JSONé–‹å§‹æ–‡å­—'{'ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return None, logs_for_caller

        brace_counter, end_index = 0, -1
        for i in range(start_index, len(raw_text)):
            char = raw_text[i]
            if char == '{': brace_counter += 1
            elif char == '}': brace_counter -= 1
            if brace_counter == 0:
                end_index = i
                break
        
        if end_index == -1:
            st.error("LLMå¿œç­”ã®JSONæ§‹é€ ãŒå£Šã‚Œã¦ã„ã¾ã™ï¼ˆæ‹¬å¼§ã®å¯¾å¿œãŒå–ã‚Œã¾ã›ã‚“ï¼‰ã€‚")
            logs_for_caller.append("âŒ LLMå¿œç­”ã®JSONæ§‹é€ ãŒå£Šã‚Œã¦ã„ã¾ã™ï¼ˆæ‹¬å¼§ã®å¯¾å¿œãŒå–ã‚Œã¾ã›ã‚“ï¼‰ã€‚")
            return None, logs_for_caller

        json_str = raw_text[start_index : end_index + 1]
        try:
            parsed_json = json.loads(json_str)
            logs_for_caller.append("âœ… JSONã®ãƒ‘ãƒ¼ã‚¹ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
        except json.JSONDecodeError as e:
            logs_for_caller.append(f"âš ï¸ JSONãƒ‘ãƒ¼ã‚¹å¤±æ•—ã€‚ä¿®å¾©è©¦è¡Œ... (ã‚¨ãƒ©ãƒ¼: {e})")
            repaired_text = re.sub(r',\s*([\}\]])', r'\1', re.sub(r'(?<!\\)\n', r'\\n', json_str))
            try:
                parsed_json = json.loads(repaired_text)
                logs_for_caller.append("âœ… JSONã®ä¿®å¾©ã¨å†ãƒ‘ãƒ¼ã‚¹ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
            except json.JSONDecodeError as final_e:
                st.error(f"JSONä¿®å¾©å¾Œã‚‚ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {final_e}")
                logs_for_caller.append(f"âŒ JSONä¿®å¾©å¾Œã‚‚ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {final_e}")
                return None, logs_for_caller

        # --- 5. æˆåŠŸæ™‚ã®æˆ»ã‚Šå€¤ ---
        if "æŠ€è¡“è€…æƒ…å ±" in doc_type:
            if "jobs" not in parsed_json: parsed_json["jobs"] = []
        elif "æ¡ˆä»¶æƒ…å ±" in doc_type:
            if "engineers" not in parsed_json: parsed_json["engineers"] = []

        # â˜… ä¿®æ­£: æˆåŠŸæ™‚ã‚‚å¿…ãšã€Œè¾æ›¸ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€ã¨ãƒ­ã‚°ã®ã‚¿ãƒ—ãƒ«ã‚’è¿”ã™
        return parsed_json, logs_for_caller

    except Exception as e:
        st.error(f"LLMã«ã‚ˆã‚‹æ§‹é€ åŒ–å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logs_for_caller.append(f"âŒ LLMã«ã‚ˆã‚‹æ§‹é€ åŒ–å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        try: st.code(response.text, language='text')
        except NameError: st.text("ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å–å¾—ã«ã‚‚å¤±æ•—ã—ã¾ã—ãŸã€‚")
        
        # â˜… ä¿®æ­£: ä¾‹å¤–ç™ºç”Ÿæ™‚ã‚‚å¿…ãšã‚¿ãƒ—ãƒ«ã‚’è¿”ã™
        return None, logs_for_caller





#@st.cache_data
def get_match_summary_with_llm(job_doc, engineer_doc):
    model = genai.GenerativeModel('models/gemini-2.5-flash-lite')
    # â–¼â–¼â–¼ å¤‰æ›´ç‚¹ 1: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å¼·åŒ– â–¼â–¼â–¼
    prompt = f"""
        ã‚ãªãŸã¯ã€çµŒé¨“è±Šå¯ŒãªITäººæç´¹ä»‹ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚
        ã‚ãªãŸã®ä»•äº‹ã¯ã€æç¤ºã•ã‚ŒãŸã€Œæ¡ˆä»¶æƒ…å ±ã€ã¨ã€ŒæŠ€è¡“è€…æƒ…å ±ã€ã‚’æ¯”è¼ƒã—ã€å®¢è¦³çš„ã‹ã¤å…·ä½“çš„ãªãƒãƒƒãƒãƒ³ã‚°è©•ä¾¡ã‚’è¡Œã†ã“ã¨ã§ã™ã€‚
        
        # çµ¶å¯¾çš„ãªãƒ«ãƒ¼ãƒ«
        - å‡ºåŠ›ã¯ã€å¿…ãšæŒ‡å®šã•ã‚ŒãŸJSONå½¢å¼ã®æ–‡å­—åˆ—ã®ã¿ã¨ã—ã¦ãã ã•ã„ã€‚è§£èª¬ã‚„ ```json ``` ã®ã‚ˆã†ãªå›²ã¿ã¯çµ¶å¯¾ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚
        - JSONå†…ã®ã™ã¹ã¦ã®æ–‡å­—åˆ—ã¯ã€å¿…ãšãƒ€ãƒ–ãƒ«ã‚¯ã‚©ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ `"` ã§å›²ã£ã¦ãã ã•ã„ã€‚
        - æ–‡å­—åˆ—ã®é€”ä¸­ã§æ”¹è¡Œã—ãªã„ã§ãã ã•ã„ã€‚æ”¹è¡ŒãŒå¿…è¦ãªå ´åˆã¯ `\\n` ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
        - `summary`ã¯æœ€ã‚‚é‡è¦ãªé …ç›®ã§ã™ã€‚çµ¶å¯¾ã«çœç•¥ã›ãšã€å¿…ãšS, A, B, C, Dã®ã„ãšã‚Œã‹ã®æ–‡å­—åˆ—ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
        
        # æŒ‡ç¤º
        ä»¥ä¸‹ã®2ã¤ã®æƒ…å ±ã‚’åˆ†æã—ã€ãƒã‚¸ãƒ†ã‚£ãƒ–ãªç‚¹ã¨æ‡¸å¿µç‚¹ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ãã ã•ã„ã€‚æœ€çµ‚çš„ã«ã€ç·åˆè©•ä¾¡ï¼ˆsummaryï¼‰ã‚’S, A, B, C, Dã®5æ®µéšã§åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
        - S: å®Œç’§ãªãƒãƒƒãƒ, A: éå¸¸ã«è‰¯ã„ãƒãƒƒãƒ, B: è‰¯ã„ãƒãƒƒãƒ, C: æ¤œè¨ã®ä½™åœ°ã‚ã‚Š, D: ãƒŸã‚¹ãƒãƒƒãƒ
        
        # JSONå‡ºåŠ›å½¢å¼
        {{"summary": "S, A, B, C, Dã®ã„ãšã‚Œã‹", "positive_points": ["ã‚¹ã‚­ãƒ«é¢ã§ã®åˆè‡´ç‚¹"], "concern_points": ["ã‚¹ã‚­ãƒ«é¢ã§ã®æ‡¸å¿µç‚¹"]}}
        ---
        # æ¡ˆä»¶æƒ…å ±
        {job_doc}
        ---
        # æŠ€è¡“è€…æƒ…å ±
        {engineer_doc}
        ---
    """
    # â–²â–²â–² å¤‰æ›´ç‚¹ 1 ã“ã“ã¾ã§ â–²â–²â–²

    generation_config = {"response_mime_type": "application/json"}
    safety_settings = {'HARM_CATEGORY_HARASSMENT': 'BLOCK_NONE', 'HARM_CATEGORY_HATE_SPEECH': 'BLOCK_NONE', 'HARM_CATEGORY_SEXUALLY_EXPLICIT': 'BLOCK_NONE', 'HARM_CATEGORY_DANGEROUS_CONTENT': 'BLOCK_NONE'}
    try:
        with st.spinner("AIãŒãƒãƒƒãƒãƒ³ã‚°æ ¹æ‹ ã‚’åˆ†æä¸­..."):
            response = model.generate_content(prompt, generation_config=generation_config, safety_settings=safety_settings)
        raw_text = response.text

        # â˜…â˜…â˜…ã€ã“ã“ã‹ã‚‰ãŒä¿®æ­£ã®æ ¸ã€‘â˜…â˜…â˜…
        # 1. æœ€åˆã® '{' ã‚’æ¢ã™
        start_index = raw_text.find('{')
        if start_index == -1:
            print(f"ERROR: get_match_summary_with_llm - No JSON object found in response: {raw_text}")
            return None

        # 2. '{' ã¨ '}' ã®å¯¾å¿œã‚’æ•°ãˆã¦ã€æœ€åˆã®å®Œå…¨ãªJSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®çµ‚ã‚ã‚Šã‚’è¦‹ã¤ã‘ã‚‹
        brace_counter = 0
        end_index = -1
        for i in range(start_index, len(raw_text)):
            char = raw_text[i]
            if char == '{':
                brace_counter += 1
            elif char == '}':
                brace_counter -= 1
            
            # brace_counterãŒ0ã«ãªã£ãŸæœ€åˆã®åœ°ç‚¹ãŒJSONã®çµ‚ã‚ã‚Š
            if brace_counter == 0:
                end_index = i
                break
        
        if end_index == -1:
            print(f"ERROR: get_match_summary_with_llm - Incomplete JSON object in response: {raw_text}")
            return None

        json_str = raw_text[start_index : end_index + 1]
        # â˜…â˜…â˜…ã€ä¿®æ­£ã“ã“ã¾ã§ã€‘â˜…â˜…â˜…

        # ãƒ‘ãƒ¼ã‚¹ã¨ä¿®å¾©ã®ãƒ­ã‚¸ãƒƒã‚¯ã¯å‰å›ã¨åŒã˜
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            print(f"WARN: Initial JSON parse failed: {e}. Attempting to repair...")
            repaired_str = re.sub(r',\s*([\}\]])', r'\1', json_str)
            repaired_str = re.sub(r'(?<!\\)\n', r'\\n', repaired_str)
            try:
                print("INFO: Retrying parse with repaired JSON string.")
                return json.loads(repaired_str)
            except json.JSONDecodeError as final_e:
                print(f"ERROR: JSON repair failed. Final parse error: {final_e}")
                print(f"Original JSON string: {json_str}")
                return None

    except Exception as e:
        print(f"ERROR: get_match_summary_with_llm - Exception during LLM call: {e}")
        return None
    
    

def update_index(index_path, items):
    embedding_model = load_embedding_model()
    if not embedding_model or not items: return
    dimension = embedding_model.get_sentence_embedding_dimension()
    index_map = faiss.IndexIDMap(faiss.IndexFlatIP(dimension))
    ids = np.array([item['id'] for item in items], dtype=np.int64)
    bodies = [str(item['document']).split('\n---\n', 1)[-1] for item in items]
    texts_with_prefix = ["passage: " + body for body in bodies]
    embeddings = embedding_model.encode(texts_with_prefix, normalize_embeddings=True, show_progress_bar=False)
    index_map.add_with_ids(embeddings, ids)
    faiss.write_index(index_map, index_path)

def search(query_text, index_path, top_k=5):
    embedding_model = load_embedding_model()
    if not embedding_model or not os.path.exists(index_path): return [], []
    index = faiss.read_index(index_path)
    if index.ntotal == 0: return [], []
    query_body = query_text.split('\n---\n', 1)[-1]
    prefixed_query = "query: " + query_body
    query_vector = embedding_model.encode([prefixed_query], normalize_embeddings=True).reshape(1, -1)
    similarities, ids = index.search(query_vector, min(top_k, index.ntotal))
    valid_ids = [int(i) for i in ids[0] if i != -1]
    valid_similarities = [similarities[0][j] for j, i in enumerate(ids[0]) if i != -1]
    return valid_similarities, valid_ids

def get_records_by_ids(table_name, ids):
    if not ids: return []
    with get_db_connection() as conn:
        with conn.cursor() as cursor:
            query = f"SELECT * FROM {table_name} WHERE id = ANY(%s)"
            cursor.execute(query, (ids,))
            results = cursor.fetchall()
            results_map = {res['id']: res for res in results}
            return [results_map[id] for id in ids if id in results_map]


def _extract_skills_from_document(document: str, item_type: str) -> set:
    """
    documentã®ãƒ¡ã‚¿æƒ…å ±ã‹ã‚‰ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆã‚’æŠ½å‡ºã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã€‚
    """
    if not document:
        return set()

    # æ¡ˆä»¶ã®å ´åˆã¯ã€Œå¿…é ˆã‚¹ã‚­ãƒ«ã€ã€æŠ€è¡“è€…ã®å ´åˆã¯ã€Œä¸»è¦ã‚¹ã‚­ãƒ«ã€ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«ã™ã‚‹
    key = "å¿…é ˆã‚¹ã‚­ãƒ«" if item_type == 'job' else "ä¸»è¦ã‚¹ã‚­ãƒ«"
    
    # [ã‚­ãƒ¼: å€¤] ã®å½¢å¼ã§ãƒ¡ã‚¿æƒ…å ±ã‚’æ­£è¦è¡¨ç¾ã§æ¤œç´¢
    match = re.search(rf"\[{key}:\s*([^\]]+)\]", document)
    if not match:
        return set()

    # æŠ½å‡ºã—ãŸã‚¹ã‚­ãƒ«æ–‡å­—åˆ—ã‚’æ•´å½¢
    skills_str = match.group(1).strip()
    if not skills_str or skills_str.lower() in ['ä¸æ˜', 'none']:
        return set()
    
    # ã‚«ãƒ³ãƒã‚„å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã§åŒºåˆ‡ã‚Šã€å„ã‚¹ã‚­ãƒ«ã‚’å°æ–‡å­—åŒ–ãƒ»ç©ºç™½é™¤å»ã—ã¦ã‚»ãƒƒãƒˆã«æ ¼ç´
    skills = {skill.strip().lower() for skill in re.split(r'[,ã€ï¼Œ\s]+', skills_str) if skill.strip()}
    return skills

# backend.py ã® run_matching_for_item é–¢æ•°ã‚’ã“ã¡ã‚‰ã«ç½®ãæ›ãˆã¦ãã ã•ã„

def run_matching_for_item(item_data, item_type, conn, now_str):
    # â–¼â–¼â–¼ã€ã“ã®é–¢æ•°å…¨ä½“ã‚’ç½®ãæ›ãˆã¦ãã ã•ã„ã€‘â–¼â–¼â–¼
    with conn.cursor() as cursor:
        # 1. æ¤œç´¢å¯¾è±¡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€ãƒ†ãƒ¼ãƒ–ãƒ«ã€åç§°ã‚’æ±ºå®š
        if item_type == 'job':
            query_text, index_path = item_data['document'], ENGINEER_INDEX_FILE
            target_table_name = 'engineers'
            source_name = item_data.get('project_name', f"æ¡ˆä»¶ID:{item_data['id']}")
        else: # item_type == 'engineer'
            query_text, index_path = item_data['document'], JOB_INDEX_FILE
            target_table_name = 'jobs'
            source_name = item_data.get('name', f"æŠ€è¡“è€…ID:{item_data['id']}")

        # TOP_K_CANDIDATESã¯AIè©•ä¾¡ã®ä¸Šé™æ•°ãªã®ã§ã€ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢æ™‚ã¯å°‘ã—å¤šã‚ã«å–å¾—ã™ã‚‹
        search_limit = TOP_K_CANDIDATES * 2

        # 2. Faissã«ã‚ˆã‚‹é¡ä¼¼åº¦æ¤œç´¢ã‚’å®Ÿè¡Œ
        similarities, ids = search(query_text, index_path, top_k=search_limit)
        if not ids:
            st.write(f"â–¶ ã€{source_name}ã€(ID:{item_data['id']}) ã®é¡ä¼¼å€™è£œã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return

        # 3. æ¤œç´¢çµæœã®å€™è£œãƒ‡ãƒ¼ã‚¿ã‚’DBã‹ã‚‰ä¸€æ‹¬å–å¾—
        candidate_records = get_records_by_ids(target_table_name, ids)
        candidate_map = {record['id']: record for record in candidate_records}

        st.write(f"â–¶ ã€{source_name}ã€(ID:{item_data['id']}) ã®é¡ä¼¼å€™è£œ **{len(ids)}ä»¶** ã‚’ç™ºè¦‹ã€‚")
        
        # â–¼â–¼â–¼ã€ã“ã“ã‹ã‚‰ãŒã‚¹ã‚­ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®ä¿®æ­£ç®‡æ‰€ã§ã™ã€‘â–¼â–¼â–¼

        # --- ã‚¹ã‚­ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ– ---
        st.info("â„¹ï¸ ç¾åœ¨ã€ã‚¹ã‚­ãƒ«ã«ã‚ˆã‚‹äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚ã™ã¹ã¦ã®é¡ä¼¼å€™è£œã‚’AIè©•ä¾¡ã®å¯¾è±¡ã¨ã—ã¾ã™ã€‚")
        
        valid_candidates = []
        for sim, candidate_id in zip(similarities, ids):
            # AIè©•ä¾¡ã®å¯¾è±¡ã‚’ TOP_K_CANDIDATES ä»¶ã«çµã‚‹
            if len(valid_candidates) >= TOP_K_CANDIDATES:
                break
            
            candidate_record = candidate_map.get(candidate_id)
            if not candidate_record: continue
            
            valid_candidates.append({
                'sim': sim,
                'id': candidate_id,
                'record': candidate_record,
                'name': candidate_record.get('project_name') or candidate_record.get('name') or f"ID:{candidate_id}"
            })
        
        # --- å°†æ¥çš„ã«å†åº¦æœ‰åŠ¹åŒ–ã™ã‚‹ãŸã‚ã®å…ƒã‚³ãƒ¼ãƒ‰ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼‰ ---
        #
        # source_skills = _extract_skills_from_document(item_data['document'], item_type)
        # if not source_skills:
        #     st.write(f"  - æ¤œç´¢å…ƒã€{source_name}ã€ã®ã‚¹ã‚­ãƒ«æƒ…å ±ãŒæŠ½å‡ºã§ããšã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        #     for sim, candidate_id in zip(similarities, ids):
        #         if len(valid_candidates) >= TOP_K_CANDIDATES: break
        #         candidate_record = candidate_map.get(candidate_id)
        #         if not candidate_record: continue
        #         valid_candidates.append({
        #             'sim': sim, 'id': candidate_id, 'record': candidate_record,
        #             'name': candidate_record.get('project_name') or candidate_record.get('name') or f"ID:{candidate_id}"
        #         })
        # else:
        #     SKILL_MATCH_RATIO_THRESHOLD = 0.5  # ä¾‹: 50%
        #     st.write(f"  - ã‚¹ã‚­ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶: ä¸€è‡´ç‡ãŒ {SKILL_MATCH_RATIO_THRESHOLD*100:.0f}% ä»¥ä¸Šã®å€™è£œã‚’å¯¾è±¡ã¨ã—ã¾ã™ã€‚")
        #     for sim, candidate_id in zip(similarities, ids):
        #         if len(valid_candidates) >= TOP_K_CANDIDATES: break
        #         candidate_record = candidate_map.get(candidate_id)
        #         if not candidate_record: continue
        #         candidate_item_type = 'engineer' if item_type == 'job' else 'job'
        #         candidate_skills = _extract_skills_from_document(candidate_record['document'], candidate_item_type)
        #         intersection_count = len(source_skills.intersection(candidate_skills))
        #         match_ratio = intersection_count / len(source_skills) if len(source_skills) > 0 else 0
        #         if match_ratio >= SKILL_MATCH_RATIO_THRESHOLD:
        #             valid_candidates.append({
        #                 'sim': sim, 'id': candidate_id, 'record': candidate_record,
        #                 'name': candidate_record.get('project_name') or candidate_record.get('name') or f"ID:{candidate_id}"
        #             })
        
        # â–²â–²â–²ã€ã‚¹ã‚­ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®ä¿®æ­£ã“ã“ã¾ã§ã€‘â–²â–²â–²

        if not valid_candidates:
            st.write(f"âœ… é¡ä¼¼å€™è£œã¯è¦‹ã¤ã‹ã‚Šã¾ã—ãŸãŒã€æœ‰åŠ¹ãªãƒ¬ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            return

        st.write(f"âœ… AIè©•ä¾¡å¯¾è±¡ã®å€™è£œã‚’ **{len(valid_candidates)}ä»¶** ã«çµã‚Šè¾¼ã¿ã¾ã—ãŸã€‚AIè©•ä¾¡ã‚’é–‹å§‹ã—ã¾ã™...")

        # 5. æœ‰åŠ¹ãªå€™è£œãƒªã‚¹ãƒˆã«å¯¾ã—ã¦AIè©•ä¾¡ã¨DBä¿å­˜ã‚’è¡Œã†
        for candidate_info in valid_candidates:
            score = float(candidate_info['sim']) * 100
            if score < MIN_SCORE_THRESHOLD: continue

            if item_type == 'job':
                job_doc, engineer_doc = item_data['document'], candidate_info['record']['document']
                job_id, engineer_id = item_data['id'], candidate_info['id']
            else:
                job_doc, engineer_doc = candidate_info['record']['document'], item_data['document']
                job_id, engineer_id = candidate_info['id'], item_data['id']

            llm_result = get_match_summary_with_llm(job_doc, engineer_doc)

            if llm_result and 'summary' in llm_result:
                grade = llm_result.get('summary')
                positive_points = json.dumps(llm_result.get('positive_points', []), ensure_ascii=False)
                concern_points = json.dumps(llm_result.get('concern_points', []), ensure_ascii=False)

                if grade in ['S', 'A', 'B']:
                    try:
                        cursor.execute(
                            'INSERT INTO matching_results (job_id, engineer_id, score, created_at, grade, positive_points, concern_points) VALUES (%s, %s, %s, %s, %s, %s, %s) ON CONFLICT (job_id, engineer_id) DO NOTHING',
                            (job_id, engineer_id, score, now_str, grade, positive_points, concern_points)
                        )
                        st.write(f"  - å€™è£œ: ã€{candidate_info['name']}ã€ -> ãƒãƒƒãƒãƒ³ã‚°è©•ä¾¡: **{grade}** (ã‚¹ã‚³ã‚¢: {score:.2f}) ... âœ… DBã«ä¿å­˜")
                    except Exception as e:
                        st.write(f"  - DBä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                else:
                    st.write(f"  - å€™è£œ: ã€{candidate_info['name']}ã€ -> ãƒãƒƒãƒãƒ³ã‚°è©•ä¾¡: **{grade}** (ã‚¹ã‚³ã‚¢: {score:.2f}) ... âŒ ã‚¹ã‚­ãƒƒãƒ—")
            else:
                st.write(f"  - å€™è£œ: ã€{candidate_info['name']}ã€ -> LLMè©•ä¾¡å¤±æ•—ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")



def process_single_content(source_data: dict, progress_bar, base_progress: float, progress_per_email: float):
    """
    å˜ä¸€ã®ãƒ¡ãƒ¼ãƒ«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å‡¦ç†ã—ã€é€²æ—ãƒãƒ¼ã‚’æ›´æ–°ã™ã‚‹ã€‚
    ãƒ¡ãƒ¼ãƒ«ã‹ã‚‰ã®æƒ…å ±æŠ½å‡ºã¨DBã¸ã®ç™»éŒ²ã®ã¿ã‚’è¡Œã„ã€ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ã¯è¡Œã‚ãªã„ã€‚
    
    Args:
        source_data (dict): ãƒ¡ãƒ¼ãƒ«ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã€‚
        progress_bar: Streamlitã®ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
        base_progress (float): ã“ã®ãƒ¡ãƒ¼ãƒ«å‡¦ç†é–‹å§‹å‰ã®é€²æ—å€¤ã€‚
        progress_per_email (float): ã“ã®ãƒ¡ãƒ¼ãƒ«1ä»¶ã‚ãŸã‚Šã®é€²æ—ã®é‡ã¿ã€‚
    """
    if not source_data: 
        st.warning("å‡¦ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")
        return False

    # ã‚¹ãƒ†ãƒƒãƒ—1: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è§£æ (LLM) - ã“ã®ãƒ¡ãƒ¼ãƒ«å‡¦ç†ã®å¤§éƒ¨åˆ†ã‚’å ã‚ã‚‹ã¨ä»®å®š
    valid_attachments_content = [f"\n\n--- æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«: {att['filename']} ---\n{att.get('content', '')}" for att in source_data.get('attachments', []) if att.get('content') and not att.get('content', '').startswith("[") and not att.get('content', '').endswith("]")]
    if valid_attachments_content: 
        st.write(f"â„¹ï¸ {len(valid_attachments_content)}ä»¶ã®æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’è§£æã«å«ã‚ã¾ã™ã€‚")
    full_text_for_llm = source_data.get('body', '') + "".join(valid_attachments_content)
    if not full_text_for_llm.strip(): 
        st.warning("è§£æå¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return False
    
    # split_text_with_llm ã¯å†…éƒ¨ã§ã‚¹ãƒ”ãƒŠãƒ¼ã‚„ãƒ­ã‚°ã‚’è¡¨ç¤ºã™ã‚‹
    parsed_data = split_text_with_llm(full_text_for_llm)
    
    # é€²æ—ãƒãƒ¼ã‚’æ›´æ–° (ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è§£æå®Œäº†)
    # ã“ã®ãƒ¡ãƒ¼ãƒ«ã«å‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸé€²æ—ã®ã†ã¡ã€60%ãŒå®Œäº†ã—ãŸã¨ã¿ãªã™
    current_progress = base_progress + (progress_per_email * 0.6)
    progress_bar.progress(current_progress, text="ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è§£æå®Œäº†")

    if not parsed_data: 
        return False
    
    new_jobs_data, new_engineers_data = parsed_data.get("jobs", []), parsed_data.get("engineers", [])
    if not new_jobs_data and not new_engineers_data: 
        st.warning("LLMã¯ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ¡ˆä»¶æƒ…å ±ã¾ãŸã¯æŠ€è¡“è€…æƒ…å ±ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        return False
    
    # ã‚¹ãƒ†ãƒƒãƒ—2: æŠ½å‡ºã•ã‚ŒãŸæƒ…å ±ã®DBã¸ã®ä¿å­˜
    st.write("æŠ½å‡ºã•ã‚ŒãŸæƒ…å ±ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã—ã¾ã™...")
    with get_db_connection() as conn:
        with conn.cursor() as cursor:
            now_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            received_at_dt = source_data.get('received_at')
            json_data_to_store = source_data.copy()
            if isinstance(json_data_to_store.get('received_at'), datetime):
                json_data_to_store['received_at'] = json_data_to_store['received_at'].isoformat()
            source_json_str = json.dumps(json_data_to_store, ensure_ascii=False, indent=2)

            newly_added_items_count = 0
            
            for item_data in new_jobs_data:
                doc = item_data.get("document") or full_text_for_llm
                project_name = item_data.get("project_name", "åç§°æœªå®šã®æ¡ˆä»¶")
                meta_info = _build_meta_info_string('job', item_data)
                full_document = meta_info + doc
                cursor.execute('INSERT INTO jobs (project_name, document, source_data_json, created_at, received_at) VALUES (%s, %s, %s, %s, %s) RETURNING id', (project_name, full_document, source_json_str, now_str, received_at_dt))
                item_id = cursor.fetchone()[0]
                st.write(f"âœ… æ–°ã—ã„æ¡ˆä»¶ã‚’ç™»éŒ²ã—ã¾ã—ãŸ: ã€{project_name}ã€ (ID: {item_id})")
                newly_added_items_count += 1
            
            for item_data in new_engineers_data:
                doc = item_data.get("document") or full_text_for_llm
                engineer_name = item_data.get("name", "åç§°ä¸æ˜ã®æŠ€è¡“è€…")
                meta_info = _build_meta_info_string('engineer', item_data)
                full_document = meta_info + doc
                cursor.execute('INSERT INTO engineers (name, document, source_data_json, created_at, received_at) VALUES (%s, %s, %s, %s, %s) RETURNING id', (engineer_name, full_document, source_json_str, now_str, received_at_dt))
                item_id = cursor.fetchone()[0]
                st.write(f"âœ… æ–°ã—ã„æŠ€è¡“è€…ã‚’ç™»éŒ²ã—ã¾ã—ãŸ: ã€{engineer_name}ã€ (ID: {item_id})")
                newly_added_items_count += 1
            
            # ã€å‰Šé™¤ã€‘ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°ã®å‡¦ç† (ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ãŒãªã„ãŸã‚ä¸è¦)
            # cursor.execute('SELECT id, document FROM jobs WHERE is_hidden = 0'); all_active_jobs = cursor.fetchall()
            # cursor.execute('SELECT id, document FROM engineers WHERE is_hidden = 0'); all_active_engineers = cursor.fetchall()
            # if all_active_jobs: update_index(JOB_INDEX_FILE, all_active_jobs)
            # if all_active_engineers: update_index(ENGINEER_INDEX_FILE, all_active_engineers)
            
            # ã€å‰Šé™¤ã€‘å†ãƒãƒƒãƒãƒ³ã‚°ã®å‡¦ç† (ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ãŒãªã„ãŸã‚ä¸è¦)
            # for new_job in newly_added_jobs:
            #     run_matching_for_item(new_job, 'job', conn, now_str)
            # for new_engineer in newly_added_engineers:
            #     run_matching_for_item(new_engineer, 'engineer', conn, now_str)
        conn.commit()

    # é€²æ—ãƒãƒ¼ã‚’æ›´æ–° (ã“ã®ãƒ¡ãƒ¼ãƒ«ã®å‡¦ç†ãŒ100%å®Œäº†)
    # ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ãŒãªããªã£ãŸã®ã§ã€é€²æ—ã®é‡ã¿ä»˜ã‘ã‚’èª¿æ•´
    current_progress = base_progress + progress_per_email
    progress_bar.progress(current_progress, text="æƒ…å ±ä¿å­˜å®Œäº†ï¼")
    
    return True






def extract_text_from_pdf(file_bytes):
    try:
        with fitz.open(stream=file_bytes, filetype="pdf") as doc:
            text = "".join(page.get_text() for page in doc)
        return text if text.strip() else "[PDFãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¤±æ•—: å†…å®¹ãŒç©ºã¾ãŸã¯ç”»åƒPDF]"
    except Exception as e:
        return f"[PDFãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}]"

def extract_text_from_docx(file_bytes):
    try:
        doc = docx.Document(io.BytesIO(file_bytes))
        text = "\n".join([para.text for para in doc.paragraphs])
        return text if text.strip() else "[DOCXãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¤±æ•—: å†…å®¹ãŒç©º]"
    except Exception as e:
        return f"[DOCXãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}]"

def extract_text_from_excel(file_bytes: bytes) -> str:
    """
    Excelãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.xlsx, .xlsï¼‰ã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘å–ã‚Šã€
    ã™ã¹ã¦ã®ã‚·ãƒ¼ãƒˆã®å†…å®¹ã‚’ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§çµåˆã—ã¦è¿”ã™ã€‚
    """
    try:
        # ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ pandas ãŒèª­ã¿è¾¼ã‚ã‚‹å½¢å¼ã«å¤‰æ›
        excel_file = io.BytesIO(file_bytes)
        
        # Excelãƒ•ã‚¡ã‚¤ãƒ«å†…ã®å…¨ã‚·ãƒ¼ãƒˆåã‚’å–å¾—
        xls = pd.ExcelFile(excel_file)
        sheet_names = xls.sheet_names
        
        all_text_parts = []
        
        # å„ã‚·ãƒ¼ãƒˆã‚’ãƒ«ãƒ¼ãƒ—ã§å‡¦ç†
        for sheet_name in sheet_names:
            # ã‚·ãƒ¼ãƒˆã‚’DataFrameã¨ã—ã¦èª­ã¿è¾¼ã‚€
            df = pd.read_excel(xls, sheet_name=sheet_name, header=None)
            
            # DataFrameãŒç©ºã§ãªã„ã“ã¨ã‚’ç¢ºèª
            if not df.empty:
                # ã‚·ãƒ¼ãƒˆã®å†…å®¹ã‚’æ–‡å­—åˆ—ã«å¤‰æ›ï¼ˆCSVå½¢å¼ã«ä¼¼ã›ã‚‹ï¼‰
                # å„ã‚»ãƒ«ã‚’ã‚¿ãƒ–åŒºåˆ‡ã‚Šã€å„è¡Œã‚’æ”¹è¡Œã§çµåˆã™ã‚‹
                sheet_text = df.to_string(header=False, index=False, na_rep='')
                
                # ã‚·ãƒ¼ãƒˆåã¨å†…å®¹ã‚’çµåˆã—ã¦ãƒªã‚¹ãƒˆã«è¿½åŠ 
                all_text_parts.append(f"\n--- ã‚·ãƒ¼ãƒˆ: {sheet_name} ---\n{sheet_text}")

        if not all_text_parts:
            return "[Excelãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¤±æ•—: ãƒ•ã‚¡ã‚¤ãƒ«å†…ã«è§£æå¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“]"
            
        # å…¨ã‚·ãƒ¼ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆã—ã¦è¿”ã™
        return "".join(all_text_parts)

    except Exception as e:
        # pandas ãŒèª­ã¿è¾¼ã‚ãªã„å½¢å¼ã‚„ç ´æãƒ•ã‚¡ã‚¤ãƒ«ãªã©ã®ã‚¨ãƒ©ãƒ¼ã‚’ã‚­ãƒ£ãƒƒãƒ
        return f"[Excelãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}]"
    

def get_email_contents(msg) -> dict:
    subject = str(make_header(decode_header(msg["subject"]))) if msg["subject"] else ""
    from_ = str(make_header(decode_header(msg["from"]))) if msg["from"] else ""
    received_at = parsedate_to_datetime(msg["Date"]) if msg["Date"] else None

    body_text, attachments = "", []
    if msg.is_multipart():
        for part in msg.walk():
            content_type, content_disposition = part.get_content_type(), str(part.get("Content-Disposition"))
            if 'text/plain' in content_type and 'attachment' not in content_disposition:
                charset = part.get_content_charset()
                try: body_text += part.get_payload(decode=True).decode(charset or 'utf-8', errors='ignore')
                except Exception: body_text += part.get_payload(decode=True).decode('utf-8', errors='ignore')
            if 'attachment' in content_disposition and (raw_filename := part.get_filename()):
                filename = str(make_header(decode_header(raw_filename)))
                st.write(f"ğŸ“„ æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚")
                file_bytes, lower_filename = part.get_payload(decode=True), filename.lower()

                #if lower_filename.endswith(".pdf"): attachments.append({"filename": filename, "content": extract_text_from_pdf(file_bytes)})
                #elif lower_filename.endswith(".docx"): attachments.append({"filename": filename, "content": extract_text_from_docx(file_bytes)})
                #elif lower_filename.endswith(".txt"): attachments.append({"filename": filename, "content": file_bytes.decode('utf-8', errors='ignore')})
                #else: st.write(f"â„¹ï¸ æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã¯æœªå¯¾å¿œã®å½¢å¼ã®ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

                # â–¼â–¼â–¼ã€ã“ã“ã‹ã‚‰ãŒä¿®æ­£ãƒ»è¿½åŠ ç®‡æ‰€ã€‘â–¼â–¼â–¼
                if lower_filename.endswith(".pdf"):
                    attachments.append({"filename": filename, "content": extract_text_from_pdf(file_bytes)})
                
                elif lower_filename.endswith(".docx"):
                    attachments.append({"filename": filename, "content": extract_text_from_docx(file_bytes)})

                elif lower_filename.endswith((".xlsx", ".xls")): # .xlsx ã¨ .xls ã®ä¸¡æ–¹ã«å¯¾å¿œ
                    attachments.append({"filename": filename, "content": extract_text_from_excel(file_bytes)})

                elif lower_filename.endswith(".txt"):
                    attachments.append({"filename": filename, "content": file_bytes.decode('utf-8', errors='ignore')})
                
                else:
                    st.write(f"â„¹ï¸ æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã¯æœªå¯¾å¿œã®å½¢å¼ã®ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                
                # â–²â–²â–²ã€ä¿®æ­£ãƒ»è¿½åŠ ã“ã“ã¾ã§ã€‘â–²â–²â–²


    else:
        charset = msg.get_content_charset()
        try: body_text = msg.get_payload(decode=True).decode(charset or 'utf-8', errors='ignore')
        except Exception: body_text = msg.get_payload(decode=True).decode('utf-8', errors='ignore')
    
    return {"subject": subject, "from": from_, "received_at": received_at, "body": body_text.strip(), "attachments": attachments}




# backend.py

def fetch_and_process_emails():
    try:
        # 1. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ä»¶æ•°ã‚’å–å¾—
        config = load_app_config()
        # .get() ã‚’ä½¿ã£ã¦å®‰å…¨ã«å€¤ã‚’å–å¾—ã—ã€å–å¾—ã§ããªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§10ã‚’è¨­å®š
        FETCH_LIMIT = config.get("email_processing", {}).get("fetch_limit", 10)

        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®åˆæœŸåŒ–ã¨é‡ã¿ä»˜ã‘å®šç¾©
        progress_bar = st.progress(0, text="å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
        
        WEIGHT_CONNECT = 0.05  # ã‚µãƒ¼ãƒãƒ¼æ¥ç¶šã«5%
        WEIGHT_FETCH_IDS = 0.05 # ãƒ¡ãƒ¼ãƒ«IDãƒªã‚¹ãƒˆå–å¾—ã«5%
        WEIGHT_LOOP = 0.90     # ãƒ¡ãƒ¼ãƒ«ã”ã¨ã®ãƒ«ãƒ¼ãƒ—å‡¦ç†å…¨ä½“ã§90%

        # ãƒ¡ãƒ¼ãƒ«ã‚µãƒ¼ãƒãƒ¼æ¥ç¶š
        try:
            SERVER, USER, PASSWORD = st.secrets["EMAIL_SERVER"], st.secrets["EMAIL_USER"], st.secrets["EMAIL_PASSWORD"]
        except KeyError as e:
            st.error(f"ãƒ¡ãƒ¼ãƒ«ã‚µãƒ¼ãƒãƒ¼ã®æ¥ç¶šæƒ…å ±ãŒSecretsã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“: {e}")
            return False, ""
        
        try:
            mail = imaplib.IMAP4_SSL(SERVER)
            mail.login(USER, PASSWORD)
            mail.select('inbox')
            progress_bar.progress(WEIGHT_CONNECT, text="ãƒ¡ãƒ¼ãƒ«ã‚µãƒ¼ãƒãƒ¼æ¥ç¶šå®Œäº†")
        except Exception as e:
            st.error(f"ãƒ¡ãƒ¼ãƒ«ã‚µãƒ¼ãƒãƒ¼ã¸ã®æ¥ç¶šã¾ãŸã¯ãƒ­ã‚°ã‚¤ãƒ³ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return False, ""
        
        total_processed_count, checked_count = 0, 0
        try:
            with st.status("æœ€æ–°ã®æœªèª­ãƒ¡ãƒ¼ãƒ«ã‚’å–å¾—ãƒ»å‡¦ç†ä¸­...", expanded=True) as status:
                _, messages = mail.search(None, 'UNSEEN')
                email_ids = messages[0].split()
                
                progress_bar.progress(WEIGHT_CONNECT + WEIGHT_FETCH_IDS, text="æœªèª­ãƒ¡ãƒ¼ãƒ«IDãƒªã‚¹ãƒˆå–å¾—å®Œäº†")
                
                if not email_ids:
                    st.write("å‡¦ç†å¯¾è±¡ã®æœªèª­ãƒ¡ãƒ¼ãƒ«ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                else:
                    latest_ids = email_ids[::-1][:FETCH_LIMIT]
                    checked_count = len(latest_ids)
                    st.write(f"æœ€æ–°ã®æœªèª­ãƒ¡ãƒ¼ãƒ« {checked_count}ä»¶ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚")

                    # ãƒ¡ãƒ¼ãƒ«1ä»¶ã‚ãŸã‚Šã®é€²æ—ã®å‰²åˆã‚’è¨ˆç®—
                    progress_per_email = WEIGHT_LOOP / checked_count if checked_count > 0 else 0
                    
                    for i, email_id in enumerate(latest_ids):
                        # ã“ã®ãƒ«ãƒ¼ãƒ—é–‹å§‹æ™‚ç‚¹ã§ã®ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹é€²æ—
                        base_progress_for_this_email = (WEIGHT_CONNECT + WEIGHT_FETCH_IDS) + (i * progress_per_email)
                        
                        # ãƒ¡ãƒ¼ãƒ«å†…å®¹å–å¾—ã®é€²æ—
                        progress_bar.progress(base_progress_for_this_email, text=f"ãƒ¡ãƒ¼ãƒ«({i+1}/{checked_count})ã®å†…å®¹ã‚’å–å¾—ä¸­...")
                        
                        _, msg_data = mail.fetch(email_id, '(RFC822)')
                        for response_part in msg_data:
                            if isinstance(response_part, tuple):
                                msg = email.message_from_bytes(response_part[1])
                                source_data = get_email_contents(msg)
                                
                                # ãƒ¡ãƒ¼ãƒ«å†…å®¹å–å¾—å®Œäº†å¾Œã®é€²æ— (ãƒ¡ãƒ¼ãƒ«1ä»¶ã®å‡¦ç†ã®20%ã‚’å‰²ã‚Šå½“ã¦)
                                fetch_complete_progress = base_progress_for_this_email + (progress_per_email * 0.2)
                                progress_bar.progress(fetch_complete_progress, text=f"ãƒ¡ãƒ¼ãƒ«({i+1}/{checked_count})ã®å†…å®¹å–å¾—å®Œäº†")

                                if source_data['body'] or source_data['attachments']:
                                    st.write("---")
                                    st.write(f"âœ… ãƒ¡ãƒ¼ãƒ«ID {email_id.decode()} ã‚’å‡¦ç†ã—ã¾ã™ã€‚")
                                    received_at_str = source_data['received_at'].strftime('%Y-%m-%d %H:%M:%S') if source_data.get('received_at') else 'å–å¾—ä¸å¯'
                                    st.write(f"   å—ä¿¡æ—¥æ™‚: {received_at_str}")
                                    st.write(f"   å·®å‡ºäºº: {source_data.get('from', 'å–å¾—ä¸å¯')}")
                                    st.write(f"   ä»¶å: {source_data.get('subject', 'å–å¾—ä¸å¯')}")
                                    
                                    # process_single_content ã«é€²æ—ç®¡ç†æƒ…å ±ã‚’æ¸¡ã™
                                    # æ®‹ã‚Šã®80%ã®é€²æ—ã‚’ã“ã®é–¢æ•°ã«å§”ã­ã‚‹
                                    if process_single_content(source_data, progress_bar, fetch_complete_progress, progress_per_email * 0.8):
                                        total_processed_count += 1
                                        mail.store(email_id, '+FLAGS', '\\Seen')
                                else:
                                    st.write(f"âœ–ï¸ ãƒ¡ãƒ¼ãƒ«ID {email_id.decode()} ã¯æœ¬æ–‡ã‚‚æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ç„¡ã„ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                                    # ã‚¹ã‚­ãƒƒãƒ—ã—ãŸå ´åˆã§ã‚‚ã€ã“ã®ãƒ¡ãƒ¼ãƒ«ã®é€²æ—ã¯å®Œäº†ã—ãŸã“ã¨ã«ã™ã‚‹
                                    final_progress_for_this_email = base_progress_for_this_email + progress_per_email
                                    progress_bar.progress(final_progress_for_this_email, text=f"ãƒ¡ãƒ¼ãƒ«({i+1}/{checked_count}) ã‚¹ã‚­ãƒƒãƒ—å®Œäº†")
                        
                        st.write(f"({i+1}/{checked_count}) ãƒã‚§ãƒƒã‚¯å®Œäº†")
                
                status.update(label="ãƒ¡ãƒ¼ãƒ«ãƒã‚§ãƒƒã‚¯å®Œäº†", state="complete")
        finally:
            mail.close()
            mail.logout()
    
        # æœ€çµ‚çš„ã«ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’100%ã«ã™ã‚‹
        progress_bar.progress(1.0, text="å…¨å‡¦ç†å®Œäº†ï¼")
        
        # å‡¦ç†å®Œäº†å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        if checked_count > 0:
            if total_processed_count > 0:
                st.success(f"ãƒã‚§ãƒƒã‚¯ã—ãŸ {checked_count} ä»¶ã®ãƒ¡ãƒ¼ãƒ«ã®ã†ã¡ã€{total_processed_count} ä»¶ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã€ä¿å­˜ã—ã¾ã—ãŸã€‚")
                st.balloons()
            else:
                st.warning(f"ãƒ¡ãƒ¼ãƒ«ã‚’ {checked_count} ä»¶ãƒã‚§ãƒƒã‚¯ã—ã¾ã—ãŸãŒã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã§ãã‚‹æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            st.info("å‡¦ç†å¯¾è±¡ã¨ãªã‚‹æ–°ã—ã„æœªèª­ãƒ¡ãƒ¼ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            
        return True, "" # ãƒ­ã‚°ã‚¹ãƒˆãƒªãƒ¼ãƒ ã¯ä½¿ã‚ãªã„ã®ã§ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™
    except Exception as e:
        st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return False, ""





# --- æ®‹ã‚Šã®é–¢æ•° (å¤‰æ›´ãªã—) ---
def hide_match(result_id):
    if not result_id: st.warning("IDãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"); return False
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute('UPDATE matching_results SET is_hidden = 1 WHERE id = %s', (result_id,))
                if cursor.rowcount > 0: st.toast(f"ãƒãƒƒãƒãƒ³ã‚°çµæœ (ID: {result_id}) ã‚’éè¡¨ç¤ºã«ã—ã¾ã—ãŸã€‚"); conn.commit(); return True
                else: st.warning(f"ãƒãƒƒãƒãƒ³ã‚°çµæœ (ID: {result_id}) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"); return False
    except (Exception, psycopg2.Error) as e: st.error(f"DBæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}"); return False

def get_all_users():
    with get_db_connection() as conn:
        with conn.cursor() as cursor:
            cursor.execute("SELECT id, username FROM users ORDER BY id"); return cursor.fetchall()

def assign_user_to_job(job_id, user_id):
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cur: cur.execute("UPDATE jobs SET assigned_user_id = %s WHERE id = %s", (user_id, job_id))
            conn.commit(); return True
        except (Exception, psycopg2.Error) as e: print(f"æ‹…å½“è€…å‰²ã‚Šå½“ã¦ã‚¨ãƒ©ãƒ¼: {e}"); conn.rollback(); return False

def set_job_visibility(job_id, is_hidden):
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cur: cur.execute("UPDATE jobs SET is_hidden = %s WHERE id = %s", (is_hidden, job_id))
            conn.commit(); return True
        except (Exception, psycopg2.Error) as e: print(f"è¡¨ç¤ºçŠ¶æ…‹ã®æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}"); conn.rollback(); return False

def assign_user_to_engineer(engineer_id, user_id):
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cur: cur.execute("UPDATE engineers SET assigned_user_id = %s WHERE id = %s", (user_id, engineer_id))
            conn.commit(); return True
        except (Exception, psycopg2.Error) as e: print(f"æŠ€è¡“è€…ã¸ã®æ‹…å½“è€…å‰²ã‚Šå½“ã¦ã‚¨ãƒ©ãƒ¼: {e}"); conn.rollback(); return False

def set_engineer_visibility(engineer_id, is_hidden):
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cur: cur.execute("UPDATE engineers SET is_hidden = %s WHERE id = %s", (is_hidden, engineer_id))
            conn.commit(); return True
        except (Exception, psycopg2.Error) as e: print(f"æŠ€è¡“è€…ã®è¡¨ç¤ºçŠ¶æ…‹ã®æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}"); conn.rollback(); return False

def update_engineer_source_json(engineer_id, new_json_str):
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cur: cur.execute("UPDATE engineers SET source_data_json = %s WHERE id = %s", (new_json_str, engineer_id))
            conn.commit(); return True
        except (Exception, psycopg2.Error) as e: print(f"æŠ€è¡“è€…ã®JSONãƒ‡ãƒ¼ã‚¿æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}"); conn.rollback(); return False



def generate_proposal_reply_with_llm(job_summary, engineer_summary, engineer_name, project_name):
    """
    ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ãƒ»å®Œæˆç‰ˆã€‘
    ææ¡ˆãƒ¡ãƒ¼ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã€‚DBæ¥ç¶šã®ç¢ºç«‹ã‚‚ã“ã®é–¢æ•°å†…ã§è¡Œã†ã€‚
    """
    if not all([job_summary, engineer_summary, engineer_name, project_name]):
        return "æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€ææ¡ˆãƒ¡ãƒ¼ãƒ«ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

    # â–¼â–¼â–¼ã€ã“ã“ã‹ã‚‰ãŒä¿®æ­£ã®æ ¸ã€‘â–¼â–¼â–¼
    
    # --- 1. AIã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãƒ­ã‚°ã‚’å…ˆã«è¨˜éŒ²ã™ã‚‹ ---
    # ã“ã®å‡¦ç†ã¯ç‹¬ç«‹ã—ã¦å®Ÿè¡Œã—ã€ä¸‡ãŒä¸€å¤±æ•—ã—ã¦ã‚‚ãƒ¡ãƒ¼ãƒ«ç”Ÿæˆã¯ç¶šè¡Œã™ã‚‹
    try:
        # ã“ã®é–¢æ•°å†…ã§DBæ¥ç¶šã‚’ç¢ºç«‹ã™ã‚‹
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³å¯¾å¿œã®ãŸã‚ã€created_atã‚‚æ˜ç¤ºçš„ã«æŒ‡å®šã™ã‚‹
                cur.execute("INSERT INTO ai_activity_log (activity_type, created_at) VALUES ('proposal_generation', NOW())")
            conn.commit()
    except Exception as db_err:
        # ãƒ­ã‚°è¨˜éŒ²ã®å¤±æ•—ã¯ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«å‡ºåŠ›ã™ã‚‹ã®ã¿ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¯å½±éŸ¿ã‚’ä¸ãˆãªã„
        print(f"Warning: Failed to record 'proposal_generation' activity log: {db_err}")

    # --- 2. AIã«å•ã„åˆã‚ã›ã¦ãƒ¡ãƒ¼ãƒ«æœ¬æ–‡ã‚’ç”Ÿæˆã™ã‚‹ ---
    prompt = f"""
        ã‚ãªãŸã¯ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«å„ªç§€ãªæŠ€è¡“è€…ã‚’ææ¡ˆã™ã‚‹ã€çµŒé¨“è±Šå¯ŒãªITå–¶æ¥­æ‹…å½“è€…ã§ã™ã€‚
        ä»¥ä¸‹ã®æ¡ˆä»¶æƒ…å ±ã¨æŠ€è¡“è€…æƒ…å ±ã‚’ã‚‚ã¨ã«ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å¿ƒã«éŸ¿ãã€ä¸å¯§ã§èª¬å¾—åŠ›ã®ã‚ã‚‹ææ¡ˆãƒ¡ãƒ¼ãƒ«ã®æ–‡é¢ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
        # å½¹å‰²
        - å„ªç§€ãªITå–¶æ¥­æ‹…å½“è€…
        # æŒ‡ç¤º
        - æœ€åˆã«ã€ææ¡ˆã™ã‚‹æŠ€è¡“è€…åã¨æ¡ˆä»¶åã‚’è¨˜è¼‰ã—ãŸä»¶åã‚’ä½œæˆã—ã¦ãã ã•ã„ (ä¾‹: ä»¶å: ã€ã€‡ã€‡æ§˜ã®ã”ææ¡ˆã€‘ã€‡ã€‡ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä»¶)ã€‚
        - æŠ€è¡“è€…ã®ã‚¹ã‚­ãƒ«ã‚„çµŒé¨“ãŒã€æ¡ˆä»¶ã®ã©ã®è¦ä»¶ã«å…·ä½“çš„ã«ãƒãƒƒãƒã—ã¦ã„ã‚‹ã‹ã‚’æ˜ç¢ºã«ç¤ºã—ã¦ãã ã•ã„ã€‚
        - ãƒã‚¸ãƒ†ã‚£ãƒ–ãªç‚¹ï¼ˆé©åˆã‚¹ã‚­ãƒ«ï¼‰ã‚’å¼·èª¿ã—ã€æŠ€è¡“è€…ã®é­…åŠ›ã‚’æœ€å¤§é™ã«ä¼ãˆã¦ãã ã•ã„ã€‚
        - æ‡¸å¿µç‚¹ï¼ˆã‚¹ã‚­ãƒ«ãƒŸã‚¹ãƒãƒƒãƒã‚„çµŒé¨“ä¸è¶³ï¼‰ãŒã‚ã‚‹å ´åˆã¯ã€æ­£ç›´ã«è§¦ã‚Œã¤ã¤ã‚‚ã€å­¦ç¿’æ„æ¬²ã‚„é¡ä¼¼çµŒé¨“ã€ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ãªã©ã§ã©ã®ã‚ˆã†ã«ã‚«ãƒãƒ¼ã§ãã‚‹ã‹ã‚’å‰å‘ãã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
        - å…¨ä½“ã¨ã—ã¦ã€ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã‹ã¤ä¸å¯§ãªãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒ¼ãƒ«ã®ãƒˆãƒ¼ãƒ³ã‚’ç¶­æŒã—ã¦ãã ã•ã„ã€‚
        - æœ€å¾Œã«ã€ãœã²ä¸€åº¦ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§ã®é¢è«‡ã®æ©Ÿä¼šã‚’è¨­ã‘ã¦ã„ãŸã ã‘ã¾ã™ã‚ˆã†ãŠé¡˜ã„ã™ã‚‹ä¸€æ–‡ã§ç· ã‚ããã£ã¦ãã ã•ã„ã€‚
        - å‡ºåŠ›ã¯ã€ä»¶åã¨æœ¬æ–‡ã‚’å«ã‚“ã ãƒ¡ãƒ¼ãƒ«å½¢å¼ã®ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã¨ã—ã¦ãã ã•ã„ã€‚ä½™è¨ˆãªè§£èª¬ã¯ä¸è¦ã§ã™ã€‚
        # æ¡ˆä»¶æƒ…å ±
        {job_summary}
        # æŠ€è¡“è€…æƒ…å ±
        {engineer_summary}
        # ææ¡ˆã™ã‚‹æŠ€è¡“è€…ã®åå‰
        {engineer_name}
        # æ¡ˆä»¶å
        {project_name}
        ---
        ãã‚Œã§ã¯ã€ä¸Šè¨˜ã®æŒ‡ç¤ºã«åŸºã¥ã„ã¦ã€æœ€é©ãªææ¡ˆãƒ¡ãƒ¼ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
    """
    try:
        # ãƒ¢ãƒ‡ãƒ«åã¯ã”è‡ªèº«ã®ç’°å¢ƒã«åˆã‚ã›ã¦èª¿æ•´ã—ã¦ãã ã•ã„
        model = genai.GenerativeModel('models/gemini-2.5-flash-lite') 
        response = model.generate_content(prompt)
        
        # å¿œç­”ãŒç©ºã§ãªã„ã“ã¨ã‚’ç¢ºèª
        if not response.text or not response.text.strip():
            return "AIãŒå¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãŠæ‰‹æ•°ã§ã™ãŒã€å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
            
        return response.text
        
    except Exception as e:
        print(f"Error generating proposal reply with LLM: {e}")
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«è¡¨ç¤ºã™ã‚‹ã€ã‚ˆã‚Šå…·ä½“çš„ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        return f"ææ¡ˆãƒ¡ãƒ¼ãƒ«ã®ç”Ÿæˆä¸­ã«AIã¨ã®é€šä¿¡ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
    





def get_evaluation_html(grade, font_size='2.5em'):
    if not grade: return ""
    color_map = {'S': '#00b894', 'A': '#28a745', 'B': '#17a2b8', 'C': '#ffc107', 'D': '#fd7e14', 'E': '#dc3545'}
    color = color_map.get(grade.upper(), '#6c757d') 
    style = f"color: {color}; font-size: {font_size}; font-weight: bold; text-align: center; line-height: 1; padding-top: 10px;"
    html_code = f"<div style='text-align: center; margin-bottom: 5px;'><span style='{style}'>{grade.upper()}</span></div><div style='text-align: center; font-size: 0.8em; color: #888;'>åˆ¤å®š</div>"
    return html_code

def get_matching_result_details(result_id):
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cursor:
                cursor.execute("SELECT * FROM matching_results WHERE id = %s", (result_id,))
                match_result = cursor.fetchone()
                if not match_result: return None
                cursor.execute("SELECT j.*, u.username as assignee_name FROM jobs j LEFT JOIN users u ON j.assigned_user_id = u.id WHERE j.id = %s", (match_result['job_id'],))
                job_data = cursor.fetchone()
                cursor.execute("SELECT e.*, u.username as assignee_name FROM engineers e LEFT JOIN users u ON e.assigned_user_id = u.id WHERE e.id = %s", (match_result['engineer_id'],))
                engineer_data = cursor.fetchone()
                return {"match_result": dict(match_result), "job_data": dict(job_data) if job_data else None, "engineer_data": dict(engineer_data) if engineer_data else None}
        except (Exception, psycopg2.Error) as e:
            print(f"ãƒãƒƒãƒãƒ³ã‚°è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}"); return None


def re_evaluate_existing_matches_for_engineer(engineer_id):
    """
    ã€ãƒ‘ã‚¿ãƒ¼ãƒ³Aã€‘
    æŒ‡å®šã•ã‚ŒãŸæŠ€è¡“è€…ã®æ—¢å­˜ã®ãƒãƒƒãƒãƒ³ã‚°çµæœã™ã¹ã¦ã«å¯¾ã—ã¦ã€AIè©•ä¾¡ã®ã¿ã‚’å†å®Ÿè¡Œã—ã€DBã‚’æ›´æ–°ã™ã‚‹ã€‚
    æ–°ã—ã„ãƒãƒƒãƒãƒ³ã‚°ã¯è¡Œã‚ãªã„ã€‚
    """
    if not engineer_id:
        st.error("æŠ€è¡“è€…IDãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return False

    conn = get_db_connection()
    try:
        with conn.cursor() as cursor:
            # 1. æŠ€è¡“è€…ã®æœ€æ–°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
            cursor.execute("SELECT document FROM engineers WHERE id = %s", (engineer_id,))
            engineer_record = cursor.fetchone()
            if not engineer_record:
                st.error(f"æŠ€è¡“è€…ID:{engineer_id} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                return False
            engineer_doc = engineer_record['document']

            # 2. ã“ã®æŠ€è¡“è€…ã«é–¢é€£ã™ã‚‹ã€è¡¨ç¤ºä¸­ã®ãƒãƒƒãƒãƒ³ã‚°çµæœã‚’å–å¾—
            cursor.execute(
                """
                SELECT r.id as match_id, j.id as job_id, j.document as job_document, j.project_name
                FROM matching_results r
                JOIN jobs j ON r.job_id = j.id
                WHERE r.engineer_id = %s AND r.is_hidden = 0 AND j.is_hidden = 0
                """,
                (engineer_id,)
            )
            existing_matches = cursor.fetchall()

            if not existing_matches:
                st.info("ã“ã®æŠ€è¡“è€…ã«ã¯å†è©•ä¾¡å¯¾è±¡ã®ãƒãƒƒãƒãƒ³ã‚°çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                return True # å‡¦ç†å¯¾è±¡ãŒãªã„ã®ã§æˆåŠŸã¨ã¿ãªã™

            st.write(f"{len(existing_matches)}ä»¶ã®æ—¢å­˜ãƒãƒƒãƒãƒ³ã‚°ã«å¯¾ã—ã¦å†è©•ä¾¡ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
            
            # 3. å„ãƒãƒƒãƒãƒ³ã‚°ã«å¯¾ã—ã¦AIè©•ä¾¡ã‚’å†å®Ÿè¡Œ
            success_count = 0
            for match in existing_matches:
                st.write(f"  - æ¡ˆä»¶ã€{match['project_name']}ã€ã¨ã®ãƒãƒƒãƒãƒ³ã‚°ã‚’å†è©•ä¾¡ä¸­...")
                
                # AIè©•ä¾¡ã‚’å‘¼ã³å‡ºã—
                llm_result = get_match_summary_with_llm(match['job_document'], engineer_doc)
                
                # DBã‚’æ›´æ–°
                if update_match_evaluation(match['match_id'], llm_result):
                    st.write(f"    -> æ–°ã—ã„è©•ä¾¡: **{llm_result.get('summary')}** ... âœ… æ›´æ–°å®Œäº†")
                    success_count += 1
                else:
                    st.write(f"    -> è©•ä¾¡ã¾ãŸã¯æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        
        # ã“ã®é–¢æ•°ã¯DBã®å¤‰æ›´ã‚’ä¼´ã‚ãªã„ã®ã§ã€conn.commit()ã¯ä¸è¦ (update_match_evaluationå†…ã§å®Œçµ)
        return success_count == len(existing_matches)

    except (Exception, psycopg2.Error) as e:
        st.error(f"å†è©•ä¾¡å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return False
    finally:
        if conn:
            conn.close()



def update_engineer_name(engineer_id, new_name):
    if not new_name or not new_name.strip(): return False
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cursor: cursor.execute("UPDATE engineers SET name = %s WHERE id = %s", (new_name.strip(), engineer_id))
            conn.commit(); return True
        except (Exception, psycopg2.Error) as e:
            print(f"æŠ€è¡“è€…æ°åã®æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}"); conn.rollback(); return False

def _build_meta_info_string(item_type, item_data):
    meta_fields = []
    if item_type == 'job':
        meta_fields = [["å›½ç±è¦ä»¶", "nationality_requirement"], ["é–‹å§‹æ™‚æœŸ", "start_date"], ["å‹¤å‹™åœ°", "location"], ["å˜ä¾¡", "unit_price"], ["å¿…é ˆã‚¹ã‚­ãƒ«", "required_skills"]]
    elif item_type == 'engineer':
        meta_fields = [["å›½ç±", "nationality"], ["ç¨¼åƒå¯èƒ½æ—¥", "availability_date"], ["å¸Œæœ›å‹¤å‹™åœ°", "desired_location"], ["å¸Œæœ›å˜ä¾¡", "desired_salary"], ["ä¸»è¦ã‚¹ã‚­ãƒ«", "main_skills"]]
    if not meta_fields: return "\n---\n"
    meta_parts = [f"[{display_name}: {item_data.get(key, 'ä¸æ˜')}]" for display_name, key in meta_fields]
    return " ".join(meta_parts) + "\n---\n"

def update_match_status(match_id, new_status):
    if not match_id or not new_status: return False

    # SQLæ–‡ã‚’ä¿®æ­£ã—ã€status_updated_at ã‚«ãƒ©ãƒ ã« NOW() ã‚’è¨­å®šã™ã‚‹
    sql = """
        UPDATE matching_results 
        SET 
            status = %s, 
            status_updated_at = NOW() 
        WHERE id = %s;
    """
    # â˜…â˜…â˜…ã€ä¿®æ­£ã“ã“ã¾ã§ã€‘â˜…â˜…â˜…

    
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cursor: 
                cursor.execute(sql, (new_status, match_id))
            #cursor.execute("UPDATE matching_results SET status = %s WHERE id = %s", (new_status, match_id))
            conn.commit(); return True
        except (Exception, psycopg2.Error) as e:
            print(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}"); conn.rollback(); return False


def delete_job(job_id):
    """
    æŒ‡å®šã•ã‚ŒãŸæ¡ˆä»¶IDã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ jobs ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å‰Šé™¤ã™ã‚‹ã€‚
    ON DELETE CASCADE åˆ¶ç´„ã«ã‚ˆã‚Šã€é–¢é€£ã™ã‚‹ matching_results ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚‚è‡ªå‹•çš„ã«å‰Šé™¤ã•ã‚Œã‚‹ã€‚
    
    Args:
        job_id (int): å‰Šé™¤å¯¾è±¡ã®æ¡ˆä»¶IDã€‚
        
    Returns:
        bool: å‰Šé™¤ãŒæˆåŠŸã—ãŸå ´åˆã¯Trueã€å¤±æ•—ã—ãŸå ´åˆã¯Falseã€‚
    """
    if not job_id:
        print("å‰Šé™¤å¯¾è±¡ã®æ¡ˆä»¶IDãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return False
        
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cursor:
                # æ¡ˆä»¶è‡ªä½“ã‚’å‰Šé™¤ã™ã‚‹ (ON DELETE CASCADE ã«ã‚ˆã‚Šé–¢é€£ãƒ‡ãƒ¼ã‚¿ã‚‚å‰Šé™¤ã•ã‚Œã‚‹)
                cursor.execute("DELETE FROM jobs WHERE id = %s", (job_id,))
                deleted_rows = cursor.rowcount
                print(f"Deleted {deleted_rows} job record with id {job_id}.")
            
            conn.commit()
            
            # æ¡ˆä»¶ãŒ1ä»¶ä»¥ä¸Šå‰Šé™¤ã•ã‚ŒãŸã‚‰æˆåŠŸã¨ã¿ãªã™
            return deleted_rows > 0
            
        except (Exception, psycopg2.Error) as e:
            print(f"æ¡ˆä»¶å‰Šé™¤ä¸­ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            conn.rollback() # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯å¤‰æ›´ã‚’å…ƒã«æˆ»ã™
            return False
        

# backend.py ã®æœ«å°¾ã‚ãŸã‚Šã«è¿½åŠ 

def delete_engineer(engineer_id):
    """
    æŒ‡å®šã•ã‚ŒãŸæŠ€è¡“è€…IDã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ engineers ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å‰Šé™¤ã™ã‚‹ã€‚
    ON DELETE CASCADE åˆ¶ç´„ã«ã‚ˆã‚Šã€é–¢é€£ã™ã‚‹ matching_results ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚‚è‡ªå‹•çš„ã«å‰Šé™¤ã•ã‚Œã‚‹ã€‚
    
    Args:
        engineer_id (int): å‰Šé™¤å¯¾è±¡ã®æŠ€è¡“è€…IDã€‚
        
    Returns:
        bool: å‰Šé™¤ãŒæˆåŠŸã—ãŸå ´åˆã¯Trueã€å¤±æ•—ã—ãŸå ´åˆã¯Falseã€‚
    """
    if not engineer_id:
        print("å‰Šé™¤å¯¾è±¡ã®æŠ€è¡“è€…IDãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return False
        
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cursor:
                # æŠ€è¡“è€…è‡ªä½“ã‚’å‰Šé™¤ã™ã‚‹ (ON DELETE CASCADE ã«ã‚ˆã‚Šé–¢é€£ãƒ‡ãƒ¼ã‚¿ã‚‚å‰Šé™¤ã•ã‚Œã‚‹)
                cursor.execute("DELETE FROM engineers WHERE id = %s", (engineer_id,))
                deleted_rows = cursor.rowcount
                print(f"Deleted {deleted_rows} engineer record with id {engineer_id}.")
            
            conn.commit()
            
            # æŠ€è¡“è€…ãŒ1ä»¶ä»¥ä¸Šå‰Šé™¤ã•ã‚ŒãŸã‚‰æˆåŠŸã¨ã¿ãªã™
            return deleted_rows > 0
            
        except (Exception, psycopg2.Error) as e:
            print(f"æŠ€è¡“è€…å‰Šé™¤ä¸­ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            conn.rollback() # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯å¤‰æ›´ã‚’å…ƒã«æˆ»ã™
            return False

def update_job_source_json(job_id, new_json_str):
    """
    æ¡ˆä»¶ã®source_data_jsonã‚’æ›´æ–°ã™ã‚‹ã€‚
    """
    if not job_id or not new_json_str:
        return False
        
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cur:
                cur.execute("UPDATE jobs SET source_data_json = %s WHERE id = %s", (new_json_str, job_id))
            conn.commit()
            return True
        except (Exception, psycopg2.Error) as e:
            print(f"æ¡ˆä»¶ã®JSONãƒ‡ãƒ¼ã‚¿æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            conn.rollback()
            return False
        

def update_match_evaluation(match_id, llm_result):
    """
    æŒ‡å®šã•ã‚ŒãŸãƒãƒƒãƒãƒ³ã‚°IDã®è©•ä¾¡çµæœã‚’æ›´æ–°ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã€‚
    """
    if not llm_result or 'summary' not in llm_result:
        return False
        
    grade = llm_result.get('summary')
    positive_points = json.dumps(llm_result.get('positive_points', []), ensure_ascii=False)
    concern_points = json.dumps(llm_result.get('concern_points', []), ensure_ascii=False)
    
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cursor:
                cursor.execute(
                    "UPDATE matching_results SET grade = %s, positive_points = %s, concern_points = %s WHERE id = %s",
                    (grade, positive_points, concern_points, match_id)
                )
            conn.commit()
            return True
        except (Exception, psycopg2.Error) as e:
            print(f"ãƒãƒƒãƒãƒ³ã‚°è©•ä¾¡ã®æ›´æ–°ã‚¨ãƒ©ãƒ¼ (ID: {match_id}): {e}")
            conn.rollback()
            return False




def re_evaluate_and_match_single_engineer(engineer_id, target_rank='B', target_count=5):
    """
    ã€æ–°ã—ã„ä»•æ§˜ã€‘
    æŒ‡å®šã•ã‚ŒãŸæŠ€è¡“è€…ã®æƒ…å ±ã‚’æœ€æ–°åŒ–ã—ã€æ—¢å­˜ã®ãƒãƒƒãƒãƒ³ã‚°ã‚’ã‚¯ãƒªã‚¢å¾Œã€
    æ¡ˆä»¶ã‚’æœ€æ–°é †ã«å‡¦ç†ã—ã€ç›®æ¨™ãƒ©ãƒ³ã‚¯ä»¥ä¸Šã®ãƒãƒƒãƒãƒ³ã‚°ãŒç›®æ¨™ä»¶æ•°ã«é”ã—ãŸã‚‰å‡¦ç†ã‚’çµ‚äº†ã™ã‚‹ã€‚
    """
    if not engineer_id:
        st.error("æŠ€è¡“è€…IDãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return False

    # ãƒ©ãƒ³ã‚¯ã®é †åºã‚’å®šç¾© (SãŒæœ€ã‚‚é«˜ã„)
    rank_order = ['S', 'A', 'B', 'C', 'D']
    try:
        # ç›®æ¨™ãƒ©ãƒ³ã‚¯ä»¥ä¸Šã®ãƒ©ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        valid_ranks = rank_order[:rank_order.index(target_rank) + 1]
    except ValueError:
        st.error(f"ç„¡åŠ¹ãªç›®æ¨™ãƒ©ãƒ³ã‚¯ãŒæŒ‡å®šã•ã‚Œã¾ã—ãŸ: {target_rank}")
        return False

    with get_db_connection() as conn:
        try:
            with conn.cursor() as cursor:
                # 1. æŠ€è¡“è€…ã®æœ€æ–°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆ

                st.write("ğŸ“„ å…ƒæƒ…å ±ã‹ã‚‰æŠ€è¡“è€…ã®æœ€æ–°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆã—ã¾ã™...")
                cursor.execute("SELECT source_data_json, name FROM engineers WHERE id = %s", (engineer_id,))
                engineer_record = cursor.fetchone()
                if not engineer_record or not engineer_record['source_data_json']:
                    st.error(f"æŠ€è¡“è€…ID:{engineer_id} ã®å…ƒæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                    return False
                
                source_data = json.loads(engineer_record['source_data_json'])
                full_text_for_llm = source_data.get('body', '') + "".join([f"\n\n--- æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«: {att['filename']} ---\n{att.get('content', '')}" for att in source_data.get('attachments', []) if att.get('content') and not att.get('content', '').startswith("[") and not att.get('content', '').endswith("]")])
                
                parsed_data = split_text_with_llm(full_text_for_llm)
                if not parsed_data or not parsed_data.get("engineers"):
                    st.error("LLMã«ã‚ˆã‚‹æƒ…å ±æŠ½å‡ºï¼ˆå†è©•ä¾¡ï¼‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                    return False
                
                item_data = parsed_data["engineers"][0]
                doc = item_data.get("document") or full_text_for_llm
                meta_info = _build_meta_info_string('engineer', item_data)
                new_full_document = meta_info + doc
                engineer_doc = new_full_document

                # â–¼â–¼â–¼ã€æŠ€è¡“è€…å¸Œæœ›å˜ä¾¡ã‚’æŠ½å‡ºã€‘â–¼â–¼â–¼
                engineer_price_str = item_data.get("desired_salary")
                engineer_price = _extract_price_from_string(engineer_price_str)
                if engineer_price:
                    st.write(f"  - æŠ€è¡“è€…ã®å¸Œæœ›å˜ä¾¡ã‚’ **{engineer_price}ä¸‡å††** ã¨ã—ã¦èªè­˜ã—ã¾ã—ãŸã€‚")
                else:
                    st.warning("  - æŠ€è¡“è€…ã®å¸Œæœ›å˜ä¾¡ãŒæŠ½å‡ºã§ããªã‹ã£ãŸãŸã‚ã€å˜ä¾¡ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")
                # â–²â–²â–²ã€æŠ€è¡“è€…å¸Œæœ›å˜ä¾¡ã®æŠ½å‡ºã“ã“ã¾ã§ã€‘â–²â–²â–²

                
                # 2. engineersãƒ†ãƒ¼ãƒ–ãƒ«ã®documentã‚’æ›´æ–°
                cursor.execute("UPDATE engineers SET document = %s WHERE id = %s", (engineer_doc, engineer_id))
                st.write("âœ… æŠ€è¡“è€…ã®AIè¦ç´„æƒ…å ±ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚")

                # 3. æ—¢å­˜ã®ãƒãƒƒãƒãƒ³ã‚°çµæœã‚’å‰Šé™¤
                cursor.execute("DELETE FROM matching_results WHERE engineer_id = %s", (engineer_id,))
                st.write(f"ğŸ—‘ï¸ æŠ€è¡“è€…ID:{engineer_id} ã®æ—¢å­˜ãƒãƒƒãƒãƒ³ã‚°çµæœã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")

                # 4. ãƒãƒƒãƒãƒ³ã‚°å¯¾è±¡ã®å…¨æ¡ˆä»¶ã‚’æœ€æ–°é †ã«å–å¾—
                st.write("ğŸ”„ æœ€æ–°ã®æ¡ˆä»¶ã‹ã‚‰é †ã«ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
                #cursor.execute("SELECT id, document, project_name FROM jobs WHERE is_hidden = 0 ORDER BY created_at DESC")
                cursor.execute("SELECT id, document, project_name, source_data_json FROM jobs WHERE is_hidden = 0 ORDER BY created_at DESC")

                all_active_jobs = cursor.fetchall()
                if not all_active_jobs:
                    st.warning("ãƒãƒƒãƒãƒ³ã‚°å¯¾è±¡ã®æ¡ˆä»¶ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                    conn.commit()
                    return True

                st.write(f"  - å¯¾è±¡æ¡ˆä»¶æ•°: {len(all_active_jobs)}ä»¶")
                st.write(f"  - çµ‚äº†æ¡ä»¶: ã€Œ**{target_rank}**ã€ãƒ©ãƒ³ã‚¯ä»¥ä¸Šã®ãƒãƒƒãƒãƒ³ã‚°ãŒ **{target_count}** ä»¶è¦‹ã¤ã‹ã£ãŸæ™‚ç‚¹")

                # 5. ãƒ«ãƒ¼ãƒ—ã§ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ã‚’å®Ÿè¡Œ
                found_count = 0
                processed_count = 0
                now_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

                for job in all_active_jobs:
                    processed_count += 1

                    # â–¼â–¼â–¼ã€å˜ä¾¡ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯ã€‘â–¼â–¼â–¼
                    try:
                        job_source_data = json.loads(job['source_data_json'])
                        job_price_str = job_source_data.get("unit_price")
                        job_price = _extract_price_from_string(job_price_str)
                    except (json.JSONDecodeError, TypeError):
                        job_price = None

                    if job_price is not None and engineer_price is not None:
                        if engineer_price > job_price + 5:
                            st.write(f"  ({processed_count}/{len(all_active_jobs)}) æ¡ˆä»¶ã€{job['project_name']}ã€ -> å˜ä¾¡ä¸ä¸€è‡´ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ— (æ¡ˆä»¶:{job_price}ä¸‡, æŠ€è¡“è€…:{engineer_price}ä¸‡)")
                            continue
                    # â–²â–²â–²ã€å˜ä¾¡ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã“ã“ã¾ã§ã€‘â–²â–²â–²

                    st.write(f"  ({processed_count}/{len(all_active_jobs)}) æ¡ˆä»¶ã€{job['project_name']}ã€ã¨ãƒãƒƒãƒãƒ³ã‚°ä¸­...")
                    
                    # LLMã«ã‚ˆã‚‹ãƒãƒƒãƒãƒ³ã‚°è©•ä¾¡ã‚’å®Ÿè¡Œ
                    llm_result = get_match_summary_with_llm(job['document'], engineer_doc)

                    if llm_result and 'summary' in llm_result:
                        grade = llm_result.get('summary')
                        positive_points = json.dumps(llm_result.get('positive_points', []), ensure_ascii=False)
                        concern_points = json.dumps(llm_result.get('concern_points', []), ensure_ascii=False)
                        
                        # é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã¯ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’è¡Œã‚ãªã„ãŸã‚ã€ãƒ€ãƒŸãƒ¼å€¤ï¼ˆä¾‹: 0ï¼‰ã‚’å…¥ã‚Œã‚‹ã‹ã€NULLè¨±å®¹ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
                        # ã“ã“ã§ã¯ score ã‚’ 0 ã¨ã—ã¾ã™ã€‚
                        score = 0.0

                        # DBã«ä¿å­˜ï¼ˆãƒ©ãƒ³ã‚¯ã«é–¢ã‚ã‚‰ãšä¸€æ—¦ã™ã¹ã¦ä¿å­˜ã™ã‚‹æ–¹ãŒå¾Œã€…ã®åˆ†æã«å½¹ç«‹ã¤å ´åˆã‚‚ã‚ã‚‹ãŒã€ä»Šå›ã¯ãƒ’ãƒƒãƒˆã—ãŸã‚‚ã®ã ã‘ä¿å­˜ï¼‰
                        if grade in valid_ranks:
                            try:
                                cursor.execute(
                                    'INSERT INTO matching_results (job_id, engineer_id, score, created_at, grade, positive_points, concern_points) VALUES (%s, %s, %s, %s, %s, %s, %s)',
                                    (job['id'], engineer_id, score, now_str, grade, positive_points, concern_points)
                                )
                                st.success(f"    -> ãƒãƒƒãƒãƒ³ã‚°è©•ä¾¡: **{grade}** ... âœ… ãƒ’ãƒƒãƒˆï¼DBã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
                                found_count += 1
                            except Exception as e:
                                st.error(f"    -> DBä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                        else:
                            st.write(f"    -> ãƒãƒƒãƒãƒ³ã‚°è©•ä¾¡: **{grade}** ... ã‚¹ã‚­ãƒƒãƒ—")
                    else:
                        st.warning(f"    -> LLMè©•ä¾¡å¤±æ•—ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")

                    # çµ‚äº†æ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯
                    if found_count >= target_count:
                        st.success(f"ğŸ‰ ç›®æ¨™ã® {target_count} ä»¶ã«åˆ°é”ã—ãŸãŸã‚ã€å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                        break
                
                if found_count < target_count:
                    st.info(f"ã™ã¹ã¦ã®æ¡ˆä»¶ã¨ã®ãƒãƒƒãƒãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸã€‚(ãƒ’ãƒƒãƒˆæ•°: {found_count}ä»¶)")

            conn.commit()
            return True
        except (Exception, psycopg2.Error) as e:
            conn.rollback()
            st.error(f"å†è©•ä¾¡ãƒ»å†ãƒãƒƒãƒãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            st.exception(e) # è©³ç´°ãªãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚’è¡¨ç¤º
            return False
        




def save_proposal_text(match_id, text):
    """
    æŒ‡å®šã•ã‚ŒãŸãƒãƒƒãƒãƒ³ã‚°IDã«å¯¾ã—ã¦ã€ç”Ÿæˆã•ã‚ŒãŸææ¡ˆãƒ¡ãƒ¼ãƒ«ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜ã—ã¾ã™ã€‚
    """
    if not match_id or text is None:
        return False
        
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cursor:
                cursor.execute(
                    "UPDATE matching_results SET proposal_text = %s WHERE id = %s",
                    (text, match_id)
                )
            conn.commit()
            return True
        except (Exception, psycopg2.Error) as e:
            print(f"Error saving proposal text for match_id {match_id}: {e}")
            conn.rollback()
            return False

# backend.py ã® get_dashboard_data é–¢æ•°ã‚’ã“ã¡ã‚‰ã«ç½®ãæ›ãˆã¦ãã ã•ã„

def get_dashboard_data():
    """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¡¨ç¤ºã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’DBã‹ã‚‰å–å¾—ãƒ»é›†è¨ˆã™ã‚‹"""
    conn = get_db_connection()
    try:
        #@st.cache_data(ttl=300) # 5åˆ†é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        def fetch_data_from_db():
            # ã€å¤‰æ›´ç‚¹1ã€‘ users ãƒ†ãƒ¼ãƒ–ãƒ«ã‚‚èª­ã¿è¾¼ã‚€
            # ã€å¤‰æ›´ç‚¹2ã€‘ SQLã§JOINã—ã¦æ‹…å½“è€…å(username)ã‚’å–å¾—ã™ã‚‹
            jobs_sql = """
                SELECT j.id, j.created_at, u.username as assignee_name
                FROM jobs j
                LEFT JOIN users u ON j.assigned_user_id = u.id
            """
            engineers_sql = """
                SELECT e.id, e.created_at, u.username as assignee_name
                FROM engineers e
                LEFT JOIN users u ON e.assigned_user_id = u.id
            """
            matches_sql = """
                SELECT m.id, m.created_at, m.grade, u_job.username as job_assignee, u_eng.username as engineer_assignee
                FROM matching_results m
                LEFT JOIN jobs j ON m.job_id = j.id
                LEFT JOIN engineers e ON m.engineer_id = e.id
                LEFT JOIN users u_job ON j.assigned_user_id = u_job.id
                LEFT JOIN users u_eng ON e.assigned_user_id = u_eng.id
            """
            jobs_df = pd.read_sql(jobs_sql, conn)
            engineers_df = pd.read_sql(engineers_sql, conn)
            matches_df = pd.read_sql(matches_sql, conn)
            
            return jobs_df, engineers_df, matches_df

        jobs_df, engineers_df, matches_df = fetch_data_from_db()

        # --- 1. ã‚µãƒãƒªãƒ¼æŒ‡æ¨™ (å¤‰æ›´ãªã—) ---
        total_jobs = len(jobs_df)
        total_engineers = len(engineers_df)
        total_matches = len(matches_df)

        jobs_df['created_at'] = pd.to_datetime(jobs_df['created_at'], errors='coerce')
        engineers_df['created_at'] = pd.to_datetime(engineers_df['created_at'], errors='coerce')
        
        now = pd.Timestamp.now()
        jobs_this_month = len(jobs_df.dropna(subset=['created_at'])[jobs_df['created_at'].dt.month == now.month])
        engineers_this_month = len(engineers_df.dropna(subset=['created_at'])[engineers_df['created_at'].dt.month == now.month])

        summary_metrics = {
            "total_jobs": total_jobs,
            "total_engineers": total_engineers,
            "total_matches": total_matches,
            "jobs_this_month": jobs_this_month,
            "engineers_this_month": engineers_this_month,
        }

        # --- 2. AIè©•ä¾¡ãƒ©ãƒ³ã‚¯ã®å‰²åˆ (å¤‰æ›´ãªã—) ---
        if not matches_df.empty:
            rank_counts = matches_df['grade'].value_counts().reindex(['S', 'A', 'B', 'C', 'D'], fill_value=0)
        else:
            rank_counts = pd.Series([0, 0, 0, 0, 0], index=['S', 'A', 'B', 'C', 'D'])

        # --- 3. æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ (å¤‰æ›´ãªã—) ---
        matches_df['created_at'] = pd.to_datetime(matches_df['created_at'], errors='coerce')
        jobs_ts = jobs_df.dropna(subset=['created_at']).set_index('created_at')
        engineers_ts = engineers_df.dropna(subset=['created_at']).set_index('created_at')
        matches_ts = matches_df.dropna(subset=['created_at']).set_index('created_at')
        daily_jobs = jobs_ts.resample('D').size().rename('æ¡ˆä»¶ç™»éŒ²æ•°')
        daily_engineers = engineers_ts.resample('D').size().rename('æŠ€è¡“è€…ç™»éŒ²æ•°')
        daily_matches = matches_ts.resample('D').size().rename('ãƒãƒƒãƒãƒ³ã‚°ç”Ÿæˆæ•°')
        time_series_df = pd.concat([daily_jobs, daily_engineers, daily_matches], axis=1).fillna(0).astype(int)
        
        # --- 4. æ‹…å½“è€…åˆ¥åˆ†æãƒ‡ãƒ¼ã‚¿ (â˜…ã“ã“ã‹ã‚‰ãŒæ–°ã—ã„ã‚³ãƒ¼ãƒ‰) ---
        # æ‹…å½“è€…ãŒæœªå‰²ã‚Šå½“ã¦(None)ã®å ´åˆã‚’ã€Œæœªæ‹…å½“ã€ã«ç½®ãæ›ãˆã‚‹
        jobs_df['assignee_name'].fillna('æœªæ‹…å½“', inplace=True)
        engineers_df['assignee_name'].fillna('æœªæ‹…å½“', inplace=True)
        
        # æ‹…å½“è€…ã”ã¨ã®æ‹…å½“ä»¶æ•°ã‚’é›†è¨ˆ
        job_counts_by_assignee = jobs_df['assignee_name'].value_counts().rename('æ¡ˆä»¶æ‹…å½“æ•°')
        engineer_counts_by_assignee = engineers_df['assignee_name'].value_counts().rename('æŠ€è¡“è€…æ‹…å½“æ•°')
        assignee_counts_df = pd.concat([job_counts_by_assignee, engineer_counts_by_assignee], axis=1).fillna(0).astype(int)
        
        # æ‹…å½“è€…ã”ã¨ã®ãƒãƒƒãƒãƒ³ã‚°ãƒ©ãƒ³ã‚¯åˆ†å¸ƒã‚’é›†è¨ˆ
        # æ¡ˆä»¶æ‹…å½“è€…ã¨æŠ€è¡“è€…æ‹…å½“è€…ã®ã©ã¡ã‚‰ã‹ãŒè¨­å®šã•ã‚Œã¦ã„ã‚Œã°ã€ãã®æ‹…å½“è€…ã®æˆæœã¨ã¿ãªã™ï¼ˆcoalesceçš„ãªå‡¦ç†ï¼‰
        matches_df['responsible_person'] = matches_df['job_assignee'].fillna(matches_df['engineer_assignee']).fillna('æœªæ‹…å½“')
        match_rank_by_assignee = pd.crosstab(
            index=matches_df['responsible_person'],
            columns=matches_df['grade']
        )
        # S,A,B,C,Dã‚«ãƒ©ãƒ ãŒå¿…ãšå­˜åœ¨ã™ã‚‹ã‚ˆã†ã«reindex
        all_ranks = ['S', 'A', 'B', 'C', 'D']
        match_rank_by_assignee = match_rank_by_assignee.reindex(columns=all_ranks, fill_value=0)

        # æˆ»ã‚Šå€¤ã«æ‹…å½“è€…åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        return summary_metrics, rank_counts, time_series_df, assignee_counts_df, match_rank_by_assignee

    finally:
        if conn:
            conn.close()





def save_match_feedback(match_id, feedback_status, feedback_comment, user_id):
    """ãƒãƒƒãƒãƒ³ã‚°çµæœã«å¯¾ã™ã‚‹æ‹…å½“è€…ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ä¿å­˜ã™ã‚‹"""
    if not all([match_id, feedback_status, user_id]):
        st.error("å¿…é ˆé …ç›®ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ä¿å­˜ã§ãã¾ã›ã‚“ã€‚")
        return False
    
    feedback_at = datetime.now()
    
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cursor:
                cursor.execute(
                    """
                    UPDATE matching_results 
                    SET feedback_status = %s, 
                        feedback_comment = %s, 
                        feedback_user_id = %s, 
                        feedback_at = %s
                    WHERE id = %s
                    """,
                    (feedback_status, feedback_comment, user_id, feedback_at, match_id)
                )
            conn.commit()
            return True
        except (Exception, psycopg2.Error) as e:
            st.error(f"ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            conn.rollback()
            return False
        

def get_matching_result_details(result_id):
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cursor:
                # ã€ä¿®æ­£ç‚¹ã€‘ usersãƒ†ãƒ¼ãƒ–ãƒ«ã‚’JOINã—ã¦ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ‹…å½“è€…åã‚’å–å¾—
                sql = """
                    SELECT r.*, u.username as feedback_username
                    FROM matching_results r
                    LEFT JOIN users u ON r.feedback_user_id = u.id
                    WHERE r.id = %s
                """
                cursor.execute(sql, (result_id,))
                match_result = cursor.fetchone()
                
                if not match_result: return None
                
                cursor.execute("SELECT j.*, u.username as assignee_name FROM jobs j LEFT JOIN users u ON j.assigned_user_id = u.id WHERE j.id = %s", (match_result['job_id'],))
                job_data = cursor.fetchone()
                
                cursor.execute("SELECT e.*, u.username as assignee_name FROM engineers e LEFT JOIN users u ON e.assigned_user_id = u.id WHERE e.id = %s", (match_result['engineer_id'],))
                engineer_data = cursor.fetchone()
                
                return {"match_result": dict(match_result), "job_data": dict(job_data) if job_data else None, "engineer_data": dict(engineer_data) if engineer_data else None}
        except (Exception, psycopg2.Error) as e:
            print(f"ãƒãƒƒãƒãƒ³ã‚°è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}"); return None
            


def save_internal_memo(match_id, memo_text):
    """ãƒãƒƒãƒãƒ³ã‚°çµæœã«å¯¾ã™ã‚‹ç¤¾å†…ãƒ¡ãƒ¢ã‚’ä¿å­˜ãƒ»æ›´æ–°ã™ã‚‹"""
    if not match_id:
        st.error("ãƒãƒƒãƒãƒ³ã‚°IDãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return False
    
    # ãƒ¡ãƒ¢ãŒç©ºæ–‡å­—åˆ—ã®å ´åˆã‚‚è¨±å®¹ã™ã‚‹ãŸã‚ã€memo_textã®ãƒã‚§ãƒƒã‚¯ã¯ç·©ã‚ã«ã™ã‚‹
    if memo_text is None:
        memo_text = "" # DBã«ã¯ç©ºæ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜

    with get_db_connection() as conn:
        try:
            with conn.cursor() as cursor:
                cursor.execute(
                    """
                    UPDATE matching_results 
                    SET internal_memo = %s
                    WHERE id = %s
                    """,
                    (memo_text, match_id)
                )
            conn.commit()
            return True
        except (Exception, psycopg2.Error) as e:
            st.error(f"ç¤¾å†…ãƒ¡ãƒ¢ã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            conn.rollback()
            return False


def delete_match(match_id):
    """æŒ‡å®šã•ã‚ŒãŸãƒãƒƒãƒãƒ³ã‚°IDã®çµæœã‚’ matching_results ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å‰Šé™¤ã™ã‚‹"""
    if not match_id:
        print("å‰Šé™¤å¯¾è±¡ã®ãƒãƒƒãƒãƒ³ã‚°IDãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return False
    
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cursor:
                cursor.execute("DELETE FROM matching_results WHERE id = %s", (match_id,))
                deleted_rows = cursor.rowcount
            
            conn.commit()
            
            # 1ä»¶ä»¥ä¸Šå‰Šé™¤ã•ã‚ŒãŸã‚‰æˆåŠŸã¨ã¿ãªã™
            return deleted_rows > 0
            
        except (Exception, psycopg2.Error) as e:
            st.error(f"ãƒãƒƒãƒãƒ³ã‚°çµæœã®å‰Šé™¤ä¸­ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            conn.rollback()
            return False




def re_evaluate_and_match_single_job(job_id, target_rank='B', target_count=5):
    """
    ã€æ–°ã—ã„é–¢æ•°ã€‘
    æŒ‡å®šã•ã‚ŒãŸæ¡ˆä»¶ã®æƒ…å ±ã‚’æœ€æ–°åŒ–ã—ã€æ—¢å­˜ã®ãƒãƒƒãƒãƒ³ã‚°ã‚’ã‚¯ãƒªã‚¢å¾Œã€
    æŠ€è¡“è€…ã‚’æœ€æ–°é †ã«å‡¦ç†ã—ã€ç›®æ¨™ãƒ©ãƒ³ã‚¯ä»¥ä¸Šã®ãƒãƒƒãƒãƒ³ã‚°ãŒç›®æ¨™ä»¶æ•°ã«é”ã—ãŸã‚‰å‡¦ç†ã‚’çµ‚äº†ã™ã‚‹ã€‚
    """
    if not job_id:
        st.error("æ¡ˆä»¶IDãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return False

    rank_order = ['S', 'A', 'B', 'C', 'D']
    try:
        valid_ranks = rank_order[:rank_order.index(target_rank) + 1]
    except ValueError:
        st.error(f"ç„¡åŠ¹ãªç›®æ¨™ãƒ©ãƒ³ã‚¯ãŒæŒ‡å®šã•ã‚Œã¾ã—ãŸ: {target_rank}")
        return False

    with get_db_connection() as conn:
        try:
            with conn.cursor() as cursor:
                # 1. æ¡ˆä»¶ã®æœ€æ–°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆ
                st.write("ğŸ“„ å…ƒæƒ…å ±ã‹ã‚‰æ¡ˆä»¶ã®æœ€æ–°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆã—ã¾ã™...")
                cursor.execute("SELECT source_data_json, project_name FROM jobs WHERE id = %s", (job_id,))
                job_record = cursor.fetchone()
                if not job_record or not job_record['source_data_json']:
                    st.error(f"æ¡ˆä»¶ID:{job_id} ã®å…ƒæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                    return False
                
                source_data = json.loads(job_record['source_data_json'])
                full_text_for_llm = source_data.get('body', '') + "".join([f"\n\n--- æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«: {att['filename']} ---\n{att.get('content', '')}" for att in source_data.get('attachments', []) if att.get('content') and not att.get('content', '').startswith("[") and not att.get('content', '').endswith("]")])
                
                parsed_data = split_text_with_llm(full_text_for_llm)
                if not parsed_data or not parsed_data.get("jobs"):
                    st.error("LLMã«ã‚ˆã‚‹æƒ…å ±æŠ½å‡ºï¼ˆå†è©•ä¾¡ï¼‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                    return False
                
                item_data = parsed_data["jobs"][0]
                doc = item_data.get("document") or full_text_for_llm
                meta_info = _build_meta_info_string('job', item_data)
                new_full_document = meta_info + doc
                job_doc = new_full_document

                # â–¼â–¼â–¼ã€æ¡ˆä»¶å˜ä¾¡ã‚’æŠ½å‡ºã€‘â–¼â–¼â–¼
                job_price_str = item_data.get("unit_price")
                job_price = _extract_price_from_string(job_price_str)
                if job_price:
                    st.write(f"  - æ¡ˆä»¶å˜ä¾¡ã‚’ **{job_price}ä¸‡å††** ã¨ã—ã¦èªè­˜ã—ã¾ã—ãŸã€‚")
                else:
                    st.warning("  - æ¡ˆä»¶ã®å˜ä¾¡ãŒæŠ½å‡ºã§ããªã‹ã£ãŸãŸã‚ã€å˜ä¾¡ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚")
                # â–²â–²â–²ã€æ¡ˆä»¶å˜ä¾¡ã®æŠ½å‡ºã“ã“ã¾ã§ã€‘â–²â–²â–²
                
                # 2. jobsãƒ†ãƒ¼ãƒ–ãƒ«ã®documentã‚’æ›´æ–°
                cursor.execute("UPDATE jobs SET document = %s WHERE id = %s", (job_doc, job_id))
                st.write("âœ… æ¡ˆä»¶ã®AIè¦ç´„æƒ…å ±ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚")

                # 3. æ—¢å­˜ã®ãƒãƒƒãƒãƒ³ã‚°çµæœã‚’å‰Šé™¤
                cursor.execute("DELETE FROM matching_results WHERE job_id = %s", (job_id,))
                st.write(f"ğŸ—‘ï¸ æ¡ˆä»¶ID:{job_id} ã®æ—¢å­˜ãƒãƒƒãƒãƒ³ã‚°çµæœã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")

                # 4. ãƒãƒƒãƒãƒ³ã‚°å¯¾è±¡ã®å…¨æŠ€è¡“è€…ã‚’æœ€æ–°é †ã«å–å¾—
                st.write("ğŸ”„ æœ€æ–°ã®æŠ€è¡“è€…ã‹ã‚‰é †ã«ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
                cursor.execute("SELECT id, document, name, source_data_json FROM engineers WHERE is_hidden = 0 ORDER BY created_at DESC")
                all_active_engineers = cursor.fetchall()
                if not all_active_engineers:
                    st.warning("ãƒãƒƒãƒãƒ³ã‚°å¯¾è±¡ã®æŠ€è¡“è€…ãŒã„ã¾ã›ã‚“ã€‚")
                    conn.commit()
                    return True

                st.write(f"  - å¯¾è±¡æŠ€è¡“è€…æ•°: {len(all_active_engineers)}å")
                st.write(f"  - çµ‚äº†æ¡ä»¶: ã€Œ**{target_rank}**ã€ãƒ©ãƒ³ã‚¯ä»¥ä¸Šã®ãƒãƒƒãƒãƒ³ã‚°ãŒ **{target_count}** ä»¶è¦‹ã¤ã‹ã£ãŸæ™‚ç‚¹")

                # 5. ãƒ«ãƒ¼ãƒ—ã§ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ã‚’å®Ÿè¡Œ
                found_count = 0
                processed_count = 0
                now_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

                for engineer in all_active_engineers:
                    processed_count += 1

                    # â–¼â–¼â–¼ã€å˜ä¾¡ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯ã€‘â–¼â–¼â–¼
                    try:
                        engineer_source_data = json.loads(engineer['source_data_json'])
                        engineer_price_str = engineer_source_data.get("desired_salary")
                        engineer_price = _extract_price_from_string(engineer_price_str)
                    except (json.JSONDecodeError, TypeError):
                        engineer_price = None

                    # æ¡ˆä»¶å˜ä¾¡ã¨æŠ€è¡“è€…å¸Œæœ›å˜ä¾¡ã®ä¸¡æ–¹ãŒå–å¾—ã§ããŸå ´åˆã®ã¿æ¯”è¼ƒ
                    if job_price is not None and engineer_price is not None:
                        # æŠ€è¡“è€…ã®å¸Œæœ›å˜ä¾¡ãŒæ¡ˆä»¶å˜ä¾¡ã‚’5ä¸‡å††ä»¥ä¸Šä¸Šå›ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                        if engineer_price > job_price + 5:
                            st.write(f"  ({processed_count}/{len(all_active_engineers)}) æŠ€è¡“è€…ã€{engineer['name']}ã€ -> å˜ä¾¡ä¸ä¸€è‡´ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ— (æ¡ˆä»¶:{job_price}ä¸‡, æŠ€è¡“è€…:{engineer_price}ä¸‡)")
                            continue # æ¬¡ã®æŠ€è¡“è€…ã¸
                    # â–²â–²â–²ã€å˜ä¾¡ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã“ã“ã¾ã§ã€‘â–²â–²â–²

                    st.write(f"  ({processed_count}/{len(all_active_engineers)}) æŠ€è¡“è€…ã€{engineer['name']}ã€ã¨ãƒãƒƒãƒãƒ³ã‚°ä¸­...")
                    
                    llm_result = get_match_summary_with_llm(job_doc, engineer['document'])

                    if llm_result and 'summary' in llm_result:
                        grade = llm_result.get('summary')
                        positive_points = json.dumps(llm_result.get('positive_points', []), ensure_ascii=False)
                        concern_points = json.dumps(llm_result.get('concern_points', []), ensure_ascii=False)
                        score = 0.0

                        if grade in valid_ranks:
                            try:
                                cursor.execute(
                                    'INSERT INTO matching_results (job_id, engineer_id, score, created_at, grade, positive_points, concern_points) VALUES (%s, %s, %s, %s, %s, %s, %s)',
                                    (job_id, engineer['id'], score, now_str, grade, positive_points, concern_points)
                                )
                                st.success(f"    -> ãƒãƒƒãƒãƒ³ã‚°è©•ä¾¡: **{grade}** ... âœ… ãƒ’ãƒƒãƒˆï¼DBã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
                                found_count += 1
                            except Exception as e:
                                st.error(f"    -> DBä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                        else:
                            st.write(f"    -> ãƒãƒƒãƒãƒ³ã‚°è©•ä¾¡: **{grade}** ... ã‚¹ã‚­ãƒƒãƒ—")
                    else:
                        st.warning(f"    -> LLMè©•ä¾¡å¤±æ•—ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")

                    if found_count >= target_count:
                        st.success(f"ğŸ‰ ç›®æ¨™ã® {target_count} ä»¶ã«åˆ°é”ã—ãŸãŸã‚ã€å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                        break
                
                if found_count < target_count:
                    st.info(f"ã™ã¹ã¦ã®æŠ€è¡“è€…ã¨ã®ãƒãƒƒãƒãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸã€‚(ãƒ’ãƒƒãƒˆæ•°: {found_count}ä»¶)")

            conn.commit()
            return True
        except (Exception, psycopg2.Error) as e:
            conn.rollback()
            st.error(f"å†è©•ä¾¡ãƒ»å†ãƒãƒƒãƒãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            st.exception(e)
            return False
        
        

def _extract_price_from_string(price_str: str) -> float | None:
    """
    "80ä¸‡å††", "75ä¸‡ï½85ä¸‡", "ã€œ90" ã®ã‚ˆã†ãªæ–‡å­—åˆ—ã‹ã‚‰æ•°å€¤ï¼ˆä¸‡å††å˜ä½ï¼‰ã‚’æŠ½å‡ºã™ã‚‹ã€‚
    ç¯„å›²ã®å ´åˆã¯ä¸‹é™å€¤ã‚’è¿”ã™ã€‚æŠ½å‡ºã§ããªã„å ´åˆã¯ None ã‚’è¿”ã™ã€‚
    """
    if not price_str or not isinstance(price_str, str):
        return None
    
    # å…¨è§’æ•°å­—ã‚’åŠè§’ã«ã€å…¨è§’ãƒã‚¤ãƒŠã‚¹ã‚’åŠè§’ã«å¤‰æ›
    price_str = price_str.translate(str.maketrans("ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼", "0123456789-"))
    
    # "ä¸‡"ã‚„"å††"ãªã©ã®æ–‡å­—ã‚’å‰Šé™¤
    price_str = price_str.replace("ä¸‡", "").replace("å††", "").replace(",", "")
    
    # æ•°å­—ï¼ˆå°æ•°ç‚¹å«ã‚€ï¼‰ã‚’ã™ã¹ã¦æŠ½å‡º
    numbers = re.findall(r'(\d+\.?\d*)', price_str)
    
    if numbers:
        # æŠ½å‡ºã•ã‚ŒãŸæ•°å­—ã®ä¸­ã‹ã‚‰æœ€å°ã®ã‚‚ã®ã‚’è¿”ã™ï¼ˆä¾‹: "75~85" -> 75ï¼‰
        try:
            return min([float(n) for n in numbers])
        except (ValueError, TypeError):
            return None
    return None


# backend.py ã® get_filtered_item_ids é–¢æ•°ã‚’ã“ã¡ã‚‰ã«ç½®ãæ›ãˆã¦ãã ã•ã„

def get_filtered_item_ids(
    item_type: str, 
    keyword: str = "", 
    assigned_user_ids: list = None,
    has_matches_only: bool = False,
    auto_match_only: bool = False,
    sort_column: str = "ç™»éŒ²æ—¥", 
    sort_order: str = "é™é †", 
    show_hidden: bool = False
) -> list:
    """
    ã€auto_match_only ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ä¿®æ­£ç‰ˆã€‘
    item_typeã®å˜æ•°å½¢ãƒ»è¤‡æ•°å½¢ã®ä¸æ•´åˆã‚’è§£æ¶ˆã€‚
    """
    if item_type not in ['jobs', 'engineers']: 
        return []

    table_name = 'jobs' if item_type == 'jobs' else 'engineers'
    name_column = "project_name" if table_name == "jobs" else "name"
    
    query = f"SELECT DISTINCT e.id FROM {table_name} e"
    joins = ""
    where_clauses = []
    params = []

    if not show_hidden:
        where_clauses.append("e.is_hidden = 0")

    if keyword:
        joins += " LEFT JOIN users u ON e.assigned_user_id = u.id"
        keywords_list = [k.strip() for k in keyword.split() if k.strip()]
        keyword_clauses = []
        for kw in keywords_list:
            keyword_clauses.append(f"(e.document ILIKE %s OR e.{name_column} ILIKE %s OR u.username ILIKE %s)")
            param = f'%{kw}%'
            params.extend([param, param, param])
        where_clauses.append(f"({' AND '.join(keyword_clauses)})")

    if assigned_user_ids:
        real_user_ids = [uid for uid in assigned_user_ids if uid != -1]
        include_unassigned = -1 in assigned_user_ids
        
        user_filter_parts = []
        if real_user_ids:
            user_filter_parts.append("e.assigned_user_id IN %s")
            params.append(tuple(real_user_ids))
        if include_unassigned:
            user_filter_parts.append("e.assigned_user_id IS NULL")
        
        if user_filter_parts:
            where_clauses.append(f"({' OR '.join(user_filter_parts)})")

    if has_matches_only:
        join_key = 'job_id' if item_type == 'jobs' else 'engineer_id'
        where_clauses.append(f"EXISTS (SELECT 1 FROM matching_results mr WHERE mr.{join_key} = e.id AND mr.is_hidden = 0)")

    # â˜…â˜…â˜…ã€ã“ã“ã‹ã‚‰ãŒä¿®æ­£ã®æ ¸ã€‘â˜…â˜…â˜…
    if auto_match_only:
        # item_type ('jobs' or 'engineers') ã‹ã‚‰æœ«å°¾ã® 's' ã‚’å–ã‚Šé™¤ãã€å˜æ•°å½¢ ('job' or 'engineer') ã«ã™ã‚‹
        singular_item_type = item_type.rstrip('s')
        
        where_clauses.append(f"""
            EXISTS (
                SELECT 1 FROM auto_matching_requests amr 
                WHERE amr.item_id = e.id 
                  AND amr.item_type = %s 
                  AND amr.is_active = TRUE
            )
        """)
        params.append(singular_item_type) # å˜æ•°å½¢ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦æ¸¡ã™
    # â˜…â˜…â˜…ã€ä¿®æ­£ã“ã“ã¾ã§ã€‘â˜…â˜…â˜…

    # --- ã‚¯ã‚¨ãƒªã®æœ€çµ‚çµ„ã¿ç«‹ã¦ ---
    final_query = query + joins
    if where_clauses:
        final_query = final_query.replace("SELECT DISTINCT e.id", "SELECT e.id") # DISTINCTã¯æœ€å¾Œã«è¿½åŠ 
        final_query += " WHERE " + " AND ".join(where_clauses)
    
    # --- ã‚½ãƒ¼ãƒˆé †ã®æ±ºå®š ---
    sort_joins = ""
    if sort_column == "æ‹…å½“è€…å" and "LEFT JOIN users u" not in final_query:
        sort_joins = " LEFT JOIN users u ON e.assigned_user_id = u.id"
    
    # JOINãŒç™ºç”Ÿã™ã‚‹å ´åˆã€çµæœãŒé‡è¤‡ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§DISTINCTã‚’ä»˜ã‘ã‚‹
    if joins or sort_joins:
        final_query = final_query.replace("SELECT e.id", "SELECT DISTINCT e.id")

    final_query += sort_joins

    sort_column_map = {"ç™»éŒ²æ—¥": "e.id", "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå": f"e.{name_column}", "æ°å": f"e.{name_column}", "æ‹…å½“è€…å": "u.username"}
    order_by_column = sort_column_map.get(sort_column, "e.id")
    
    order_map = {"é™é †": "DESC", "æ˜‡é †": "ASC"}
    order_by_direction = order_map.get(sort_order, "DESC")
    nulls_order = "NULLS LAST" if order_by_direction == "DESC" else "NULLS FIRST"
    
    final_query += f" ORDER BY {order_by_column} {order_by_direction} {nulls_order}"

    # --- DBå®Ÿè¡Œ ---
    conn = get_db_connection()
    try:
        with conn.cursor() as cursor:
            cursor.execute(final_query, tuple(params))
            return [item[0] for item in cursor.fetchall()]
    except Exception as e:
        print(f"IDãƒªã‚¹ãƒˆã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        try:
            print("--- FAILED QUERY (get_filtered_item_ids) ---")
            print(cursor.mogrify(final_query, tuple(params)).decode('utf-8'))
            print("--------------------")
        except:
            pass
        return []
    finally:
        if conn:
            conn.close()




def get_items_by_ids(item_type: str, ids: list) -> list:
    """
    ã€ä¿®æ­£ç‰ˆã€‘
    IDãƒªã‚¹ãƒˆã«åŸºã¥ããƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—ã€‚æ‹…å½“è€…åã‚‚JOINã—ã€
    çµæœã‚’ã€Œå¤‰æ›´å¯èƒ½ãªã€é€šå¸¸ã®è¾æ›¸(dict)ã®ãƒªã‚¹ãƒˆã¨ã—ã¦è¿”ã™ã€‚
    """
    if not ids or item_type not in ['jobs', 'engineers']:
        return []

    table_name = 'jobs' if item_type == 'jobs' else 'engineers'
    
    query = f"""
        SELECT 
            t.*, 
            u.username as assigned_username,
            fb_counts.feedback_count
        FROM {table_name} t
        LEFT JOIN users u ON t.assigned_user_id = u.id
        LEFT JOIN (
            SELECT 
                {'job_id' if item_type == 'jobs' else 'engineer_id'} as join_key, 
                COUNT(*) as feedback_count
            FROM matching_results
            WHERE feedback_status IS NOT NULL AND feedback_comment IS NOT NULL AND feedback_comment != ''
            GROUP BY join_key
        ) AS fb_counts ON t.id = fb_counts.join_key
        WHERE t.id = ANY(%s)
    """

    conn = get_db_connection()
    try:
        with conn.cursor() as cursor:
            cursor.execute(query, (ids,))
            
            # â–¼â–¼â–¼ã€ã“ã“ã‹ã‚‰ãŒä¿®æ­£ã®æ ¸ã¨ãªã‚‹éƒ¨åˆ†ã€‘â–¼â–¼â–¼
            
            # fetchall() ã®çµæœã¯ DictRow ã®ãƒªã‚¹ãƒˆ
            results_from_db = cursor.fetchall()

            # DictRow ã‚’é€šå¸¸ã® dict ã«å¤‰æ›ã™ã‚‹
            dict_results = [dict(row) for row in results_from_db]
            
            # IDã‚’ã‚­ãƒ¼ã«ã—ãŸè¾æ›¸ã‚’ä½œæˆã—ã¦ã€å…ƒã®IDãƒªã‚¹ãƒˆã®é †åºã«ä¸¦ã¹æ›¿ãˆã‚‹
            results_map = {res['id']: res for res in dict_results}
            return [results_map[id] for id in ids if id in results_map]

            # â–²â–²â–²ã€ä¿®æ­£ã“ã“ã¾ã§ã€‘â–²â–²â–²

    except Exception as e:
        print(f"IDã«ã‚ˆã‚‹ã‚¢ã‚¤ãƒ†ãƒ å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return []
    finally:
        if conn:
            conn.close()







def generate_ai_analysis_on_feedback(job_doc: str, engineer_doc: str, feedback_evaluation: str, feedback_comment: str) -> str:
    """
    æ¡ˆä»¶ãƒ»æŠ€è¡“è€…æƒ…å ±ã¨ã€ãã‚Œã«å¯¾ã™ã‚‹äººé–“ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’å—ã‘å–ã‚Šã€
    AIãŒãã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰ä½•ã‚’å­¦ç¿’ã—ãŸã‹ã‚’åˆ†æãƒ»è¦ç´„ã—ã¦è¿”ã™ã€‚
    """
    if not all([job_doc, engineer_doc, feedback_evaluation, feedback_comment]):
        return "åˆ†æå¯¾è±¡ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚"

    # AIã¸ã®æŒ‡ç¤ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    prompt = f"""
        ã‚ãªãŸã¯ã€ITäººæã®ãƒãƒƒãƒãƒ³ã‚°ç²¾åº¦ã‚’æ—¥ã€…æ”¹å–„ã—ã¦ã„ã‚‹ã€å­¦ç¿’ã™ã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
        ã‚ãªãŸã®ä»•äº‹ã¯ã€æ¡ˆä»¶æƒ…å ±ã¨æŠ€è¡“è€…æƒ…å ±ã€ãã—ã¦ãã‚Œã«å¯¾ã™ã‚‹äººé–“ã®æ‹…å½“è€…ã‹ã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’åˆ†æã—ã€ãã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰ä½•ã‚’å­¦ã³ã€æ¬¡ã«ã©ã†æ´»ã‹ã™ã‹ã‚’ç°¡æ½”ã«è¨€èªåŒ–ã™ã‚‹ã“ã¨ã§ã™ã€‚

        # æŒ‡ç¤º
        - æ‹…å½“è€…ã‹ã‚‰ã®è©•ä¾¡ã¨ã‚³ãƒ¡ãƒ³ãƒˆã®æœ¬è³ªã‚’æ‰ãˆã¦ãã ã•ã„ã€‚
        - ãƒã‚¸ãƒ†ã‚£ãƒ–ãªè©•ä¾¡ã§ã‚ã‚Œã°ã€ãªãœãã‚ŒãŒè‰¯ã‹ã£ãŸã®ã‹ã€ãã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã©ã†å¼·åŒ–ã™ã‚‹ã‹ã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
        - ãƒã‚¬ãƒ†ã‚£ãƒ–ãªè©•ä¾¡ã§ã‚ã‚Œã°ã€ãªãœãã‚ŒãŒæ‚ªã‹ã£ãŸã®ã‹ã€ãã®é–“é•ã„ã‚’ä»Šå¾Œã©ã†é¿ã‘ã‚‹ã‹ã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
        - ã€Œå˜ä¾¡ã€ã€Œã‚¹ã‚­ãƒ«ã€ã€ŒçµŒé¨“å¹´æ•°ã€ã€Œå‹¤å‹™åœ°ã€ãªã©ã®å…·ä½“çš„ãªè¦ç´ ã«è¨€åŠã—ã¦ãã ã•ã„ã€‚
        - ã‚ãªãŸè‡ªèº«ã®è¨€è‘‰ã§ã€å­¦ç¿’å†…å®¹ã‚’å®£è¨€ã™ã‚‹ã‚ˆã†ã«è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚

        # åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿
        ---
        ## æ¡ˆä»¶æƒ…å ±
        {job_doc}
        ---
        ## æŠ€è¡“è€…æƒ…å ±
        {engineer_doc}
        ---
        ## æ‹…å½“è€…ã‹ã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
        - **è©•ä¾¡:** {feedback_evaluation}
        - **ã‚³ãƒ¡ãƒ³ãƒˆ:** {feedback_comment}
        ---

        # å‡ºåŠ›ä¾‹
        - ã“ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰ã€ã‚³ã‚¢ã‚¹ã‚­ãƒ«ãŒå®Œå…¨ã«ä¸€è‡´ã™ã‚‹å ´åˆã€å˜ä¾¡ã«10ä¸‡å††ç¨‹åº¦ã®å·®ãŒã‚ã£ã¦ã‚‚ã€Œè‰¯ã„ãƒãƒƒãƒãƒ³ã‚°ã€ã¨è©•ä¾¡ã•ã‚Œã‚‹ã“ã¨ã‚’å­¦ç¿’ã—ã¾ã—ãŸã€‚ä»Šå¾Œã¯ã€ã‚¹ã‚­ãƒ«ã®ä¸€è‡´åº¦ã‚’ã‚ˆã‚Šé‡è¦–ã—ã€å˜ä¾¡ã®æ¡ä»¶ã‚’å°‘ã—ç·©å’Œã—ã¦å€™è£œã‚’ææ¡ˆã—ã¾ã™ã€‚
        - ã“ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰ã€xxã¨ã„ã†ã‚¹ã‚­ãƒ«ã¯yyã¨ã„ã†æ¥­å‹™å†…å®¹ã¨é–¢é€£æ€§ãŒä½ã„ã¨åˆ¤æ–­ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’å­¦ã³ã¾ã—ãŸã€‚ä»Šå¾Œã¯ã€ã“ã®çµ„ã¿åˆã‚ã›ã§ã®ãƒãƒƒãƒãƒ³ã‚°ã‚¹ã‚³ã‚¢ã‚’ä¸‹æ–¹ä¿®æ­£ã—ã¾ã™ã€‚

        # ã‚ãªãŸã®åˆ†æçµæœã‚’ç”Ÿæˆã—ã¦ãã ã•ã„
    """

    try:
        model = genai.GenerativeModel('models/gemini-2.5-flash-lite')
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        print(f"ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®AIåˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return f"AIã«ã‚ˆã‚‹åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

# â–²â–²â–²ã€æ–°ã—ã„é–¢æ•°ã“ã“ã¾ã§ã€‘â–²â–²â–²


# backend.py

def find_candidates_on_demand(input_text: str, target_rank: str, target_count: int):
    """
    ã€æœ€çµ‚å®Œæˆç‰ˆã€‘
    ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§DBã‹ã‚‰å€™è£œIDã‚’å…¨ä»¶å–å¾—å¾Œã€100ä»¶ãšã¤ã®ãƒãƒƒãƒã§
    ã€Œå‹•çš„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç”Ÿæˆã€ã€Œãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã€ã€ŒAIè©•ä¾¡ã€ã‚’ç¹°ã‚Šè¿”ã—ã€
    ç›®æ¨™ä»¶æ•°ã«é”ã—ãŸã‚‰å‡¦ç†ã‚’æ‰“ã¡åˆ‡ã‚‹ã€åŠ¹ç‡çš„ã‹ã¤é«˜å“è³ªãªå®Ÿè£…ã€‚
    """
    # --- ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã€å€™è£œIDã®å…¨ä»¶å–å¾— ---
    yield "ã‚¹ãƒ†ãƒƒãƒ—1/3: å…¥åŠ›æƒ…å ±ã‹ã‚‰è©•ä¾¡å¯¾è±¡ã¨ãªã‚‹å…¨å€™è£œã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ã„ã¾ã™...\n"
    
    # 1a. ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã¨è¦ç´„
    parsed_data, _ = split_text_with_llm(input_text)
    if not parsed_data:
        yield "âŒ ã‚¨ãƒ©ãƒ¼: å…¥åŠ›æƒ…å ±ã‹ã‚‰æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\n"; return
    
    source_doc_type, search_target_type, source_item = (None, None, None)
    if parsed_data.get("jobs") and parsed_data["jobs"]:
        source_doc_type, search_target_type, source_item = 'job', 'engineer', parsed_data['jobs'][0]
    elif parsed_data.get("engineers") and parsed_data["engineers"]:
        source_doc_type, search_target_type, source_item = 'engineer', 'job', parsed_data['engineers'][0]
    
    if not source_doc_type or not source_item:
        yield "âŒ ã‚¨ãƒ©ãƒ¼: AIã¯ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹é€ åŒ–ã—ã¾ã—ãŸãŒã€ä¸­èº«ãŒæ¡ˆä»¶ã‹æŠ€è¡“è€…ã‹åˆ¤æ–­ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\n"; return
    
    yield f"  > å…¥åŠ›ã¯ã€Œ{source_doc_type}ã€æƒ…å ±ã¨åˆ¤æ–­ã€‚æ¤œç´¢ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¯ã€Œ{search_target_type}ã€ã§ã™ã€‚\n"
    source_doc = _build_meta_info_string(source_doc_type, source_item) + source_item.get("document", "")

    # 1b. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
    yield "  > æ¤œç´¢ã®æ ¸ã¨ãªã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’AIãŒæŠ½å‡ºã—ã¦ã„ã¾ã™...\n"
    search_keywords = []
    try:
        keyword_extraction_prompt = f"""
            ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã«æœ‰åŠ¹ãªæŠ€è¡“è¦ç´ ã€å½¹è·ã€ã‚¹ã‚­ãƒ«åã‚’æœ€å¤§5ã¤ã€ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®å˜èªãƒªã‚¹ãƒˆã¨ã—ã¦æŠœãå‡ºã—ã¦ãã ã•ã„ã€‚
            ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚„çµŒé¨“å¹´æ•°ãªã©ã®ä»˜éšæƒ…å ±ã¯å«ã‚ãšã€å˜èªã®ã¿ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
            ä¾‹:
            å…¥åŠ›:ã€ŒLaravelï¼ˆv10ï¼‰ã§ã®é–‹ç™ºçµŒé¨“ãŒã‚ã‚Šã€Vue.jsï¼ˆv3ï¼‰ã‚‚ä½¿ãˆã¾ã™ã€‚PMè£œä½ã®çµŒé¨“ã‚‚ã‚ã‚Šã¾ã™ã€‚ã€
            å‡ºåŠ›: Laravel, Vue.js, PM
            å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ: --- {input_text} ---
            å‡ºåŠ›:
        """
        model = genai.GenerativeModel('models/gemini-2.5-flash-lite')
        response = model.generate_content(keyword_extraction_prompt)
        keywords_from_ai = [kw.strip() for kw in response.text.strip().split(',') if kw.strip()]
        if not keywords_from_ai: raise ValueError("AIã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¿”ã—ã¾ã›ã‚“ã§ã—ãŸã€‚")
        search_keywords = keywords_from_ai
    except Exception as e:
        yield f"  > âš ï¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®AIæŠ½å‡ºã«å¤±æ•—({e})ã€‚ä»£æ›¿ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢ã—ã¾ã™ã€‚\n"
        fallback_keyword = source_item.get("project_name") or source_item.get("name")
        if fallback_keyword: search_keywords = [fallback_keyword.strip()]

    if not search_keywords:
        yield "  > âŒ æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚\n"; return
    yield f"  > æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: `{'`, `'.join(search_keywords)}`\n"

    # 1c. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ä¸€è‡´ã™ã‚‹ã€ŒIDã®ã¿ã€ã‚’DBã‹ã‚‰å…¨ä»¶å–å¾—
    target_table = search_target_type + 's'
    name_column = "project_name" if search_target_type == 'job' else "name"
    query = f"SELECT id FROM {target_table} WHERE is_hidden = 0 AND ("
    or_conditions = [f"(document ILIKE %s OR {name_column} ILIKE %s)" for _ in search_keywords]
    params = [f"%{kw}%" for kw in search_keywords for _ in (0, 1)]
    query += " OR ".join(or_conditions)
    query += ") ORDER BY id DESC"

    conn = get_db_connection()
    try:
        with conn.cursor() as cursor:
            cursor.execute(query, tuple(params))
            all_candidate_ids = [item['id'] for item in cursor.fetchall()]
    finally:
        if conn: conn.close()

    if not all_candidate_ids:
        yield "âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ¤œç´¢ã—ã¾ã—ãŸãŒã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ä¸€è‡´ã™ã‚‹å€™è£œã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n"; return
    yield f"  > ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã®çµæœã€{len(all_candidate_ids)}ä»¶ã®è©•ä¾¡å¯¾è±¡å€™è£œã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸã€‚\n"

    # --- ãƒ«ãƒ¼ãƒ—ã®åˆæœŸåŒ– ---
    final_candidates = []
    DB_FETCH_BATCH_SIZE = 25
    rank_order = ['S', 'A', 'B', 'C', 'D']
    valid_ranks = rank_order[:rank_order.index(target_rank) + 1]
    embedding_model = load_embedding_model()
    if not embedding_model:
        yield "âŒ ã‚¨ãƒ©ãƒ¼: åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚\n"; return
    query_vector = embedding_model.encode([source_doc], normalize_embeddings=True)

    # --- ã‚¹ãƒ†ãƒƒãƒ—2: ç›®æ¨™ä»¶æ•°ã«é”ã™ã‚‹ã¾ã§æ¤œç´¢ãƒ»è©•ä¾¡ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œ ---
    yield f"\nã‚¹ãƒ†ãƒƒãƒ—2/3: å€™è£œè€…ã‚’{DB_FETCH_BATCH_SIZE}ä»¶ãšã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†ã‘ã¦ã€AIè©•ä¾¡ã‚’é–‹å§‹ã—ã¾ã™...\n"
    yield f"  > ç›®æ¨™: ã€Œ{target_rank}ã€ãƒ©ãƒ³ã‚¯ä»¥ä¸Šã‚’ {target_count}ä»¶ è¦‹ã¤ã‘ã‚‹ã¾ã§å‡¦ç†ã‚’ç¶šã‘ã¾ã™ã€‚\n"
    
    for page in range(0, len(all_candidate_ids), DB_FETCH_BATCH_SIZE):
        batch_ids = all_candidate_ids[page : page + DB_FETCH_BATCH_SIZE]
        if not batch_ids: break

        yield f"\n--- æ¤œç´¢ã‚µã‚¤ã‚¯ãƒ« {page//DB_FETCH_BATCH_SIZE + 1} (DBã® {page+1}ä»¶ç›®ã€œ) ---\n"
        
        # 2a. ã“ã®ãƒãƒƒãƒã§å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’DBã‹ã‚‰å–å¾—
        candidate_records_for_indexing = get_items_by_ids(search_target_type + 's', batch_ids)
        if not candidate_records_for_indexing: continue
        
        # 2b. å‹•çš„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç”Ÿæˆã¨ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ (ã“ã®ãƒãƒƒãƒå†…ã§ã®å‡¦ç†)
        yield f"  > {len(candidate_records_for_indexing)}ä»¶ã®å€™è£œã‹ã‚‰ã€æ„å‘³çš„ã«è¿‘ã„ã‚‚ã®ã‚’æ¢ã—ã¦ã„ã¾ã™...\n"
        dimension = embedding_model.get_sentence_embedding_dimension()
        index = faiss.IndexIDMap(faiss.IndexFlatIP(dimension))
        ids = np.array([item['id'] for item in candidate_records_for_indexing], dtype=np.int64)
        documents = [str(item['document']) for item in candidate_records_for_indexing]
        embeddings = embedding_model.encode(documents, normalize_embeddings=True, show_progress_bar=False)
        index.add_with_ids(embeddings, ids)
        
        _, result_ids = index.search(query_vector, len(documents))
        batch_sorted_ids = [int(i) for i in result_ids[0] if i != -1]
        if not batch_sorted_ids:
            yield "  > ã“ã®ãƒãƒƒãƒã«ã¯é¡ä¼¼ã™ã‚‹å€™è£œãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ¬¡ã®ã‚µã‚¤ã‚¯ãƒ«ã«é€²ã¿ã¾ã™ã€‚\n"; continue

        # 2c. AIã«ã‚ˆã‚‹å†è©•ä¾¡
        candidate_records_for_eval = get_items_by_ids(search_target_type + 's', batch_sorted_ids)
        for record in candidate_records_for_eval:
            candidate = dict(record)
            name = candidate.get('name') or candidate.get('project_name')
            
            skills_text = ""
            if search_target_type == 'engineer':
                match = re.search(r'\[ä¸»è¦ã‚¹ã‚­ãƒ«:\s*([^\]]+)\]', candidate.get('document', ''))
                if match: skills_text = match.group(1)
            elif search_target_type == 'job':
                match = re.search(r'\[å¿…é ˆã‚¹ã‚­ãƒ«:\s*([^\]]+)\]', candidate.get('document', ''))
                if match: skills_text = match.group(1)
            
            yield {"type": "eval_progress", "message": f"ã€Œ{name}ã€ã‚’è©•ä¾¡ä¸­...", "skills": skills_text[:100] + "..." if len(skills_text) > 100 else skills_text}
            
            llm_result = get_match_summary_with_llm(source_doc, candidate['document'])

            if llm_result and llm_result.get('summary') in valid_ranks:
                candidate['grade'] = llm_result.get('summary')
                candidate['positive_points'] = llm_result.get('positive_points', [])
                candidate['concern_points'] = llm_result.get('concern_points', [])
                final_candidates.append(candidate)
                yield f"    -> âœ… ãƒ’ãƒƒãƒˆï¼ (ãƒ©ãƒ³ã‚¯: **{candidate['grade']}**) - ç¾åœ¨ {len(final_candidates)}/{target_count} ä»¶\n"
            else:
                actual_grade = llm_result.get('summary') if llm_result else "è©•ä¾¡å¤±æ•—"
                yield f"    -> ï½½ï½·ï½¯ï¾Œï¾Ÿ (ãƒ©ãƒ³ã‚¯: **{actual_grade}**)\n"

            if len(final_candidates) >= target_count:
                yield f"\nğŸ‰ ç›®æ¨™ã® {target_count} ä»¶ã«åˆ°é”ã—ãŸãŸã‚ã€å…¨ã¦ã®å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚\n"
                break
        
        if len(final_candidates) >= target_count:
            break
    
    if not final_candidates:
        yield "\nâ„¹ï¸ å…¨ã¦ã®å€™è£œè€…ã‚’è©•ä¾¡ã—ã¾ã—ãŸãŒã€ç›®æ¨™ãƒ©ãƒ³ã‚¯ã«é”ã™ã‚‹çµæœã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n"
    elif len(final_candidates) < target_count:
        yield "\nâ„¹ï¸ å…¨ã¦ã®å€™è£œè€…ã®è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\n"

    # --- ã‚¹ãƒ†ãƒƒãƒ—3: æœ€çµ‚çµæœã®è¡¨ç¤º ---
    yield f"\nã‚¹ãƒ†ãƒƒãƒ—3/3: æœ€çµ‚çš„ãªå€™è£œè€…ãƒªã‚¹ãƒˆã‚’è¡¨ç¤ºã—ã¾ã™ã€‚\n---\n### **æœ€çµ‚å€™è£œè€…ãƒªã‚¹ãƒˆ**\n"
    if not final_candidates:
        yield f"è©•ä¾¡ã®çµæœã€æŒ‡å®šã•ã‚ŒãŸæ¡ä»¶ï¼ˆ{target_rank}ãƒ©ãƒ³ã‚¯ä»¥ä¸Šï¼‰ã«åˆè‡´ã™ã‚‹å€™è£œè€…ã¯ã„ã¾ã›ã‚“ã§ã—ãŸã€‚\n"; return
    
    final_candidates.sort(key=lambda x: rank_order.index(x['grade']))

    for i, candidate in enumerate(final_candidates):
        name = candidate.get('name') or candidate.get('project_name')
        page_name = "æŠ€è¡“è€…è©³ç´°" if search_target_type == 'engineer' else "æ¡ˆä»¶è©³ç´°"
        link = f"/{page_name}?id={candidate['id']}" 
        
        yield f"#### **{i+1}. [{name} (ID: {candidate['id']})]({link}) - ãƒ©ãƒ³ã‚¯: {candidate['grade']}**\n"
        if candidate.get('positive_points'):
            yield "**ãƒã‚¸ãƒ†ã‚£ãƒ–ãªç‚¹:**\n"
            for point in candidate['positive_points']: yield f"- {point}\n"
        if candidate.get('concern_points'):
            yield "**æ‡¸å¿µç‚¹:**\n"
            for point in candidate['concern_points']: yield f"- {point}\n"
        yield "\n"


# backend.py ã® get_all_candidate_ids_and_source_doc é–¢æ•°ã‚’ã“ã¡ã‚‰ã«ç½®ãæ›ãˆã¦ãã ã•ã„

def get_all_candidate_ids_and_source_doc(input_text: str) -> dict:
    """
    ã€ä¿®æ­£ç‰ˆã€‘
    å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’è§£æã—ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ä¸€æ¬¡çµã‚Šè¾¼ã¿ã‚’è¡Œã£ãŸå€™è£œIDã®å…¨ãƒªã‚¹ãƒˆã¨ã€
    å¾Œç¶šã®å‡¦ç†ã§å¿…è¦ãªæƒ…å ±ã‚’è¾æ›¸ã§è¿”ã™ã€‚
    """
    logs = []
    
    # --- 1a. ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã¨è¦ç´„ ---
    logs.append("ã‚¹ãƒ†ãƒƒãƒ—1/2: å…¥åŠ›æƒ…å ±ã‚’è§£æã—ã¦ã„ã¾ã™...")
    # split_text_with_llm ã¯UIä¾å­˜ã®å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€UIéä¾å­˜ç‰ˆãŒæœ›ã¾ã—ã„
    # ã“ã“ã§ã¯ãã®ã¾ã¾ä½¿ç”¨ã™ã‚‹ãŒã€st.spinnerãªã©ã¯å‘¼ã³å‡ºã—å…ƒã§ç®¡ç†ã™ã‚‹ã®ãŒç†æƒ³
    with st.spinner("AIãŒå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†é¡ãƒ»æ§‹é€ åŒ–ä¸­..."):
        parsed_data, llm_logs = split_text_with_llm(input_text)
    logs.extend(llm_logs)

    if not parsed_data:
        logs.append("âŒ ã‚¨ãƒ©ãƒ¼: å…¥åŠ›æƒ…å ±ã‹ã‚‰æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        return {"logs": logs}
    
    source_doc_type, search_target_type, source_item = (None, None, None)
    if parsed_data.get("jobs") and parsed_data["jobs"]:
        source_doc_type, search_target_type, source_item = 'job', 'engineer', parsed_data['jobs'][0]
    elif parsed_data.get("engineers") and parsed_data["engineers"]:
        source_doc_type, search_target_type, source_item = 'engineer', 'job', parsed_data['engineers'][0]
    
    if not source_doc_type or not source_item:
        logs.append("âŒ ã‚¨ãƒ©ãƒ¼: ãƒ†ã‚­ã‚¹ãƒˆã¯æ§‹é€ åŒ–ã•ã‚Œã¾ã—ãŸãŒã€æ¡ˆä»¶ã‹æŠ€è¡“è€…ã‹åˆ¤æ–­ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        return {"logs": logs}
        
    logs.append(f"  > å…¥åŠ›ã¯ã€Œ{source_doc_type}ã€æƒ…å ±ã¨åˆ¤æ–­ã€‚æ¤œç´¢ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¯ã€Œ{search_target_type}ã€ã§ã™ã€‚")
    source_doc = _build_meta_info_string(source_doc_type, source_item) + source_item.get("document", "")

    # --- 1b. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º ---
    logs.append("  > æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’AIãŒæŠ½å‡ºã—ã¦ã„ã¾ã™...")
    search_keywords = []
    try:
        keyword_extraction_prompt = f"""
            ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã«æœ‰åŠ¹ãªæŠ€è¡“è¦ç´ ã€å½¹è·ã€ã‚¹ã‚­ãƒ«åã‚’æœ€å¤§5ã¤ã€ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®å˜èªãƒªã‚¹ãƒˆã¨ã—ã¦æŠœãå‡ºã—ã¦ãã ã•ã„ã€‚
            ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚„çµŒé¨“å¹´æ•°ãªã©ã®ä»˜éšæƒ…å ±ã¯å«ã‚ãšã€å˜èªã®ã¿ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
            ä¾‹:
            å…¥åŠ›:ã€ŒLaravelï¼ˆv10ï¼‰ã§ã®é–‹ç™ºçµŒé¨“ãŒã‚ã‚Šã€Vue.jsï¼ˆv3ï¼‰ã‚‚ä½¿ãˆã¾ã™ã€‚PMè£œä½ã®çµŒé¨“ã‚‚ã‚ã‚Šã¾ã™ã€‚ã€
            å‡ºåŠ›: Laravel, Vue.js, PM
            å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ: --- {input_text} ---
            å‡ºåŠ›:
        """
        model = genai.GenerativeModel('models/gemini-2.5-flash-lite')
        response = model.generate_content(keyword_extraction_prompt)
        keywords_from_ai = [kw.strip() for kw in response.text.strip().split(',') if kw.strip()]
        if not keywords_from_ai: raise ValueError("AI did not return keywords.")
        search_keywords = keywords_from_ai
    except Exception as e:
        logs.append(f"  > âš ï¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºå¤±æ•—({e})ã€‚ä»£æ›¿ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢ã—ã¾ã™ã€‚")
        fallback_keyword = source_item.get("project_name") or source_item.get("name")
        if fallback_keyword:
            search_keywords = [fallback_keyword.strip()]

    if not search_keywords:
        logs.append("  > âŒ æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç‰¹å®šã§ããšã€å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
        # â˜…â˜…â˜… å€™è£œãŒ0ä»¶ã§ã‚ã‚‹ã“ã¨ã‚’æ˜ç¤ºã—ã¦è¿”ã™ â˜…â˜…â˜…
        return {"logs": logs, "all_candidate_ids": []}
        
    logs.append(f"  > æŠ½å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: `{'`, `'.join(search_keywords)}`")

    # --- 1c. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ä¸€è‡´ã™ã‚‹ã€ŒIDã®ã¿ã€ã‚’DBã‹ã‚‰å…¨ä»¶å–å¾— ---
    logs.append("ã‚¹ãƒ†ãƒƒãƒ—2/2: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ä¸€è‡´ã™ã‚‹å€™è£œã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ã„ã¾ã™...")
    target_table = search_target_type + 's'
    name_column = "project_name" if search_target_type == 'job' else "name"
    
    # â˜…â˜…â˜… `params` ã‚’ã“ã“ã§åˆæœŸåŒ–ã™ã‚‹ â˜…â˜…â˜…
    params = []
    
    query = f"SELECT id FROM {target_table} WHERE is_hidden = 0 AND ("
    or_conditions = []
    for kw in search_keywords:
        or_conditions.append(f"(document ILIKE %s OR {name_column} ILIKE %s)")
        params.extend([f"%{kw}%", f"%{kw}%"])
    
    query += " OR ".join(or_conditions)
    query += ") ORDER BY id DESC"

    conn = get_db_connection()
    try:
        with conn.cursor() as cursor:
            cursor.execute(query, tuple(params))
            all_candidate_ids = [item['id'] for item in cursor.fetchall()]
    except Exception as e:
        logs.append(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return {"logs": logs, "all_candidate_ids": []}
    finally:
        if conn: conn.close()

    if not all_candidate_ids:
        logs.append("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ¤œç´¢ã—ã¾ã—ãŸãŒã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ä¸€è‡´ã™ã‚‹å€™è£œã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return {"logs": logs, "all_candidate_ids": []}
    
    return {
        "logs": logs,
        "all_candidate_ids": all_candidate_ids,
        "source_doc": source_doc,
        "search_target_type": search_target_type,
    }


# backend.py ã® evaluate_next_candidates é–¢æ•°ã‚’ã“ã¡ã‚‰ã«ç½®ãæ›ãˆã¦ãã ã•ã„
def evaluate_next_candidates(candidate_ids: list, source_doc: str, search_target_type: str, target_rank: str):
    """
    ã€ä¿®æ­£ç‰ˆã€‘
    ãƒ’ãƒƒãƒˆã—ãŸå ´åˆã€Markdownã§ã¯ãªãã€ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã§å‡¦ç†ã—ã‚„ã™ã„ã‚ˆã†ã«
    æ§‹é€ åŒ–ã•ã‚ŒãŸè¾æ›¸ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™ã€‚
    """
    if not candidate_ids:
        return

    rank_order = ['S', 'A', 'B', 'C', 'D']
    try:
        valid_ranks = rank_order[:rank_order.index(target_rank) + 1]
    except ValueError:
        valid_ranks = []

    candidate_records = get_items_by_ids(search_target_type + 's', candidate_ids)

    for record in candidate_records:
        page_name = "æŠ€è¡“è€…è©³ç´°" if search_target_type == 'engineer' else "æ¡ˆä»¶è©³ç´°"
        candidate = dict(record)
        name = candidate.get('name') or candidate.get('project_name')
        
        # ... (å‰ç•¥: eval_progress, llm_start ã® yield) ...
        
        llm_result = get_match_summary_with_llm(source_doc, candidate['document'])

        if llm_result and llm_result.get('summary') in valid_ranks:
            # â˜…â˜…â˜…ã€ã“ã“ã‹ã‚‰ãŒä¿®æ­£ã®æ ¸ã€‘â˜…â˜…â˜…
            # --- ãƒ’ãƒƒãƒˆã—ãŸå ´åˆ ---
            
            # ä»¥å‰: Markdownã‚’ç”Ÿæˆã—ã¦ã„ãŸ
            # å¤‰æ›´å¾Œ: å¿…è¦ãªæƒ…å ±ã‚’ã™ã¹ã¦å«ã‚“ã è¾æ›¸ã‚’è¿”ã™
            yield {
                "type": "hit_candidate",
                "data": {
                    "id": candidate['id'],
                    "name": name,
                    "grade": llm_result.get('summary'),
                    "positive_points": llm_result.get('positive_points', []),
                    "concern_points": llm_result.get('concern_points', []),
                    "page_name": page_name
                }
            }
            yield {"type": "pause"}
            # â˜…â˜…â˜…ã€ä¿®æ­£ã“ã“ã¾ã§ã€‘â˜…â˜…â˜…

        else:
            # --- ã‚¹ã‚­ãƒƒãƒ—ã—ãŸå ´åˆ (å¤‰æ›´ãªã—) ---
            actual_grade = llm_result.get('summary') if llm_result else "è©•ä¾¡å¤±æ•—"
            yield {
                "type": "skip_log",
                "message": f"å€™è£œã€Œ{name}ã€ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚(AIè©•ä¾¡: {actual_grade})"
            }






def rematch_job_with_keyword_filtering(job_id, target_rank='B', target_count=5):
    """
    ã€æ¡ˆä»¶è©³ç´°ãƒšãƒ¼ã‚¸å°‚ç”¨ã€‘
    AIã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã«ã‚ˆã‚‹çµã‚Šè¾¼ã¿ã‚’è¡Œã„ã€æœ€æ–°ã®æŠ€è¡“è€…ã‹ã‚‰é †ã«ãƒãƒƒãƒãƒ³ã‚°è©•ä¾¡ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
    å‡¦ç†ã®å…¨ã‚¹ãƒ†ãƒƒãƒ—ã‚’ãƒ­ã‚°ã¨ã—ã¦yieldã™ã‚‹ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã€‚
    """
    if not job_id:
        yield "âŒ æ¡ˆä»¶IDãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
        return

    rank_order = ['S', 'A', 'B', 'C', 'D']
    try:
        valid_ranks = rank_order[:rank_order.index(target_rank) + 1]
    except ValueError:
        yield f"âŒ ç„¡åŠ¹ãªç›®æ¨™ãƒ©ãƒ³ã‚¯ãŒæŒ‡å®šã•ã‚Œã¾ã—ãŸ: {target_rank}"
        return

    conn = get_db_connection()
    try:
        with conn.cursor() as cursor:
            # --- ã‚¹ãƒ†ãƒƒãƒ—1: æ¡ˆä»¶æƒ…å ±ã®å–å¾— ---
            yield "ğŸ“„ å¯¾è±¡æ¡ˆä»¶ã®å…ƒæƒ…å ±ã‚’å–å¾—ã—ã¦ã„ã¾ã™..."
            cursor.execute("SELECT source_data_json, document FROM jobs WHERE id = %s", (job_id,))
            job_record = cursor.fetchone()
            if not job_record or not job_record['source_data_json']:
                yield f"âŒ æ¡ˆä»¶ID:{job_id} ã®å…ƒæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
                return
            
            job_doc = job_record['document']
            source_data = json.loads(job_record['source_data_json'])
            original_text = source_data.get('body', '') + "".join([f"\n\n--- æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«: {att['filename']} ---\n{att.get('content', '')}" for att in source_data.get('attachments', []) if att.get('content')])

            # --- ã‚¹ãƒ†ãƒƒãƒ—2: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æŠ½å‡º ---
            yield "ğŸ¤– æ¤œç´¢ã®æ ¸ã¨ãªã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’AIãŒæŠ½å‡ºã—ã¦ã„ã¾ã™..."

            # â˜…â˜…â˜… AIã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãƒ­ã‚°ã‚’è¨˜éŒ² (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º) â˜…â˜…â˜…
            try:
                cursor.execute("INSERT INTO ai_activity_log (activity_type) VALUES ('keyword_extraction')")
                # ã“ã“ã§ã¯ã¾ã ã‚³ãƒŸãƒƒãƒˆã—ãªã„ï¼ˆãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã®ä¸€éƒ¨ã¨ã™ã‚‹ï¼‰
            except Exception as log_err:
                yield f"  - âš ï¸ AIã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãƒ­ã‚°ã®è¨˜éŒ²ã«å¤±æ•—: {log_err}"

            search_keywords = []
            try:
                keyword_extraction_prompt = f"""
                    ä»¥ä¸‹ã®æ¡ˆä»¶æƒ…å ±ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€æŠ€è¡“è€…ã‚’æ¢ã™ä¸Šã§é‡è¦ã¨ãªã‚‹æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æœ€å¤§10å€‹ã€ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®å˜èªãƒªã‚¹ãƒˆã¨ã—ã¦æŠœãå‡ºã—ã¦ãã ã•ã„ã€‚
                    ã€Œå¿…é ˆã€ã€Œæ­“è¿ã€ãªã©ã®æ•è©ã‚„ã€çµŒé¨“å¹´æ•°ã€å˜ä¾¡ãªã©ã®ä»˜éšæƒ…å ±ã¯å«ã‚ãšã€æŠ€è¡“åã‚„å½¹è·åãªã©ã®å˜èªã®ã¿ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
                    å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ: --- {original_text} ---
                    å‡ºåŠ›:
                """
                model = genai.GenerativeModel('models/gemini-2.5-flash-lite')
                response = model.generate_content(keyword_extraction_prompt)
                search_keywords = [kw.strip() for kw in response.text.strip().split(',') if kw.strip()]
                if not search_keywords: raise ValueError("AI did not return keywords.")
                yield f"  > æŠ½å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: `{', '.join(search_keywords)}`"
            except Exception as e:
                yield f"âš ï¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®AIæŠ½å‡ºã«å¤±æ•—({e})ã€‚å…¨æŠ€è¡“è€…ã‚’å¯¾è±¡ã«å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™ã€‚"

            # --- ã‚¹ãƒ†ãƒƒãƒ—3: DBä¸€æ¬¡çµã‚Šè¾¼ã¿ (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ä¸€è‡´ã™ã‚‹æŠ€è¡“è€…IDã‚’å–å¾—) ---
            yield "ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ä¸€è‡´ã™ã‚‹æŠ€è¡“è€…å€™è£œã‚’DBã‹ã‚‰ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ã„ã¾ã™..."

            # â˜…â˜…â˜…ã€ã“ã“ã‹ã‚‰ãŒä¿®æ­£ã®æ ¸ã€‘â˜…â˜…â˜…
            CANDIDATE_LIMIT = 1000 # è©•ä¾¡å¯¾è±¡ã®ä¸Šé™æ•°ã‚’å®šç¾©

            base_query = "SELECT id FROM engineers WHERE is_hidden = 0"
            where_clauses = []
            params = []

            if search_keywords:
                or_conditions = [f"(document ILIKE %s OR name ILIKE %s)" for _ in search_keywords]
                where_clauses.append(f"({ ' OR '.join(or_conditions) })")
                params.extend([f"%{kw}%" for kw in search_keywords for _ in (0, 1)])
            
            if where_clauses:
                base_query += " AND " + " AND ".join(where_clauses)
            
            # ORDER BYã§æœ€æ–°é †ã«ä¸¦ã¹ã€LIMITã§ä¸Šé™ã‚’è¨­å®š
            final_query = f"{base_query} ORDER BY id DESC LIMIT {CANDIDATE_LIMIT}"
            
            cursor.execute(final_query, tuple(params))
            # â˜…â˜…â˜…ã€ä¿®æ­£ã“ã“ã¾ã§ã€‘â˜…â˜…â˜…
            
            
            candidate_ids = [item['id'] for item in cursor.fetchall()]

            if not candidate_ids:
                yield "âš ï¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ä¸€è‡´ã™ã‚‹æŠ€è¡“è€…ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚"
                conn.commit()
                return
            yield f"  > **{len(candidate_ids)}å** ã®è©•ä¾¡å¯¾è±¡å€™è£œã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸã€‚"
            
            # --- ã‚¹ãƒ†ãƒƒãƒ—4: æ—¢å­˜ãƒãƒƒãƒãƒ³ã‚°ã®ã‚¯ãƒªã‚¢ã¨é€æ¬¡è©•ä¾¡ ---
            cursor.execute("DELETE FROM matching_results WHERE job_id = %s", (job_id,))
            yield f"ğŸ—‘ï¸ æ¡ˆä»¶ID:{job_id} ã®æ—¢å­˜ãƒãƒƒãƒãƒ³ã‚°çµæœã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚"
            yield "ğŸ”„ çµã‚Šè¾¼ã‚“ã å€™è£œè€…ãƒªã‚¹ãƒˆã«å¯¾ã—ã¦ã€é †ã«ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™..."

            candidate_records = get_items_by_ids_sync('engineers', candidate_ids) # åŒæœŸç‰ˆã§ä¸€æ‹¬å–å¾—
            
            found_count = 0
            processed_count = 0
            now_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

            for engineer in candidate_records:
                processed_count += 1
                yield f"  `({processed_count}/{len(candidate_records)})` æŠ€è¡“è€… **{engineer['name']}** ã¨ãƒãƒƒãƒãƒ³ã‚°ä¸­..."
                
                # â˜…â˜…â˜…ã€ã“ã“ã‹ã‚‰ãŒä¿®æ­£ã®æ ¸ã€‘â˜…â˜…â˜…
                # AIè©•ä¾¡ã®å®Ÿè¡Œå‰ã«ã€ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãƒ­ã‚°ã‚’è¨˜éŒ²ã™ã‚‹
                try:
                    cursor.execute(
                        "INSERT INTO ai_activity_log (activity_type) VALUES ('evaluation')"
                    )
                    # ã“ã®INSERTã¯ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã®ä¸€éƒ¨ãªã®ã§ã€ã“ã“ã§ã¯ã¾ã commitã—ãªã„
                except Exception as log_err:
                    yield f"  - âš ï¸ AIã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãƒ­ã‚°ã®è¨˜éŒ²ã«å¤±æ•—: {log_err}"
                # â˜…â˜…â˜…ã€ä¿®æ­£ã“ã“ã¾ã§ã€‘â˜…â˜…â˜…

                llm_result = get_match_summary_with_llm(job_doc, engineer['document'])

                if llm_result and 'summary' in llm_result:
                    grade = llm_result.get('summary')
                    if grade in valid_ranks:
                        positive_points = json.dumps(llm_result.get('positive_points', []), ensure_ascii=False)
                        concern_points = json.dumps(llm_result.get('concern_points', []), ensure_ascii=False)
                        score = 0.0
                        
                        try:
                            cursor.execute(
                                'INSERT INTO matching_results (job_id, engineer_id, score, created_at, grade, positive_points, concern_points) VALUES (%s, %s, %s, %s, %s, %s, %s)',
                                (job_id, engineer['id'], score, now_str, grade, positive_points, concern_points)
                            )
                            yield f"    -> **<span style='color: #28a745;'>âœ… ãƒ’ãƒƒãƒˆï¼</span>** è©•ä¾¡: **{grade}** ... DBã«ä¿å­˜ã—ã¾ã—ãŸã€‚"
                            found_count += 1
                        except Exception as db_err:
                            yield f"    -> <span style='color: #dc3545;'>âŒ DBä¿å­˜ã‚¨ãƒ©ãƒ¼</span>: {db_err}"
                    else:
                        yield f"    -> <span style='color: #ffc107;'>â­ï¸ ã‚¹ã‚­ãƒƒãƒ—</span> (è©•ä¾¡: {grade})"
                else:
                    yield "    -> <span style='color: #dc3545;'>âŒ LLMè©•ä¾¡å¤±æ•—</span>"

                if found_count >= target_count:
                    yield f"ğŸ‰ ç›®æ¨™ã® {target_count} ä»¶ã«åˆ°é”ã—ãŸãŸã‚ã€å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚"
                    break
            
            if found_count < target_count:
                yield f"â„¹ï¸ ã™ã¹ã¦ã®å€™è£œè€…ã®è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸã€‚(ãƒ’ãƒƒãƒˆæ•°: {found_count}ä»¶)"

        conn.commit()
        yield "âœ… ã™ã¹ã¦ã®å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚"
    except Exception as e:
        conn.rollback()
        yield f"âŒ å†è©•ä¾¡ãƒ»å†ãƒãƒƒãƒãƒ³ã‚°ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
        import traceback
        yield f"```\n{traceback.format_exc()}\n```"

def get_items_by_ids_sync(item_type: str, ids: list, conn=None) -> list:
    """
    ã€SQLä¿®æ­£ç‰ˆã€‘
    çœç•¥ã•ã‚Œã¦ã„ãŸSQLã‚¯ã‚¨ãƒªã‚’å®Œå…¨ã«å®Ÿè£…ã™ã‚‹ã€‚
    DBæ¥ç¶šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å¼•æ•°ã§å—ã‘å–ã‚Œã‚‹DIãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œã€‚
    """
    if not ids or item_type not in ['jobs', 'engineers']:
        return []

    should_close_conn = False
    if conn is None:
        conn = get_db_connection()
        if not conn: return []
        should_close_conn = True
    
    table_name = 'jobs' if item_type == 'jobs' else 'engineers'
    
    # â˜…â˜…â˜…ã€ã“ã“ã‹ã‚‰ãŒä¿®æ­£ã®æ ¸ã€‘â˜…â˜…â˜…
    # çœç•¥ã•ã‚Œã¦ã„ãŸã‚¯ã‚¨ãƒªã‚’å®Œå…¨ã«è¨˜è¿°ã™ã‚‹
    query = f"""
        SELECT 
            t.*, 
            u.username as assigned_username,
            COALESCE(mc.match_count, 0) as match_count,
            CASE 
                WHEN amr.is_active = TRUE THEN TRUE
                ELSE FALSE
            END as auto_match_active
        FROM {table_name} t
        LEFT JOIN users u ON t.assigned_user_id = u.id
        LEFT JOIN (
            SELECT 
                {'job_id' if item_type == 'jobs' else 'engineer_id'} as item_id, 
                COUNT(*) as match_count
            FROM matching_results
            WHERE is_hidden = 0
            GROUP BY item_id
        ) AS mc ON t.id = mc.item_id
        LEFT JOIN auto_matching_requests amr 
            ON t.id = amr.item_id AND amr.item_type = %s
        WHERE t.id = ANY(%s)
    """
    # â˜…â˜…â˜…ã€ä¿®æ­£ã“ã“ã¾ã§ã€‘â˜…â˜…â˜…

    BATCH_SIZE = 200
    results_map = {}
    
    try:
        with conn.cursor() as cursor:
            for i in range(0, len(ids), BATCH_SIZE):
                batch_ids = ids[i : i + BATCH_SIZE]
                if not batch_ids: continue
                
                # item_type ã‚’ã‚¯ã‚¨ãƒªã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«è¿½åŠ 
                cursor.execute(query, (item_type.rstrip('s'), batch_ids))
                
                batch_results = cursor.fetchall()
                for row in batch_results:
                    results_map[row['id']] = dict(row)
    except Exception as e:
        print(f"IDã«ã‚ˆã‚‹ã‚¢ã‚¤ãƒ†ãƒ å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return []
    finally:
        if should_close_conn and conn:
            conn.close()
    
    final_ordered_results = [results_map[id] for id in ids if id in results_map]
    return final_ordered_results



# å¿…è¦ã§ã‚ã‚Œã°ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç‰ˆã‚‚å®šç¾©ã—ã¦ãŠã
# (ç¾æ™‚ç‚¹ã§ã¯ç›´æ¥å‘¼ã³å‡ºã•ã‚Œã¦ã„ã¾ã›ã‚“ãŒã€å°†æ¥ã®ãŸã‚ã«æ®‹ã—ã¦ãŠãã¨è‰¯ã„ã§ã—ã‚‡ã†)
def get_items_by_ids_stream(item_type: str, ids: list):
    """
    ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç‰ˆãƒ»ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã€‘
    å¤§é‡ã®IDã‚’ãƒãƒƒãƒã§å–å¾—ã—ã€ãã®é€²æ—ã‚’yieldã§å ±å‘Šã™ã‚‹ã€‚
    æœ€çµ‚çš„ã«å…¨ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆã‚’returnã§è¿”ã™ã€‚
    """
    if not ids or item_type not in ['jobs', 'engineers']:
        return []

    table_name = 'jobs' if item_type == 'jobs' else 'engineers'
    BATCH_SIZE = 200
    results_map = {}
    total_ids = len(ids)

    conn = get_db_connection()
    try:
        with conn.cursor() as cursor:
            for i in range(0, total_ids, BATCH_SIZE):
                batch_ids = ids[i : i + BATCH_SIZE]
                if not batch_ids:
                    continue
                
                processed_count = min(i + BATCH_SIZE, total_ids)
                yield f"  - DBã‹ã‚‰è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­... ({processed_count} / {total_ids} ä»¶)"
                
                query = f"""
                    SELECT ... 
                    WHERE t.id = ANY(%s)
                """ # (ã‚¯ã‚¨ãƒªæœ¬ä½“ã¯ sync ç‰ˆã¨åŒã˜)
                cursor.execute(query, (batch_ids,))
                
                batch_results = cursor.fetchall()
                for row in batch_results:
                    results_map[row['id']] = dict(row)
        
        yield f"  - âœ… å…¨ {total_ids} ä»¶ã®ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†ã€‚"

    except Exception as e:
        yield f"âŒ IDã«ã‚ˆã‚‹ã‚¢ã‚¤ãƒ†ãƒ å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
        return []
    finally:
        if conn:
            conn.close()

    final_ordered_results = [results_map[id] for id in ids if id in results_map]
    return final_ordered_results



# backend.py ã®æœ«å°¾ã«ã€ä»¥ä¸‹ã®æ–°ã—ã„é–¢æ•°ã‚’è¿½åŠ ã—ã¦ãã ã•ã„
# backend.py ã® rematch_engineer_with_keyword_filtering é–¢æ•°ã‚’ã“ã¡ã‚‰ã«ç½®ãæ›ãˆã¦ãã ã•ã„

def rematch_engineer_with_keyword_filtering(engineer_id, target_rank='B', target_count=5):
    """
    ã€æŠ€è¡“è€…è©³ç´°ãƒšãƒ¼ã‚¸å°‚ç”¨ãƒ»å®Œæˆç‰ˆã€‘
    AIã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºâ†’DBçµã‚Šè¾¼ã¿â†’é€æ¬¡è©•ä¾¡ã‚’è¡Œã†ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã€‚
    DIãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œã—ã€st.secretsã«ä¾å­˜ã—ãªã„ã€‚
    """
    if not engineer_id:
        yield "âŒ æŠ€è¡“è€…IDãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
        return

    rank_order = ['S', 'A', 'B', 'C', 'D']
    try:
        valid_ranks = rank_order[:rank_order.index(target_rank) + 1]
    except ValueError:
        yield f"âŒ ç„¡åŠ¹ãªç›®æ¨™ãƒ©ãƒ³ã‚¯ãŒæŒ‡å®šã•ã‚Œã¾ã—ãŸ: {target_rank}"
        return

    # â˜…â˜…â˜… DIãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãŸã‚ã€ã“ã®é–¢æ•°ã¯UIã‹ã‚‰ç›´æ¥å‘¼ã³å‡ºã•ã‚Œã‚‹ã“ã¨ã‚’å‰æã¨ã—ã€
    # get_db_connection() ã‚’ä½¿ã†ã€‚ãƒãƒƒãƒå‡¦ç†ã‹ã‚‰ã¯å‘¼ã°ãªã„ã€‚
    conn = get_db_connection()
    if not conn:
        yield "âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
        return

    try:
        with conn.cursor() as cursor:
            # --- ã‚¹ãƒ†ãƒƒãƒ—1: æŠ€è¡“è€…æƒ…å ±ã®å–å¾— ---
            yield "ğŸ“„ å¯¾è±¡æŠ€è¡“è€…ã®å…ƒæƒ…å ±ã‚’å–å¾—ã—ã¦ã„ã¾ã™..."
            cursor.execute("SELECT source_data_json, document FROM engineers WHERE id = %s", (engineer_id,))
            engineer_record = cursor.fetchone()
            if not engineer_record or not engineer_record['source_data_json']:
                yield f"âŒ æŠ€è¡“è€…ID:{engineer_id} ã®å…ƒæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"; return
            
            engineer_doc = engineer_record['document']
            source_data = json.loads(engineer_record['source_data_json'])
            original_text = source_data.get('body', '') + "".join([f"\n\n--- æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«: {att['filename']} ---\n{att.get('content', '')}" for att in source_data.get('attachments', []) if att.get('content')])

            # --- ã‚¹ãƒ†ãƒƒãƒ—2: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æŠ½å‡º ---
            yield "ğŸ¤– æ¤œç´¢ã®æ ¸ã¨ãªã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’AIãŒæŠ½å‡ºã—ã¦ã„ã¾ã™..."
            search_keywords = []
            try:
                # â–¼â–¼â–¼ã€ã“ã“ã‹ã‚‰ãŒä¿®æ­£ã®æ ¸ã€‘â–¼â–¼â–¼
                keyword_extraction_prompt = f"""
                    ä»¥ä¸‹ã®æŠ€è¡“è€…æƒ…å ±ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€æ¡ˆä»¶ã‚’æ¢ã™ä¸Šã§é‡è¦ã¨ãªã‚‹æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æœ€å¤§10å€‹ã€ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®å˜èªãƒªã‚¹ãƒˆã¨ã—ã¦æŠœãå‡ºã—ã¦ãã ã•ã„ã€‚
                    ã€Œå¿…é ˆã€ã€Œæ­“è¿ã€ãªã©ã®æ•è©ã‚„ã€çµŒé¨“å¹´æ•°ã€å˜ä¾¡ãªã©ã®ä»˜éšæƒ…å ±ã¯å«ã‚ãšã€æŠ€è¡“åã‚„å½¹è·åãªã©ã®å˜èªã®ã¿ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
                    ä¾‹:
                    å…¥åŠ›:ã€ŒJava(SpringBoot), PHP(CakePHP)ã§ã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é–‹ç™ºçµŒé¨“ãŒè±Šå¯Œã§ã™ã€‚AWSä¸Šã§ã®ã‚¤ãƒ³ãƒ•ãƒ©æ§‹ç¯‰ã‚‚å¯¾å¿œå¯èƒ½ã§ã™ã€‚ã€
                    å‡ºåŠ›: Java, SpringBoot, PHP, CakePHP, AWS

                    å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ: ---
                    {original_text}
                    ---
                    å‡ºåŠ›:
                """
                # â–²â–²â–²ã€ä¿®æ­£ã“ã“ã¾ã§ã€‘â–²â–²â–²

                
                model = genai.GenerativeModel('models/gemini-2.5-flash-lite')
                response = model.generate_content(keyword_extraction_prompt)
                search_keywords = [kw.strip() for kw in response.text.strip().split(',') if kw.strip()]
                if not search_keywords: raise ValueError("AI did not return keywords.")
                yield f"  > æŠ½å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: `{', '.join(search_keywords)}`"
            except Exception as e:
                yield f"âš ï¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®AIæŠ½å‡ºã«å¤±æ•—({e})ã€‚å…¨æ¡ˆä»¶ã‚’å¯¾è±¡ã«å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™ã€‚"

            # --- ã‚¹ãƒ†ãƒƒãƒ—3: DBä¸€æ¬¡çµã‚Šè¾¼ã¿ ---
            yield "ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ä¸€è‡´ã™ã‚‹æ¡ˆä»¶å€™è£œã‚’DBã‹ã‚‰ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ã„ã¾ã™..."
            CANDIDATE_LIMIT = 1000
            base_query = "SELECT id FROM jobs WHERE is_hidden = 0"
            where_clauses = []
            params = []

            if search_keywords:
                or_conditions = [f"(document ILIKE %s OR project_name ILIKE %s)" for _ in search_keywords]
                where_clauses.append(f"({ ' OR '.join(or_conditions) })")
                # â˜…â˜…â˜…ã€ã“ã“ãŒä¿®æ­£ã®æ ¸ã€‘â˜…â˜…â˜…
                # ãƒ«ãƒ¼ãƒ—ã®å¤–ã§ã¾ã¨ã‚ã¦extendã™ã‚‹ã®ã§ã¯ãªãã€ãƒ«ãƒ¼ãƒ—ã®ä¸­ã§éƒ½åº¦è¿½åŠ ã™ã‚‹
                for kw in search_keywords:
                    param = f"%{kw}%"
                    params.extend([param, param])
                # â˜…â˜…â˜…ã€ä¿®æ­£ã“ã“ã¾ã§ã€‘â˜…â˜…â˜…
            
            if where_clauses:
                base_query += " AND " + " AND ".join(where_clauses)
            
            final_query = f"{base_query} ORDER BY id DESC LIMIT {CANDIDATE_LIMIT}"
            cursor.execute(final_query, tuple(params))
            
            candidate_ids = [item['id'] for item in cursor.fetchall()]

            if not candidate_ids:
                yield "âš ï¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ä¸€è‡´ã™ã‚‹æ¡ˆä»¶ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"; conn.commit(); return
            yield f"  > DBã‹ã‚‰æœ€æ–° **{len(candidate_ids)}ä»¶** (æœ€å¤§{CANDIDATE_LIMIT}ä»¶) ã®è©•ä¾¡å¯¾è±¡å€™è£œã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸã€‚"
            
            # --- ã‚¹ãƒ†ãƒƒãƒ—4: æ—¢å­˜ãƒãƒƒãƒãƒ³ã‚°ã®ã‚¯ãƒªã‚¢ã¨é€æ¬¡è©•ä¾¡ ---
            # ã“ã®ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³å†…ã§DELETEã‚’å®Ÿè¡Œ
            cursor.execute("DELETE FROM matching_results WHERE engineer_id = %s", (engineer_id,))
            yield f"ğŸ—‘ï¸ æŠ€è¡“è€…ID:{engineer_id} ã®æ—¢å­˜ãƒãƒƒãƒãƒ³ã‚°çµæœã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚"
            
            yield "ğŸ”„ çµã‚Šè¾¼ã‚“ã å€™è£œæ¡ˆä»¶ãƒªã‚¹ãƒˆã«å¯¾ã—ã¦ã€é †ã«ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™..."

            # get_items_by_ids_sync ã«ç¾åœ¨ã®æ¥ç¶šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ¸¡ã™
            candidate_records = get_items_by_ids_sync('jobs', candidate_ids, conn=conn)
            
            found_count, processed_count = 0, 0
            now_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

            for job in candidate_records:
                processed_count += 1
                yield f"  `({processed_count}/{len(candidate_records)})` æ¡ˆä»¶ **{job.get('project_name', 'N/A')}** ã¨ãƒãƒƒãƒãƒ³ã‚°ä¸­..."
                
                # â˜…â˜…â˜…ã€ã“ã“ã‹ã‚‰ãŒä¿®æ­£ã®æ ¸ã€‘â˜…â˜…â˜…
                # AIè©•ä¾¡ã®å®Ÿè¡Œå‰ã«ã€ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãƒ­ã‚°ã‚’è¨˜éŒ²ã™ã‚‹
                try:
                    cursor.execute(
                        "INSERT INTO ai_activity_log (activity_type) VALUES ('evaluation')"
                    )
                except Exception as log_err:
                    yield f"  - âš ï¸ AIã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãƒ­ã‚°ã®è¨˜éŒ²ã«å¤±æ•—: {log_err}"
                # â˜…â˜…â˜…ã€ä¿®æ­£ã“ã“ã¾ã§ã€‘â˜…â˜…â˜…

                llm_result = get_match_summary_with_llm(job['document'], engineer_doc)
                if llm_result and 'summary' in llm_result:
                    grade = llm_result.get('summary')
                    if grade in valid_ranks:
                        try:
                            # create_or_update_match_record ã«ç¾åœ¨ã®æ¥ç¶šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ¸¡ã™
                            create_or_update_match_record(job['id'], engineer_id, 0.0, grade, llm_result, conn=conn)
                            yield f"    -> **<span style='color: #28a745;'>âœ… ãƒ’ãƒƒãƒˆï¼</span>** è©•ä¾¡: **{grade}**"
                            found_count += 1
                        except Exception as db_err:
                            yield f"    -> <span style='color: #dc3545;'>âŒ DBä¿å­˜ã‚¨ãƒ©ãƒ¼</span>: {db_err}"
                else:
                    yield f"    -> <span style='color: #ffc107;'>â­ï¸ ã‚¹ã‚­ãƒƒãƒ—</span> (è©•ä¾¡: {llm_result.get('summary', 'å¤±æ•—') if llm_result else 'å¤±æ•—'})"
                if found_count >= target_count:
                    yield f"ğŸ‰ ç›®æ¨™ã® {target_count} ä»¶ã«åˆ°é”ã—ãŸãŸã‚ã€å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚"; break
            
            if found_count < target_count:
                yield f"â„¹ï¸ ã™ã¹ã¦ã®å€™è£œæ¡ˆä»¶ã®è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸã€‚(ãƒ’ãƒƒãƒˆæ•°: {found_count}ä»¶)"

        conn.commit()
        yield "âœ… ã™ã¹ã¦ã®å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚"
    except Exception as e:
        conn.rollback()
        yield f"âŒ å†è©•ä¾¡ãƒ»å†ãƒãƒƒãƒãƒ³ã‚°ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
        import traceback; yield f"```\n{traceback.format_exc()}\n```"
    finally:
        if conn:
            conn.close()






def update_job_project_name(job_id: int, new_project_name: str) -> bool:
    """
    æŒ‡å®šã•ã‚ŒãŸæ¡ˆä»¶IDã® project_name ã‚’æ›´æ–°ã™ã‚‹ã€‚
    """
    # æ–°ã—ã„æ¡ˆä»¶åãŒç©ºæ–‡å­—åˆ—ã‚„ç©ºç™½ã®ã¿ã®å ´åˆã¯æ›´æ–°ã—ãªã„
    if not new_project_name or not new_project_name.strip():
        print("æ–°ã—ã„æ¡ˆä»¶åãŒç©ºã®ãŸã‚ã€æ›´æ–°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")
        return False
        
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cur:
                cur.execute(
                    "UPDATE jobs SET project_name = %s WHERE id = %s",
                    (new_project_name.strip(), job_id)
                )
            conn.commit()
            return True
        except (Exception, psycopg2.Error) as e:
            print(f"æ¡ˆä»¶åã®æ›´æ–°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            conn.rollback()
            return False



import smtplib
from email.mime.text import MIMEText
from email.header import Header

# backend.py ã® add_or_update_auto_match_request é–¢æ•°ã‚’ã“ã¡ã‚‰ã«ç½®ãæ›ãˆã¦ãã ã•ã„

def add_or_update_auto_match_request(item_id, item_type, target_rank, email, user_id):
    """
    ã€UPSERTä¿®æ­£ç‰ˆã€‘
    è‡ªå‹•ãƒãƒƒãƒãƒ³ã‚°ä¾é ¼ã‚’ç™»éŒ²ã¾ãŸã¯æ›´æ–°ã™ã‚‹ã€‚
    - æ–°è¦ç™»éŒ²æ™‚: last_processed_id ã«ç¾åœ¨ã®æœ€æ–°IDã‚’è¨­å®šã€‚
    - æ›´æ–°æ™‚: ã‚‚ã— last_processed_id ãŒ NULL ãªã‚‰ã°ã€ç¾åœ¨ã®æœ€æ–°IDã§æ›´æ–°ã™ã‚‹ã€‚
    """
    conn = get_db_connection()
    if not conn:
        return False

    try:
        with conn.cursor() as cur:
            # --- ã‚¹ãƒ†ãƒƒãƒ—1: ç¾åœ¨ã®æœ€æ–°IDã‚’å–å¾— ---
            cur.execute("SELECT MAX(id) FROM jobs")
            current_max_job_id = (res := cur.fetchone()) and res['max'] or 0
            
            cur.execute("SELECT MAX(id) FROM engineers")
            current_max_engineer_id = (res := cur.fetchone()) and res['max'] or 0

            # --- ã‚¹ãƒ†ãƒƒãƒ—2: ä¾é ¼ã‚’DBã«ç™»éŒ²/æ›´æ–° (UPSERT) ---
            # â˜…â˜…â˜…ã€ã“ã“ã‹ã‚‰ãŒä¿®æ­£ã®æ ¸ã€‘â˜…â˜…â˜…
            sql = """
                INSERT INTO auto_matching_requests (
                    item_id, item_type, target_rank, notification_email, created_by_user_id, 
                    is_active, last_processed_job_id, last_processed_engineer_id
                )
                VALUES (%s, %s, %s, %s, %s, TRUE, %s, %s)
                ON CONFLICT (item_id, item_type) 
                DO UPDATE SET 
                    target_rank = EXCLUDED.target_rank,
                    notification_email = EXCLUDED.notification_email,
                    is_active = TRUE,
                    created_by_user_id = EXCLUDED.created_by_user_id,
                    -- æ—¢å­˜ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æ›´æ–°ã™ã‚‹éš›ã«ã€ã‚‚ã— last_processed_id ãŒ NULL ãªã‚‰ã°ã€
                    -- ç¾åœ¨ã®æœ€æ–°IDã§æ›´æ–°ã™ã‚‹ã€‚æ—¢ã«å€¤ãŒå…¥ã£ã¦ã„ã‚Œã°å¤‰æ›´ã—ãªã„ã€‚
                    last_processed_job_id = COALESCE(auto_matching_requests.last_processed_job_id, EXCLUDED.last_processed_job_id),
                    last_processed_engineer_id = COALESCE(auto_matching_requests.last_processed_engineer_id, EXCLUDED.last_processed_engineer_id)
                RETURNING last_processed_job_id, last_processed_engineer_id;
            """
            cur.execute(sql, (
                item_id, item_type, target_rank, email, user_id, 
                current_max_job_id, current_max_engineer_id
            ))
            
            # å®Ÿéš›ã«DBã«æ›¸ãè¾¼ã¾ã‚ŒãŸå€¤ã‚’å–å¾—
            result = cur.fetchone()
            actual_last_job_id = result['last_processed_job_id'] if result else 'N/A'
            actual_last_eng_id = result['last_processed_engineer_id'] if result else 'N/A'
            # â˜…â˜…â˜…ã€ä¿®æ­£ã“ã“ã¾ã§ã€‘â˜…â˜…â˜…
            
        conn.commit()
        # å®Ÿéš›ã«DBã«æ›¸ãè¾¼ã¾ã‚ŒãŸå€¤ã§ãƒ­ã‚°ã‚’å‡ºåŠ›
        print(f"âœ… Successfully UPSERTED auto-match request for {item_type} ID: {item_id}.")
        print(f"   DB value for last_processed_job_id: {actual_last_job_id}")
        print(f"   DB value for last_processed_engineer_id: {actual_last_eng_id}")
        return True
        
    except (Exception, psycopg2.Error) as e:
        print(f"âŒ Error in add_or_update_auto_match_request: {e}")
        conn.rollback()
        return False
        
    finally:
        if conn:
            conn.close()





def deactivate_auto_match_request(item_id: int, item_type: str) -> bool:
    """
    æŒ‡å®šã•ã‚ŒãŸ item_id ã¨ item_type ã«ä¸€è‡´ã™ã‚‹è‡ªå‹•ãƒãƒƒãƒãƒ³ã‚°ä¾é ¼ã‚’
    éã‚¢ã‚¯ãƒ†ã‚£ãƒ–ï¼ˆis_active = FALSEï¼‰ã«ã™ã‚‹ã€‚
    """
    # 1. å®Ÿè¡Œã™ã‚‹SQLæ–‡ã‚’å®šç¾©
    sql = """
        UPDATE auto_matching_requests 
        SET is_active = FALSE 
        WHERE item_id = %s AND item_type = %s;
    """
    
    # 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¥ç¶š
    conn = get_db_connection()
    if not conn:
        return False
        
    try:
        # 3. ã‚«ãƒ¼ã‚½ãƒ«ã‚’ä½œæˆã—ã€ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ
        with conn.cursor() as cur:
            # â˜…â˜…â˜…ã€ä¿®æ­£ç‚¹1ã€‘â˜…â˜…â˜…
            # SQLæ–‡ãŒå¿…è¦ã¨ã™ã‚‹å¼•æ•°ã®ã¿ï¼ˆ2ã¤ï¼‰ã‚’æ¸¡ã™
            cur.execute(sql, (item_id, item_type))
            
        # 4. å¤‰æ›´ã‚’ã‚³ãƒŸãƒƒãƒˆ
        conn.commit()
        
        print(f"âœ… Deactivated auto-match request for {item_type} ID: {item_id}")
        return True
        
    except (Exception, psycopg2.Error) as e:
        # â˜…â˜…â˜…ã€ä¿®æ­£ç‚¹2ã€‘â˜…â˜…â˜…
        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã“ã®é–¢æ•°å°‚ç”¨ã®ã‚‚ã®ã«ä¿®æ­£
        print(f"âŒ Error deactivating auto_match_request for {item_type} ID: {item_id}. Error: {e}")
        conn.rollback()
        return False
        
    finally:
        # 5. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’é–‰ã˜ã‚‹
        if conn:
            conn.close()


        

def get_auto_match_request(item_id: int, item_type: str) -> dict | None:
    """
    æŒ‡å®šã•ã‚ŒãŸ item_id ã¨ item_type ã«ä¸€è‡´ã™ã‚‹ã€ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªè‡ªå‹•ãƒãƒƒãƒãƒ³ã‚°ä¾é ¼ã‚’
    ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰1ä»¶ã ã‘å–å¾—ã—ã¦è¿”ã™ã€‚
    å­˜åœ¨ã—ãªã„å ´åˆã¯ None ã‚’è¿”ã™ã€‚
    """
    # 1. å–å¾—ã—ãŸã„ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ç‰¹å®šã™ã‚‹ãŸã‚ã®SQLæ–‡ã‚’å®šç¾©
    #    WHEREå¥ã§æ¡ä»¶ã‚’çµã‚Šè¾¼ã¿ã€LIMIT 1 ã§ç¢ºå®Ÿã«1è¡Œã ã‘ã‚’å–å¾—ã™ã‚‹ã‚ˆã†ã«ã™ã‚‹
    sql = """
        SELECT * 
        FROM auto_matching_requests 
        WHERE item_id = %s AND item_type = %s AND is_active = TRUE
        LIMIT 1;
    """
    
    # æˆ»ã‚Šå€¤ç”¨ã®å¤‰æ•°ã‚’ None ã§åˆæœŸåŒ–
    result_data = None

    # 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¥ç¶š
    conn = get_db_connection()
    if not conn:
        # æ¥ç¶šã«å¤±æ•—ã—ãŸå ´åˆã¯ä½•ã‚‚ã§ããªã„ã®ã§ None ã‚’è¿”ã™
        return None
        
    try:
        # 3. ã‚«ãƒ¼ã‚½ãƒ«ã‚’ä½œæˆã—ã€ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ
        with conn.cursor() as cursor:
            cursor.execute(sql, (item_id, item_type))
            
            # 4. fetchone() ã§çµæœã‚’1è¡Œã ã‘å–å¾—
            result_data = cursor.fetchone()
            
    except (Exception, psycopg2.Error) as e:
        print(f"Error in get_auto_match_request: {e}")
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã‚‚ None ã‚’è¿”ã™
        return None
    finally:
        # 5. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’é–‰ã˜ã‚‹
        if conn:
            conn.close()

    # 6. å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ï¼ˆè¾æ›¸ or Noneï¼‰ã‚’è¿”ã™
    if result_data:
        # DictCursorãŒè¿”ã™DictRowã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ã€é€šå¸¸ã®è¾æ›¸ã«å¤‰æ›ã—ã¦è¿”ã™ã¨ã‚ˆã‚Šå®‰å…¨
        return dict(result_data)
    else:
        # ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã¯ None ã‚’è¿”ã™
        return None
    

import smtplib
from email.mime.text import MIMEText
from email.header import Header
import streamlit as st # st.secrets ã‚’ä½¿ã†ãŸã‚ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒå¿…è¦

# --- ãƒ¡ãƒ¼ãƒ«é€šçŸ¥æ©Ÿèƒ½ ---
def send_email_notification(recipient_email, subject, body):
    try:
        # st.secrets.ã‚»ã‚¯ã‚·ãƒ§ãƒ³å.ã‚­ãƒ¼å ã®å½¢å¼ã§ã‚¢ã‚¯ã‚»ã‚¹
        SMTP_SERVER = st.secrets.smtp.server
        SMTP_PORT = st.secrets.smtp.port
        SMTP_USER = st.secrets.smtp.user
        SMTP_PASSWORD = st.secrets.smtp.password
        FROM_EMAIL = st.secrets.smtp.from_email
        
        msg = MIMEText(body, 'plain', 'utf-8')
        msg['Subject'] = Header(subject, 'utf-8')
        msg['From'] = FROM_EMAIL
        msg['To'] = recipient_email

        # smtplib.SMTP ã‚’ä½¿ã£ã¦ã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶š
        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:
            
            # ã‚µãƒ¼ãƒãƒ¼ã«æŒ¨æ‹¶ (EHLO) ã‚’é€ã‚Šã€STARTTLSã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ã‹ç¢ºèª
            server.ehlo()
            # STARTTLSã‚³ãƒãƒ³ãƒ‰ã§æš—å·åŒ–é€šä¿¡ã‚’é–‹å§‹
            server.starttls()
            # å†åº¦æŒ¨æ‹¶ã‚’é€ã‚‹
            server.ehlo()
            # ãƒ­ã‚°ã‚¤ãƒ³
            server.login(SMTP_USER, SMTP_PASSWORD)
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡
            server.send_message(msg)
        
        print(f"âœ… Notification email sent to {recipient_email}")
        return True
        
    except KeyError as e:
        print(f"âŒ Email sending failed: SMTP secret key '{e}' not found in [smtp] section.")
        return False
    except Exception as e:
        print(f"âŒ Email sending failed: {e}")
        return False
    

def update_auto_match_last_processed_ids(request_id, last_job_id, last_engineer_id, conn=None):

    """è‡ªå‹•ãƒãƒƒãƒãƒ³ã‚°ä¾é ¼ã®ã€æœ€å¾Œã«å‡¦ç†ã—ãŸIDã‚’æ›´æ–°ã™ã‚‹ã€‚"""
    updates = []
    params = []
    if last_job_id:
        updates.append("last_processed_job_id = %s")
        params.append(last_job_id)
    if last_engineer_id:
        updates.append("last_processed_engineer_id = %s")
        params.append(last_engineer_id)

    if not updates:
        return True # æ›´æ–°å¯¾è±¡ãŒãªã‘ã‚Œã°ä½•ã‚‚ã—ãªã„

    params.append(request_id)
    sql = f"UPDATE auto_matching_requests SET {', '.join(updates)} WHERE id = %s"
    
    with get_db_connection() as conn:
        try:
            with conn.cursor() as cur:
                cur.execute(sql, tuple(params))
            conn.commit()
            return True
        except Exception as e:
            print(f"Error updating last processed IDs: {e}")
            conn.rollback()
            return False
        

def create_or_update_match_record(job_id, engineer_id, score, grade, llm_result, conn=None):

    """
    matching_resultsãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æŒ¿å…¥ã¾ãŸã¯æ›´æ–°ã™ã‚‹ï¼ˆUPSERTï¼‰ã€‚
    æˆåŠŸã—ãŸå ´åˆã¯ã€ä½œæˆ/æ›´æ–°ã•ã‚ŒãŸãƒ¬ã‚³ãƒ¼ãƒ‰ã®IDã‚’è¿”ã™ã€‚
    """
    # llm_resultã‹ã‚‰ãƒã‚¸ãƒ†ã‚£ãƒ–/æ‡¸å¿µç‚¹ã‚’JSONæ–‡å­—åˆ—ã«å¤‰æ›
    positive_points = json.dumps(llm_result.get('positive_points', []), ensure_ascii=False)
    concern_points = json.dumps(llm_result.get('concern_points', []), ensure_ascii=False)
    now_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    # ON CONFLICTå¥ã‚’ä½¿ã£ãŸUPSERTæ–‡
    sql = """
        INSERT INTO matching_results (
            job_id, engineer_id, score, created_at, grade, 
            positive_points, concern_points, status
        )
        VALUES (%s, %s, %s, %s, %s, %s, %s, 'æ–°è¦')
        ON CONFLICT (job_id, engineer_id) 
        DO UPDATE SET
            score = EXCLUDED.score,
            grade = EXCLUDED.grade,
            positive_points = EXCLUDED.positive_points,
            concern_points = EXCLUDED.concern_points,
            created_at = EXCLUDED.created_at, -- æ›´æ–°æ—¥æ™‚ã‚‚æœ€æ–°ã«ã™ã‚‹
            status = 'æ–°è¦' -- å†è©•ä¾¡æ™‚ã¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆ
        RETURNING id;
    """
    
    conn = get_db_connection()
    if not conn:
        return None
        
    try:
        with conn.cursor() as cur:
            cur.execute(sql, (
                job_id, engineer_id, score, now_str, grade, 
                positive_points, concern_points
            ))
            result = cur.fetchone()
        conn.commit()
        
        # RETURNINGã§è¿”ã•ã‚ŒãŸIDã‚’è¿”ã™
        return result['id'] if result else None
        
    except (Exception, psycopg2.Error) as e:
        print(f"Error in create_or_update_match_record: {e}")
        conn.rollback()
        return None
        
    finally:
        if conn:
            conn.close()





def clear_matches_for_job(job_id: int, conn=None) -> bool:
    """æŒ‡å®šã•ã‚ŒãŸæ¡ˆä»¶IDã«ç´ã¥ãå…¨ã¦ã®ãƒãƒƒãƒãƒ³ã‚°çµæœã‚’å‰Šé™¤ã™ã‚‹ã€‚"""
    if not job_id:
        return False
    
    should_close_conn = False
    if conn is None:
        conn = get_db_connection()
        if not conn: return False
        should_close_conn = True
        
    try:
        with conn.cursor() as cur:
            cur.execute("DELETE FROM matching_results WHERE job_id = %s", (job_id,))
            print(f"âœ… Cleared {cur.rowcount} matches for job_id: {job_id}")
        conn.commit()
        return True
    except Exception as e:
        print(f"âŒ Error clearing matches for job_id {job_id}: {e}")
        conn.rollback()
        return False
    finally:
        if should_close_conn and conn:
            conn.close()


def clear_matches_for_engineer(engineer_id: int, conn=None) -> bool:
    """æŒ‡å®šã•ã‚ŒãŸæŠ€è¡“è€…IDã«ç´ã¥ãå…¨ã¦ã®ãƒãƒƒãƒãƒ³ã‚°çµæœã‚’å‰Šé™¤ã™ã‚‹ã€‚"""
    if not engineer_id:
        return False
    
    should_close_conn = False
    if conn is None:
        conn = get_db_connection()
        if not conn: return False
        should_close_conn = True
        
    try:
        with conn.cursor() as cur:
            cur.execute("DELETE FROM matching_results WHERE engineer_id = %s", (engineer_id,))
            print(f"âœ… Cleared {cur.rowcount} matches for engineer_id: {engineer_id}")
        conn.commit()
        return True
    except Exception as e:
        print(f"âŒ Error clearing matches for engineer_id {engineer_id}: {e}")
        conn.rollback()
        return False
    finally:
        if should_close_conn and conn:
            conn.close()

# backend.py ã® get_live_dashboard_data é–¢æ•°ã‚’ã“ã¡ã‚‰ã«ç½®ãæ›ãˆã¦ãã ã•ã„


@st.cache_data(ttl=10)
def get_live_dashboard_data():
    """
    ã€ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³å¯¾å¿œãƒ»å®Œå…¨ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆã€‘
    çµŒå–¶è€…å‘ã‘ãƒ©ã‚¤ãƒ–ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã¾ã¨ã‚ã¦å–å¾—ã™ã‚‹ã€‚
    """
    data = {
        "jobs_today": 0, "engineers_today": 0, "processed_items_today": 0,
        "new_matches_today": 0, "adopted_count_today": 0,
        "ai_activity_counts": {}, "funnel_data": {}, "proposal_count_total": 0,
        "active_auto_request_count": 0, "active_auto_requests": [],
        "top_performers": [], "recent_matches": [], "live_log_feed": []
    }
    
    conn = get_db_connection()
    if not conn:
        return data

    try:
        with conn.cursor() as cur:

            target_tz = pytz.timezone('America/Los_Angeles') #Asia/Tokyo
            now_in_target_tz = datetime.now(target_tz)
            
            # ã€Œéå»24æ™‚é–“å‰ã€ã®æ™‚åˆ»ã‚’è¨ˆç®—
            twenty_four_hours_ago = now_in_target_tz - timedelta(hours=24)
            
            # ã€Œä»Šæœˆã®å§‹ã¾ã‚Šã€ã¯ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã§ä½¿ã†ãŸã‚æ®‹ã—ã¦ãŠã
            this_month_start = now_in_target_tz.replace(day=1, hour=0, minute=0, second=0, microsecond=0)


            # éå»24æ™‚é–“ä»¥å†…ã«ç™»éŒ²ã•ã‚ŒãŸæ¡ˆä»¶ãƒ»æŠ€è¡“è€…ã®æ•°
            cur.execute("SELECT COUNT(*) FROM jobs WHERE is_hidden = 0 AND created_at >= %s", (twenty_four_hours_ago,))
            data["jobs_today"] = cur.fetchone()['count']
            
            cur.execute("SELECT COUNT(*) FROM engineers WHERE is_hidden = 0 AND created_at >= %s", (twenty_four_hours_ago,))
            data["engineers_today"] = cur.fetchone()['count']
            
            # ãƒ©ãƒ™ãƒ«ã¯ã€Œæœ¬æ—¥ã€ã®ã¾ã¾ã ãŒã€ä¸­èº«ã¯éå»24æ™‚é–“ã®é›†è¨ˆã«ãªã‚‹
            data["processed_items_today"] = data["jobs_today"] + data["engineers_today"]

            # éå»24æ™‚é–“ä»¥å†…ã«ä½œæˆã•ã‚ŒãŸãƒãƒƒãƒãƒ³ã‚°ã®ç·æ•°
            cur.execute("SELECT COUNT(*) FROM matching_results WHERE is_hidden = 0 AND created_at >= %s", (twenty_four_hours_ago,))
            data["new_matches_today"] = cur.fetchone()['count']

            # éå»24æ™‚é–“ä»¥å†…ã«ã€Œæ¡ç”¨ã€ã«ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å¤‰æ›´ã•ã‚ŒãŸä»¶æ•°
            cur.execute("SELECT COUNT(*) FROM matching_results WHERE status = 'æ¡ç”¨' AND status_updated_at >= %s", (twenty_four_hours_ago,))
            data["adopted_count_today"] = cur.fetchone()['count']

            # éå»24æ™‚é–“ã®AIã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£
            cur.execute("SELECT activity_type, COUNT(*) as count FROM ai_activity_log WHERE created_at >= %s GROUP BY activity_type", (twenty_four_hours_ago,))
            for row in cur.fetchall():
                data["ai_activity_counts"][row['activity_type']] = row['count']

            # --- 3. ãã®ä»–ã®é›†è¨ˆ ---
            # ãƒ•ã‚¡ãƒãƒ«ãƒãƒ£ãƒ¼ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿
            cur.execute("SELECT status, COUNT(*) as count FROM matching_results WHERE is_hidden = 0 GROUP BY status")
            for row in cur.fetchall():
                data['funnel_data'][row['status']] = row['count']

            # æ‹…å½“è€…åˆ¥ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆä»Šæœˆã®ã€Œæ¡ç”¨ã€ä»¶æ•°ï¼‰
            cur.execute("""
                SELECT u.username, COUNT(r.id) as adoption_count FROM matching_results r
                JOIN jobs j ON r.job_id = j.id JOIN users u ON j.assigned_user_id = u.id
                WHERE r.status = 'æ¡ç”¨' AND r.created_at >= %s GROUP BY u.username
                ORDER BY adoption_count DESC LIMIT 5
            """, (this_month_start,))
            data["top_performers"] = [dict(row) for row in cur.fetchall()]

            # ææ¡ˆä¸­ã®ç·ä»¶æ•°
            cur.execute("SELECT COUNT(*) FROM matching_results WHERE status IN ('ææ¡ˆæº–å‚™ä¸­', 'ææ¡ˆä¸­')")
            data["proposal_count_total"] = cur.fetchone()['count']
            
            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªè‡ªå‹•ãƒãƒƒãƒãƒ³ã‚°ä¾é ¼
            cur.execute("SELECT COUNT(*) FROM auto_matching_requests WHERE is_active = TRUE")
            data["active_auto_request_count"] = cur.fetchone()['count']
            cur.execute("""
                (SELECT req.*, j.project_name as item_name, j.document, COALESCE(mc.match_count, 0) as match_count FROM auto_matching_requests req JOIN jobs j ON req.item_id = j.id AND req.item_type = 'job' LEFT JOIN (SELECT job_id, COUNT(*) as match_count FROM matching_results WHERE is_hidden = 0 GROUP BY job_id) mc ON j.id = mc.job_id WHERE req.is_active = TRUE)
                UNION ALL
                (SELECT req.*, e.name as item_name, e.document, COALESCE(mc.match_count, 0) as match_count FROM auto_matching_requests req JOIN engineers e ON req.item_id = e.id AND req.item_type = 'engineer' LEFT JOIN (SELECT engineer_id, COUNT(*) as match_count FROM matching_results WHERE is_hidden = 0 GROUP BY engineer_id) mc ON e.id = mc.engineer_id WHERE req.is_active = TRUE)
                ORDER BY created_at DESC LIMIT 5
            """)
            data["active_auto_requests"] = [dict(row) for row in cur.fetchall()]

            # â–¼â–¼â–¼ã€ã“ã“ã‹ã‚‰ãŒä¿®æ­£ã®æ ¸ã€‘â–¼â–¼â–¼
            # 10. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´»å‹•ãƒ­ã‚°
            cur.execute("""
                SELECT * FROM (
                    SELECT 
                        'processing' as log_type, 
                        j.project_name, 
                        e.name as engineer_name, 
                        r.grade, 
                        r.created_at, 
                        j.id as job_id, 
                        e.id as engineer_id,
                        r.id as result_id  -- â˜…â˜…â˜… ãƒãƒƒãƒãƒ³ã‚°çµæœIDã‚’è¿½åŠ 
                    FROM matching_results r 
                    JOIN jobs j ON r.job_id = j.id 
                    JOIN engineers e ON r.engineer_id = e.id
                    WHERE r.is_hidden = 0

                    UNION ALL

                    SELECT 
                        'input' as log_type, 
                        project_name, 
                        NULL as engineer_name, 
                        NULL as grade, 
                        created_at, 
                        id as job_id, 
                        NULL as engineer_id,
                        NULL as result_id -- â˜…â˜…â˜… å‹ã‚’åˆã‚ã›ã‚‹ãŸã‚NULLã‚’è¿½åŠ 

                    FROM jobs WHERE is_hidden = 0

                    UNION ALL

                    SELECT 
                        'input' as log_type, 
                        NULL as project_name, 
                        name as engineer_name, 
                        NULL as grade, 
                        created_at, 
                        NULL as job_id, 
                        id as engineer_id,
                        NULL as result_id -- â˜…â˜…â˜… å‹ã‚’åˆã‚ã›ã‚‹ãŸã‚NULLã‚’è¿½åŠ 
                        
                    FROM engineers WHERE is_hidden = 0
                ) AS combined_logs
                ORDER BY created_at DESC
                LIMIT 10;
            """)
            data["live_log_feed"] = [dict(row) for row in cur.fetchall()]
            # â–²â–²â–²ã€ä¿®æ­£ã“ã“ã¾ã§ã€‘â–²â–²â–²
                    






    except Exception as e:
        print(f"Error in get_live_dashboard_data: {e}")
    finally:
        if conn:
            conn.close()
            
    return data





def generate_text(prompt: str, max_tokens: int = 150) -> str:
    """
    Gemini APIã‚’å‘¼ã³å‡ºã—ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã™ã‚‹å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°ã€‚
    """

    # --- Gemini APIã®åˆæœŸè¨­å®š ---
    # ã“ã®éƒ¨åˆ†ã¯ãƒ•ã‚¡ã‚¤ãƒ«ã®å…ˆé ­ã‚„é©åˆ‡ãªå ´æ‰€ã§è¡Œã†
    try:
        
        gemini_model = genai.GenerativeModel('models/gemini-2.5-flash-lite')

        response = gemini_model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                max_output_tokens=max_tokens,
                temperature=0.7 # å‰µé€ æ€§ã®åº¦åˆã„
            )
        )
        return response.text.strip()
    
    except Exception as e:
        # st.secretsãŒèª­ã¿è¾¼ã‚ãªã„å ´åˆãªã©ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        print(f"Gemini APIã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        gemini_model = None




def get_current_time_str_in_jst(format_str: str = '%Y-%m-%d %H:%M:%S') -> str:
    """
    ç¾åœ¨ã®æ™‚åˆ»ã‚’JSTï¼ˆæ—¥æœ¬æ¨™æº–æ™‚ï¼‰ã§å–å¾—ã—ã€æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®æ–‡å­—åˆ—ã¨ã—ã¦è¿”ã™ã€‚

    Args:
        format_str (str, optional): 
            æ™‚åˆ»ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ–‡å­—åˆ—ã€‚
            ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ '%Y-%m-%d %H:%M:%S'ã€‚

    Returns:
        str: ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸJSTæ™‚åˆ»ã®æ–‡å­—åˆ—ã€‚
    """
    try:
        # 1. æ±äº¬ã®ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—
        jst_tz = pytz.timezone('Asia/Tokyo')
        
        # 2. æ±äº¬æ™‚é–“ã§ã®ç¾åœ¨æ™‚åˆ»ã‚’å–å¾—
        now_in_jst = datetime.now(jst_tz)
        
        # 3. æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§æ–‡å­—åˆ—ã«å¤‰æ›ã—ã¦è¿”ã™
        return now_in_jst.strftime(format_str)
        
    except Exception as e:
        # pytzãŒè¦‹ã¤ã‹ã‚‰ãªã„ãªã©ã®ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        print(f"ERROR in get_current_time_str_in_jst: {e}")
        # ã¨ã‚Šã‚ãˆãšã€ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æƒ…å ±ãªã—ã®ç¾åœ¨æ™‚åˆ»ã‚’è¿”ã™
        return datetime.now().strftime(format_str)
    

