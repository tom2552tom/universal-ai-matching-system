import streamlit as st
import sqlite3
import faiss
from sentence_transformers import SentenceTransformer
import numpy as np
import os
import google.generativeai as genai
import json
import shutil
from datetime import datetime
import pandas as pd

# --- 1. åˆæœŸè¨­å®šã¨å®šæ•° ---
# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦APIã‚­ãƒ¼ã‚’å®šç¾©
API_KEY = "AIzaSyA4Vv_MWpMZ-2y8xGslYQJ9yvcBY9Pc-VA" # â˜…â˜…â˜… ã‚ãªãŸã®Google AI APIã‚­ãƒ¼ã«æ›¸ãæ›ãˆã¦ãã ã•ã„ â˜…â˜…â˜…

# èµ·å‹•æ™‚ã«ä¸€åº¦ã ã‘APIã‚­ãƒ¼ã®æœ‰åŠ¹æ€§ã‚’ãƒã‚§ãƒƒã‚¯
try:
    genai.configure(api_key=API_KEY)
except Exception as e:
    st.error(f"èµ·å‹•æ™‚ã®APIã‚­ãƒ¼è¨­å®šã«å•é¡ŒãŒã‚ã‚Šã¾ã™: {e}")
    st.stop()

DB_FILE = "backend_system.db"
JOB_INDEX_FILE = "backend_job_index.faiss"
ENGINEER_INDEX_FILE = "backend_engineer_index.faiss"
MODEL_NAME = 'intfloat/multilingual-e5-large'
MAILBOX_DIR = "mailbox"
PROCESSED_DIR = "processed"

# --- 2. ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã¨DBåˆæœŸåŒ– ---
@st.cache_resource
def load_embedding_model():
    return SentenceTransformer(MODEL_NAME)

def init_database():
    os.makedirs(MAILBOX_DIR, exist_ok=True)
    os.makedirs(PROCESSED_DIR, exist_ok=True)
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()
    cursor.execute('CREATE TABLE IF NOT EXISTS jobs (id INTEGER PRIMARY KEY AUTOINCREMENT, document TEXT NOT NULL, created_at TEXT)')
    cursor.execute('CREATE TABLE IF NOT EXISTS engineers (id INTEGER PRIMARY KEY AUTOINCREMENT, document TEXT NOT NULL, created_at TEXT)')
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS matching_results (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        job_id INTEGER,
        engineer_id INTEGER,
        score REAL,
        created_at TEXT,
        FOREIGN KEY (job_id) REFERENCES jobs (id),
        FOREIGN KEY (engineer_id) REFERENCES engineers (id)
    )''')
    conn.commit()
    conn.close()

def get_db_connection():
    conn = sqlite3.connect(DB_FILE)
    conn.row_factory = sqlite3.Row
    return conn

# --- 3. LLMé–¢é€£ã®é–¢æ•° ---

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡åŠ¹åŒ–
def split_text_with_llm(text_content):
    # é–¢æ•°å†…ã§APIã‚­ãƒ¼ã‚’å†è¨­å®š
    genai.configure(api_key=API_KEY)
    model = genai.GenerativeModel('models/gemini-2.5-pro')
    prompt = f"""
ã‚ãªãŸã¯ã€ITæ¥­ç•Œã®æ¡ˆä»¶æƒ…å ±ã¨æŠ€è¡“è€…æƒ…å ±ã‚’æ•´ç†ã™ã‚‹å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€ã€Œæ¡ˆä»¶æƒ…å ±ã€ã¨ã€ŒæŠ€è¡“è€…æƒ…å ±ã€ã‚’ãã‚Œãã‚Œå€‹åˆ¥ã®å˜ä½ã§æŠ½å‡ºã—ã€JSONå½¢å¼ã®ãƒªã‚¹ãƒˆã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
# åˆ¶ç´„æ¡ä»¶
- æ¡ˆä»¶æƒ…å ±ã«ã¯ã€ãƒã‚¸ã‚·ãƒ§ãƒ³ã€æ¥­å‹™å†…å®¹ã€ã‚¹ã‚­ãƒ«ãªã©ã®æƒ…å ±ã‚’å«ã‚ã¦ãã ã•ã„ã€‚
- æŠ€è¡“è€…æƒ…å ±ã«ã¯ã€æ°åã€ã‚¹ã‚­ãƒ«ã€çµŒé¨“ãªã©ã®æƒ…å ±ã‚’å«ã‚ã¦ãã ã•ã„ã€‚
- å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã«å«ã¾ã‚Œãªã„æƒ…å ±ã¯å‰µä½œã—ãªã„ã§ãã ã•ã„ã€‚
- ãã‚Œãã‚Œæ˜ç¢ºã«åˆ†å‰²ã—ã€ä»–ã®æ¡ˆä»¶ã‚„æŠ€è¡“è€…ã®æƒ…å ±ãŒæ··ã–ã‚‰ãªã„ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚
- è©²å½“ã™ã‚‹æƒ…å ±ãŒãªã„å ´åˆã¯ã€ç©ºã®ãƒªã‚¹ãƒˆ `[]` ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
# å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
---
{text_content}
---
# å‡ºåŠ›å½¢å¼ (JSONã®ã¿ã‚’å‡ºåŠ›ã™ã‚‹ã“ã¨)
{{
  "jobs": [ {{ "document": "<æŠ½å‡ºã—ãŸæ¡ˆä»¶1ã®æƒ…å ±>" }} ],
  "engineers": [ {{ "document": "<æŠ½å‡ºã—ãŸæŠ€è¡“è€…1ã®æƒ…å ±>" }} ]
}}
"""
    generation_config = {"response_mime_type": "application/json"}
    safety_settings = {
        'HARM_CATEGORY_HARASSMENT': 'BLOCK_NONE', 'HARM_CATEGORY_HATE_SPEECH': 'BLOCK_NONE',
        'HARM_CATEGORY_SEXUALLY_EXPLICIT': 'BLOCK_NONE', 'HARM_CATEGORY_DANGEROUS_CONTENT': 'BLOCK_NONE',
    }
    try:
        with st.spinner("LLMãŒãƒ†ã‚­ã‚¹ãƒˆã‚’è§£æãƒ»åˆ†å‰²ä¸­..."):
            response = model.generate_content(prompt, generation_config=generation_config, safety_settings=safety_settings)
        return json.loads(response.text)
    except Exception as e:
        st.error(f"LLMã«ã‚ˆã‚‹åˆ†å‰²ã«å¤±æ•—: {e}")
        return None

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡åŠ¹åŒ–
def get_match_summary_with_llm(job_doc, engineer_doc):
    # é–¢æ•°å†…ã§APIã‚­ãƒ¼ã‚’å†è¨­å®š
    genai.configure(api_key=API_KEY)
    model = genai.GenerativeModel('models/gemini-2.5-pro')
    prompt = f"""
ã‚ãªãŸã¯å„ªç§€ãªITæ¡ç”¨æ‹…å½“è€…ã§ã™ã€‚ä»¥ä¸‹ã®ã€æ¡ˆä»¶æƒ…å ±ã€‘ã¨ã€æŠ€è¡“è€…æƒ…å ±ã€‘ã‚’æ¯”è¼ƒã—ã€ãªãœã“ã®äºŒè€…ãŒãƒãƒƒãƒã™ã‚‹ã®ã‹ã€ã‚ã‚‹ã„ã¯ã—ãªã„ã®ã‹ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚
çµæœã¯å¿…ãšæŒ‡å®šã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
# åˆ†æã®ãƒã‚¤ãƒ³ãƒˆ
- positive_points: æ¡ˆä»¶ã®å¿…é ˆã‚¹ã‚­ãƒ«ã‚„æ¥­å‹™å†…å®¹ã¨ã€æŠ€è¡“è€…ã®ã‚¹ã‚­ãƒ«ã‚„çµŒé¨“ãŒåˆè‡´ã™ã‚‹ç‚¹ã‚’å…·ä½“çš„ã«æŒ™ã’ã¦ãã ã•ã„ã€‚
- concern_points: æ¡ˆä»¶ãŒè¦æ±‚ã—ã¦ã„ã‚‹ãŒæŠ€è¡“è€…ã®çµŒæ­´ã‹ã‚‰ã¯èª­ã¿å–ã‚Œãªã„ã‚¹ã‚­ãƒ«ã‚„ã€çµŒé¨“å¹´æ•°ã®ã‚®ãƒ£ãƒƒãƒ—ãªã©ã€æ‡¸å¿µã•ã‚Œã‚‹ç‚¹ã‚’æŒ™ã’ã¦ãã ã•ã„ã€‚
- summary: ä¸Šè¨˜ã‚’ç·åˆçš„ã«åˆ¤æ–­ã—ã€æ¡ç”¨æ‹…å½“è€…å‘ã‘ã®ç°¡æ½”ãªã‚µãƒãƒªãƒ¼ã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
# ã€æ¡ˆä»¶æƒ…å ±ã€‘
{job_doc}
# ã€æŠ€è¡“è€…æƒ…å ±ã€‘
{engineer_doc}
# å‡ºåŠ›å½¢å¼ (JSONã®ã¿)
{{
  "positive_points": ["<åˆè‡´ã™ã‚‹ç‚¹1>", "<åˆè‡´ã™ã‚‹ç‚¹2>"],
  "concern_points": ["<æ‡¸å¿µç‚¹1>", "<æ‡¸å¿µç‚¹2>"],
  "summary": "<ç·åˆè©•ä¾¡ã‚µãƒãƒªãƒ¼>"
}}
"""
    generation_config = {"response_mime_type": "application/json"}
    safety_settings = {
        'HARM_CATEGORY_HARASSMENT': 'BLOCK_NONE', 'HARM_CATEGORY_HATE_SPEECH': 'BLOCK_NONE',
        'HARM_CATEGORY_SEXUALLY_EXPLICIT': 'BLOCK_NONE', 'HARM_CATEGORY_DANGEROUS_CONTENT': 'BLOCK_NONE',
    }
    try:
        with st.spinner("AIãŒãƒãƒƒãƒãƒ³ã‚°æ ¹æ‹ ã‚’åˆ†æä¸­..."):
            response = model.generate_content(prompt, generation_config=generation_config, safety_settings=safety_settings)
        return json.loads(response.text)
    except Exception as e:
        st.error(f"æ ¹æ‹ ã®åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return None

# --- 4. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨æ¤œç´¢ã®é–¢æ•° ---
def update_index(index_path, new_items):
    embedding_model = load_embedding_model()
    dimension = embedding_model.get_sentence_embedding_dimension()
    if os.path.exists(index_path):
        index = faiss.read_index(index_path)
    else:
        index = faiss.IndexIDMap(faiss.IndexFlatL2(dimension))
    ids = np.array([item['id'] for item in new_items], dtype=np.int64)
    texts = [item['document'] for item in new_items]
    embeddings = embedding_model.encode(texts, normalize_embeddings=True, show_progress_bar=False)
    index.add_with_ids(embeddings, ids)
    faiss.write_index(index, index_path)

def search(query_text, index_path, top_k=5):
    if not os.path.exists(index_path): return [], []
    index = faiss.read_index(index_path)
    if index.ntotal == 0: return [], []
    embedding_model = load_embedding_model()
    query_vector = embedding_model.encode(query_text, normalize_embeddings=True).reshape(1, -1)
    distances, ids = index.search(query_vector, min(top_k, index.ntotal))
    return distances[0].tolist(), ids[0].tolist()

# --- 5. ãƒ¡ã‚¤ãƒ³ã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰å‡¦ç†é–¢æ•° ---
def process_new_mails():
    conn = get_db_connection()
    cursor = conn.cursor()
    now_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    existing_job_ids = {row['id'] for row in conn.execute('SELECT id FROM jobs').fetchall()}
    existing_engineer_ids = {row['id'] for row in conn.execute('SELECT id FROM engineers').fetchall()}

    unread_mails = [f for f in os.listdir(MAILBOX_DIR) if f.endswith('.txt')]
    if not unread_mails:
        st.toast("æ–°ã—ã„ãƒ¡ãƒ¼ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return True
    st.toast(f"{len(unread_mails)}ä»¶ã®æ–°ã—ã„ãƒ¡ãƒ¼ãƒ«ã‚’å‡¦ç†ã—ã¾ã™ã€‚")
    all_new_jobs, all_new_engineers = [], []
    for mail_file in unread_mails:
        mail_path = os.path.join(MAILBOX_DIR, mail_file)
        with open(mail_path, 'r', encoding='utf-8') as f: content = f.read()
        parsed_data = split_text_with_llm(content)
        if not parsed_data:
            st.warning(f"ãƒ•ã‚¡ã‚¤ãƒ« '{mail_file}' ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãŸã‚ã€ä¸­æ–­ã—ã¾ã—ãŸã€‚")
            return False

        new_jobs = parsed_data.get("jobs", [])
        new_engineers = parsed_data.get("engineers", [])
        
        # â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
        # â˜…ã€ä¿®æ­£ç‚¹ã€‘'document'ã®ä¸­èº«ãŒè¾æ›¸ã‹æ–‡å­—åˆ—ã‹ã‚’åˆ¤å®šã—ã¦å‡¦ç†ã™ã‚‹ â˜…
        # â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
        if new_jobs:
            for job in new_jobs:
                doc_content = job.get('document')
                # doc_contentãŒè¾æ›¸ã®å ´åˆã€è¦‹ã‚„ã™ã„æ–‡å­—åˆ—ã«å¤‰æ›ã™ã‚‹
                if isinstance(doc_content, dict):
                    doc_str = "\n".join([f"{k}ï¼š{v}" for k, v in doc_content.items()])
                else:
                    doc_str = str(doc_content) # æ–‡å­—åˆ—ãªã‚‰ãã®ã¾ã¾
                
                cursor.execute('INSERT INTO jobs (document, created_at) VALUES (?, ?)', (doc_str, now_str))
                job['id'] = cursor.lastrowid
                job['document'] = doc_str # å¾Œç¶šå‡¦ç†ã®ãŸã‚ã«æ–‡å­—åˆ—ã§ä¸Šæ›¸ã
                all_new_jobs.append(job)

        if new_engineers:
            for engineer in new_engineers:
                doc_content = engineer.get('document')
                # doc_contentãŒè¾æ›¸ã®å ´åˆã€è¦‹ã‚„ã™ã„æ–‡å­—åˆ—ã«å¤‰æ›ã™ã‚‹
                if isinstance(doc_content, dict):
                    doc_str = "\n".join([f"{k}ï¼š{v}" for k, v in doc_content.items()])
                else:
                    doc_str = str(doc_content) # æ–‡å­—åˆ—ãªã‚‰ãã®ã¾ã¾

                cursor.execute('INSERT INTO engineers (document, created_at) VALUES (?, ?)', (doc_str, now_str))
                engineer['id'] = cursor.lastrowid
                engineer['document'] = doc_str # å¾Œç¶šå‡¦ç†ã®ãŸã‚ã«æ–‡å­—åˆ—ã§ä¸Šæ›¸ã
                all_new_engineers.append(engineer)
        
        conn.commit()
        shutil.move(mail_path, os.path.join(PROCESSED_DIR, mail_file))

    # (ä»¥é™ã®ãƒãƒƒãƒãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯ã¯å¤‰æ›´ãªã—)
    if all_new_jobs: update_index(JOB_INDEX_FILE, all_new_jobs)
    if all_new_engineers: update_index(ENGINEER_INDEX_FILE, all_new_engineers)
    
    # ... (é‡è¤‡æ’é™¤ãƒãƒƒãƒãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯) ...
    if all_new_jobs and existing_engineer_ids:
        for job in all_new_jobs:
            distances, ids = search(job['document'], ENGINEER_INDEX_FILE)
            for dist, eng_id in zip(distances, ids):
                if eng_id != -1 and eng_id in existing_engineer_ids:
                    score = (1 - dist) * 100; cursor.execute('INSERT INTO matching_results (job_id, engineer_id, score, created_at) VALUES (?, ?, ?, ?)',(job['id'], int(eng_id), score, now_str))
    if all_new_engineers:
        for engineer in all_new_engineers:
            distances, ids = search(engineer['document'], JOB_INDEX_FILE)
            for dist, job_id in zip(distances, ids):
                if job_id != -1:
                    score = (1 - dist) * 100; cursor.execute('INSERT INTO matching_results (job_id, engineer_id, score, created_at) VALUES (?, ?, ?, ?)', (int(job_id), engineer['id'], score, now_str))

    conn.commit()
    conn.close()
    st.success("ãƒ¡ãƒ¼ãƒ«å‡¦ç†ã¨ãƒãƒƒãƒãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    return True


# --- 6. Streamlit UIã®å®šç¾© ---
st.set_page_config(page_title="AIãƒãƒƒãƒãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ", layout="wide")
st.markdown('<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css">', unsafe_allow_html=True)

load_embedding_model()
init_database()

st.title("AIãƒãƒƒãƒãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ  ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
if st.button("ğŸ“§ æ–°ç€ãƒ¡ãƒ¼ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ãƒãƒƒãƒãƒ³ã‚°å®Ÿè¡Œ", type="primary"):
    with st.spinner("å‡¦ç†ã‚’å®Ÿè¡Œä¸­..."):
        success = process_new_mails()
    if success:
        st.rerun()

st.divider()
st.header("æœ€æ–°ãƒãƒƒãƒãƒ³ã‚°çµæœä¸€è¦§")

conn = get_db_connection()
results = conn.execute('''
    SELECT r.id as res_id, r.job_id, j.document as job_doc, r.engineer_id, e.document as eng_doc, r.score, r.created_at
    FROM matching_results r JOIN jobs j ON r.job_id = j.id JOIN engineers e ON r.engineer_id = e.id
    ORDER BY r.created_at DESC, r.score DESC LIMIT 50
''').fetchall()
conn.close()

if not results:
    st.info("ã¾ã ãƒãƒƒãƒãƒ³ã‚°çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã€Œæ–°ç€ãƒ¡ãƒ¼ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
else:
    for res in results:
        score = res['score']
        if score > 75: border_class = "border-success"
        elif score > 60: border_class = "border-primary"
        else: border_class = "border-secondary"
        st.markdown(f'<div class="card {border_class} mb-3">', unsafe_allow_html=True)
        header_html = f'<div class="card-header d-flex justify-content-between align-items-center"><span>ãƒãƒƒãƒãƒ³ã‚°æ—¥æ™‚: {res["created_at"]}</span>'
        if score > 75: header_html += f'<span class="badge bg-success">é«˜ãƒãƒƒãƒ</span>'
        header_html += '</div>'
        st.markdown(header_html, unsafe_allow_html=True)
        st.markdown('<div class="card-body">', unsafe_allow_html=True)
        col1, col2 = st.columns(2)
        with col1:
            st.markdown(f"<h6>æ¡ˆä»¶ (ID: {res['job_id']})</h6>", unsafe_allow_html=True)
            st.text_area("", res['job_doc'], height=150, disabled=True, key=f"job_{res['res_id']}")
        with col2:
            st.markdown(f"<h6>æŠ€è¡“è€… (ID: {res['engineer_id']})</h6>", unsafe_allow_html=True)
            st.text_area("", res['eng_doc'], height=150, disabled=True, key=f"eng_{res['res_id']}")
        st.progress(int(score), text=f"ãƒãƒƒãƒåº¦: {score:.1f}%")
        button_key = f"summary_btn_{res['res_id']}"
        if st.button("AIã«ã‚ˆã‚‹è©•ä¾¡ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º", key=button_key):
            summary_data = get_match_summary_with_llm(res['job_doc'], res['eng_doc'])
            if summary_data:
                st.markdown("---")
                st.markdown("<h5><i class='bi bi-search-heart'></i> AIè©•ä¾¡ã‚µãƒãƒªãƒ¼</h5>", unsafe_allow_html=True)
                st.info(f"**ç·åˆè©•ä¾¡:** {summary_data.get('summary', 'N/A')}")
                st.markdown("<h6>ãƒã‚¸ãƒ†ã‚£ãƒ–ãªç‚¹</h6>", unsafe_allow_html=True)
                for point in summary_data.get('positive_points', []):
                    st.markdown(f"- {point}")
                st.markdown("<h6 style='margin-top: 1rem;'>æ‡¸å¿µç‚¹</h6>", unsafe_allow_html=True)
                concern_points = summary_data.get('concern_points', [])
                if concern_points:
                    for point in concern_points:
                        st.markdown(f"- {point}")
                else:
                    st.caption("ç‰¹ã«æ‡¸å¿µç‚¹ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        st.markdown('</div></div>', unsafe_allow_html=True)