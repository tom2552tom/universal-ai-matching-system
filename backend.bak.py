import streamlit as st
import sqlite3
import faiss
from sentence_transformers import SentenceTransformer
import numpy as np
import os
import google.generativeai as genai
import json
from datetime import datetime
import imaplib
import email
from email.header import decode_header
import io
import contextlib
import toml

# --- 1. åˆæœŸè¨­å®šã¨å®šæ•° ---
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=API_KEY)
except (KeyError, Exception):
    st.error("`secrets.toml` ã« `GOOGLE_API_KEY` ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    st.stop()

DB_FILE = "backend_system.db"
JOB_INDEX_FILE = "backend_job_index.faiss"
ENGINEER_INDEX_FILE = "backend_engineer_index.faiss"
MODEL_NAME = 'intfloat/multilingual-e5-large'
TOP_K_CANDIDATES = 50 
MIN_SCORE_THRESHOLD = 0.0 

@st.cache_data
def load_app_config():
    try:
        with open("config.toml", "r", encoding="utf-8") as f: return toml.load(f)
    except FileNotFoundError: return {"app": {"title": "Universal AI Agent"}}

@st.cache_resource
def load_embedding_model():
    try: return SentenceTransformer(MODEL_NAME)
    except Exception as e: st.error(f"åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ« '{MODEL_NAME}' ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}"); return None

def init_database():
    with sqlite3.connect(DB_FILE) as conn:
        cursor = conn.cursor()
        cursor.execute('CREATE TABLE IF NOT EXISTS jobs (id INTEGER PRIMARY KEY AUTOINCREMENT, document TEXT NOT NULL, source_data_json TEXT, created_at TEXT)')
        cursor.execute('CREATE TABLE IF NOT EXISTS engineers (id INTEGER PRIMARY KEY AUTOINCREMENT, document TEXT NOT NULL, source_data_json TEXT, created_at TEXT)')
        cursor.execute('CREATE TABLE IF NOT EXISTS matching_results (id INTEGER PRIMARY KEY AUTOINCREMENT, job_id INTEGER, engineer_id INTEGER, score REAL, created_at TEXT, is_hidden INTEGER DEFAULT 0, FOREIGN KEY (job_id) REFERENCES jobs (id), FOREIGN KEY (engineer_id) REFERENCES engineers (id))')

def get_db_connection():
    conn = sqlite3.connect(DB_FILE); conn.row_factory = sqlite3.Row; return conn

# â˜…â˜…â˜… 404ã‚¨ãƒ©ãƒ¼ä¿®æ­£: ãƒ¦ãƒ¼ã‚¶ãƒ¼æ§˜ãŒç™ºè¦‹ã—ãŸæ­£ã—ã„ãƒ¢ãƒ‡ãƒ«åã‚’æ¡ç”¨ â˜…â˜…â˜…
def split_text_with_llm(text_content):
    #model = genai.GenerativeModel('models/gemini-1.5-pro-latest')
    model = genai.GenerativeModel('models/gemini-2.5-pro')
    prompt = f"""
ã‚ãªãŸã¯ã€ITæ¥­ç•Œã®æ¡ˆä»¶æƒ…å ±ã¨æŠ€è¡“è€…æƒ…å ±ã‚’æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æŠ½å‡ºã™ã‚‹å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ãƒ¡ãƒ¼ãƒ«æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€ã€Œæ¡ˆä»¶æƒ…å ±ã€ã¨ã€ŒæŠ€è¡“è€…æƒ…å ±ã€ã‚’ãã‚Œãã‚ŒæŠ½å‡ºã—ã€æŒ‡å®šã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
# æŠ½å‡ºé …ç›®ã¨ãƒ«ãƒ¼ãƒ«
- document: å…ƒã®æƒ…å ±ã‚’ã‚µãƒãƒªãƒ¼ã—ãŸã€æ¦‚è¦ãƒ†ã‚­ã‚¹ãƒˆã€‚
- nationality: æŠ€è¡“è€…ã®å›½ç±ã€‚
- nationality_requirement: æ¡ˆä»¶ã®å›½ç±è¦ä»¶ã€‚
- start_date: ç¨¼åƒé–‹å§‹å¯èƒ½æ—¥ã¾ãŸã¯æ¡ˆä»¶ã®é–‹å§‹æ™‚æœŸã€‚
- è©²å½“ã™ã‚‹æƒ…å ±ãŒãªã„é …ç›®ã¯ null ã¨ã—ã¦ãã ã•ã„ã€‚
- æŠ½å‡ºã§ãã‚‹æƒ…å ±ãŒãªã„å ´åˆã¯ã€ç©ºã®ãƒªã‚¹ãƒˆ `[]` ã‚’æŒã¤JSONã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
# å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
---
{text_content}
---
# å‡ºåŠ›å½¢å¼ (JSONã®ã¿)
{{ "jobs": [ ... ], "engineers": [ ... ] }}
"""
    generation_config = {"response_mime_type": "application/json"}
    safety_settings = {'HARM_CATEGORY_HARASSMENT': 'BLOCK_NONE', 'HARM_CATEGORY_HATE_SPEECH': 'BLOCK_NONE', 'HARM_CATEGORY_SEXUALLY_EXPLICIT': 'BLOCK_NONE', 'HARM_CATEGORY_DANGEROUS_CONTENT': 'BLOCK_NONE'}
    try:
        with st.spinner("LLMãŒãƒ†ã‚­ã‚¹ãƒˆã‚’è§£æãƒ»æ§‹é€ åŒ–ä¸­..."):
            response = model.generate_content(prompt, generation_config=generation_config, safety_settings=safety_settings)
        return json.loads(response.text)
    except Exception as e:
        st.error(f"LLMã«ã‚ˆã‚‹æ§‹é€ åŒ–ã«å¤±æ•—: {e}"); return None

def get_match_summary_with_llm(job_doc, engineer_doc):
    #model = genai.GenerativeModel('models/gemini-1.5-pro-latest')
    model = genai.GenerativeModel('models/gemini-2.5-pro')
    prompt = f"""
ã‚ãªãŸã¯å„ªç§€ãªITæ¡ç”¨æ‹…å½“è€…ã§ã™ã€‚ä»¥ä¸‹ã®ã€æ¡ˆä»¶æƒ…å ±ã€‘ã¨ã€æŠ€è¡“è€…æƒ…å ±ã€‘ã‚’æ¯”è¼ƒã—ã€åˆ†æã—ã¦ãã ã•ã„ã€‚
çµæœã¯å¿…ãšæŒ‡å®šã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
# åˆ†æã®ãƒã‚¤ãƒ³ãƒˆ
- positive_points: ãƒãƒƒãƒã™ã‚‹ç‚¹ã€‚
- concern_points: æ‡¸å¿µç‚¹ã€‚
- summary: ç·åˆçš„ãªã‚µãƒãƒªãƒ¼ã€‚
# ã€æ¡ˆä»¶æƒ…å ±ã€‘
{job_doc}
# ã€æŠ€è¡“è€…æƒ…å ±ã€‘
{engineer_doc}
# å‡ºåŠ›å½¢å¼ (JSONã®ã¿)
{{ "positive_points": ["<åˆè‡´ã™ã‚‹ç‚¹1>"], "concern_points": ["<æ‡¸å¿µç‚¹1>"], "summary": "<ç·åˆè©•ä¾¡ã‚µãƒãƒªãƒ¼>" }}
"""
    generation_config = {"response_mime_type": "application/json"}
    safety_settings = {'HARM_CATEGORY_HARASSMENT': 'BLOCK_NONE', 'HARM_CATEGORY_HATE_SPEECH': 'BLOCK_NONE', 'HARM_CATEGORY_SEXUALLY_EXPLICIT': 'BLOCK_NONE', 'HARM_CATEGORY_DANGEROUS_CONTENT': 'BLOCK_NONE'}
    try:
        with st.spinner("AIãŒãƒãƒƒãƒãƒ³ã‚°æ ¹æ‹ ã‚’åˆ†æä¸­..."):
            response = model.generate_content(prompt, generation_config=generation_config, safety_settings=safety_settings)
        return json.loads(response.text)
    except Exception as e:
        st.error(f"æ ¹æ‹ ã®åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}"); return None

def process_single_content(source_data: dict):
    if not source_data: st.warning("å‡¦ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚"); return False
    body_text_for_llm = source_data.get('body', '')
    if not body_text_for_llm.strip(): st.warning("ãƒ¡ãƒ¼ãƒ«æœ¬æ–‡ãŒç©ºã®ãŸã‚ã€AIã«ã‚ˆã‚‹è§£æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"); return False
    parsed_data = split_text_with_llm(body_text_for_llm)
    if not parsed_data: return False
    new_jobs_data = parsed_data.get("jobs", []); new_engineers_data = parsed_data.get("engineers", [])
    if not new_jobs_data and not new_engineers_data: st.warning("LLMã¯ãƒ¡ãƒ¼ãƒ«æœ¬æ–‡ã‹ã‚‰æ¡ˆä»¶æƒ…å ±ã¾ãŸã¯æŠ€è¡“è€…æƒ…å ±ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"); return False
    with get_db_connection() as conn:
        cursor = conn.cursor(); now_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        source_json_str = json.dumps(source_data, ensure_ascii=False, indent=2)
        for item_data in new_jobs_data:
            doc = item_data.get("document")
            if not (doc and str(doc).strip() and str(doc).lower() != 'none'): st.warning("LLMãŒæ¡ˆä»¶ã®è¦ç´„ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ¡ãƒ¼ãƒ«æœ¬æ–‡ã‚’ä»£æ›¿ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚"); doc = body_text_for_llm
            meta_info = f"[å›½ç±è¦ä»¶: {item_data.get('nationality_requirement', 'ä¸æ˜')}] [é–‹å§‹æ™‚æœŸ: {item_data.get('start_date', 'ä¸æ˜')}]\n---\n"; full_document = meta_info + doc
            cursor.execute('INSERT INTO jobs (document, source_data_json, created_at) VALUES (?, ?, ?)', (full_document, source_json_str, now_str)); 
        for item_data in new_engineers_data:
            doc = item_data.get("document")
            if not (doc and str(doc).strip() and str(doc).lower() != 'none'): st.warning("LLMãŒæŠ€è¡“è€…ã®è¦ç´„ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ¡ãƒ¼ãƒ«æœ¬æ–‡ã‚’ä»£æ›¿ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚"); doc = body_text_for_llm
            meta_info = f"[å›½ç±: {item_data.get('nationality', 'ä¸æ˜')}] [ç¨¼åƒå¯èƒ½æ—¥: {item_data.get('start_date', 'ä¸æ˜')}]\n---\n"; full_document = meta_info + doc
            cursor.execute('INSERT INTO engineers (document, source_data_json, created_at) VALUES (?, ?, ?)', (full_document, source_json_str, now_str)); 
    return True

def get_email_contents(msg) -> dict:
    body_text = ""; attachments = []
    if msg.is_multipart():
        for part in msg.walk():
            content_type = part.get_content_type(); content_disposition = str(part.get("Content-Disposition"))
            if 'text/plain' in content_type and 'attachment' not in content_disposition:
                charset = part.get_content_charset()
                try: body_text += part.get_payload(decode=True).decode(charset if charset else 'utf-8', errors='ignore')
                except Exception: body_text += part.get_payload(decode=True).decode('utf-8', errors='ignore')
            if 'attachment' in content_disposition:
                raw_filename = part.get_filename()
                if raw_filename:
                    decoded_header = decode_header(raw_filename)
                    filename = "".join([s.decode(c or 'utf-8', 'ignore') if isinstance(s, bytes) else s for s, c in decoded_header])
                    st.write(f"ğŸ“„ æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚"); attachments.append({"filename": filename})
    else:
        charset = msg.get_content_charset()
        try: body_text = msg.get_payload(decode=True).decode(charset if charset else 'utf-8', errors='ignore')
        except Exception: body_text = msg.get_payload(decode=True).decode('utf-8', errors='ignore')
    return {"body": body_text.strip(), "attachments": attachments}

# â˜…â˜…â˜… TypeErrorä¿®æ­£: ã™ã¹ã¦ã®returnãƒ‘ã‚¹ã§2ã¤ã®å€¤ã‚’è¿”ã™ã‚ˆã†ã«ä¿®æ­£ â˜…â˜…â˜…
def fetch_and_process_emails():
    log_stream = io.StringIO()
    try:
        with contextlib.redirect_stdout(log_stream):
            try:
                SERVER, USER, PASSWORD = st.secrets["EMAIL_SERVER"], st.secrets["EMAIL_USER"], st.secrets["EMAIL_PASSWORD"]
            except KeyError as e:
                st.error(f"ãƒ¡ãƒ¼ãƒ«ã‚µãƒ¼ãƒãƒ¼ã®æ¥ç¶šæƒ…å ±ãŒSecretsã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“: {e}")
                return False, log_stream.getvalue() # 2ã¤ã®å€¤ã‚’è¿”ã™

            try:
                mail = imaplib.IMAP4_SSL(SERVER)
                mail.login(USER, PASSWORD)
                mail.select('inbox')
            except Exception as e:
                st.error(f"ãƒ¡ãƒ¼ãƒ«ã‚µãƒ¼ãƒãƒ¼ã¸ã®æ¥ç¶šã¾ãŸã¯ãƒ­ã‚°ã‚¤ãƒ³ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                return False, log_stream.getvalue() # 2ã¤ã®å€¤ã‚’è¿”ã™

            total_processed_count = 0
            try:
                with st.status("æœ€æ–°ã®æœªèª­ãƒ¡ãƒ¼ãƒ«ã‚’å–å¾—ãƒ»å‡¦ç†ä¸­...", expanded=True) as status:
                    _, messages = mail.search(None, 'UNSEEN')
                    email_ids = messages[0].split()
                    if not email_ids:
                        st.write("å‡¦ç†å¯¾è±¡ã®æœªèª­ãƒ¡ãƒ¼ãƒ«ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                    else:
                        latest_ids = email_ids[::-1][:10]
                        st.write(f"æœ€æ–°ã®æœªèª­ãƒ¡ãƒ¼ãƒ« {len(latest_ids)}ä»¶ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚")
                        for i, email_id in enumerate(latest_ids):
                            _, msg_data = mail.fetch(email_id, '(RFC822)')
                            for response_part in msg_data:
                                if isinstance(response_part, tuple):
                                    msg = email.message_from_bytes(response_part[1])
                                    source_data = get_email_contents(msg)
                                    if source_data['body'] or source_data['attachments']:
                                        st.write(f"âœ… ãƒ¡ãƒ¼ãƒ«ID {email_id.decode()} ã¯å‡¦ç†å¯¾è±¡ã§ã™ã€‚è§£æã‚’é–‹å§‹ã—ã¾ã™...")
                                        if process_single_content(source_data):
                                            total_processed_count += 1
                                            mail.store(email_id, '+FLAGS', '\\Seen')
                                    else: st.write(f"âœ–ï¸ ãƒ¡ãƒ¼ãƒ«ID {email_id.decode()} ã¯å‡¦ç†å¯¾è±¡å¤–ã§ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                            st.write(f"({i+1}/{len(latest_ids)}) ãƒã‚§ãƒƒã‚¯å®Œäº†")
                    status.update(label="ãƒ¡ãƒ¼ãƒ«ãƒã‚§ãƒƒã‚¯å®Œäº†", state="complete")
            finally:
                mail.close()
                mail.logout()

        if total_processed_count > 0:
            st.success(f"åˆè¨ˆ {total_processed_count} ä»¶ã®ãƒ¡ãƒ¼ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã€ä¿å­˜ã—ã¾ã—ãŸã€‚"); st.balloons()
        else:
            st.info("å‡¦ç†å¯¾è±¡ã¨ãªã‚‹æ–°ã—ã„æœªèª­ãƒ¡ãƒ¼ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return True, log_stream.getvalue() # 2ã¤ã®å€¤ã‚’è¿”ã™

    except Exception as e:
        st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return False, log_stream.getvalue() # 2ã¤ã®å€¤ã‚’è¿”ã™

def hide_match(result_id): pass
# (ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢é–¢é€£ã® update_index, search, run_matching_for_item ã¯ä»Šå›ã®ãƒ‡ãƒãƒƒã‚°ã§ã¯ä½¿ç”¨ã—ã¦ã„ãªã„ãŸã‚ã€çœç•¥ã—ã¦ã„ã¾ã™)
